[
  {
    "id": "01",
    "title": "Myfirst Agent",
    "description": "AI ì—ì´ì „íŠ¸ ê°œë°œì˜ ì‹œì‘ì„ ìœ„í•œ ìŠ¤íƒ€í„° í”„ë¡œì íŠ¸",
    "directory": "01_myfirst_agent",
    "readmePath": "01_myfirst_agent/README.md",
    "mainCodePath": "01_myfirst_agent/main.ipynb",
    "readme": "",
    "codes": {
      "main.ipynb": "import os\nfrom dotenv import load_dotenv\nimport google.generativeai as genai\nfrom google.generativeai.types import GenerationConfig\n\nload_dotenv()\n\ngenai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n\nmodel = genai.GenerativeModel(\n    \"gemini-2.0-flash\",\n    system_instruction=\"\"\"\n    You have the following functions in my system:\n    `get_weather`\n    `get_news`\n    `get_currency`\n    \n    When the user asks a question, identify the appropriate function.\n    Then, provide a detailed answer to the question.\n    Finally, wrap your detailed answer inside the function call.\n    \n    Format: function_name(detailed_answer)\n    \n    Example:\n    User: What is the weather in Spain?\n    AI: get_weather(The weather in Spain is sunny with temperatures around 25 degrees...)\n    \"\"\"\n)\n\nchat = model.start_chat(history=[])\n\nwhile True:\n    user_question = input(\"Ask a question (or type 'q' to quit): \")\n    if user_question.lower() in [\"q\", \"quit\"]:\n        break      \n    response = chat.send_message(user_question)\n    print(f\"AI: {response.text}\")\n\n# ---\n\nimport os\nfrom dotenv import load_dotenv\nimport google.generativeai as genai\n\nload_dotenv()\n\ngenai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n\ndef get_weather(country):\n    return f\"The weather in {country} is sunny.\"\n\ndef get_news(country):\n    return f\"News in {country}: No major events today.\"\n\ndef get_currency(country):\n    return f\"Currency in {country} is Euro.\"\n\ntools = [get_weather, get_news, get_currency]\n\nmodel_with_tools = genai.GenerativeModel(\n    \"gemini-2.0-flash\",\n    tools=tools\n)\n\nchat_with_tools = model_with_tools.start_chat(enable_automatic_function_calling=True)\n\nwhile True:\n    user_question = input(\"Ask a question to Tool Agent (or type 'q' to quit): \")\n    if user_question.lower() in [\"q\", \"quit\"]:\n        break      \n    response = chat_with_tools.send_message(user_question)\n    print(f\"Tool Agent: {response.text}\")",
      "main.py": "def main():\n    print(\"Hello from 1-myfirst-agent!\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    "assets": [],
    "techStack": [
      "Python"
    ]
  },
  {
    "id": "02",
    "title": "News Reader Agent",
    "description": "CrewAIë¥¼ í™œìš©í•œ ì§€ëŠ¥í˜• ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë³´ê³  ì‹œìŠ¤í…œ",
    "directory": "02_news-reader-agent",
    "readmePath": "02_news-reader-agent/README.md",
    "mainCodePath": "02_news-reader-agent/main.py",
    "readme": "# AI News Reader Agent\n\nAn intelligent news aggregation and reporting system built with [CrewAI](https://crewai.com) and [Google Gemini](https://deepmind.google/technologies/gemini/). This agentic workflow automates the process of discovering, summarizing, and curating news on specific topics, delivering professional-grade briefings in Korean.\n\n## ğŸš€ Features\n\n- **Automated Content Harvesting**: \n  - Searches for recent news articles using Serper API.\n  - Filters for credibility and relevance.\n  - Scrapes full article content.\n- **Multi-Tier Summarization (Korean)**:\n  - **Headline Summary**: Tweet-style, under 280 characters.\n  - **Executive Summary**: Concise briefing for quick reading.\n  - **Comprehensive Summary**: Detailed context and analysis.\n- **Professional Curation**:\n  - Assembles a cohesive \"Daily News Briefing\".\n  - Includes editorial analysis, key themes, and looking-ahead sections.\n  - Formatted as a publication-ready Markdown report.\n\n## ğŸ¤– Agents\n\nThe system utilizes three specialized agents:\n\n1.  **News Hunter Agent**: Responsible for searching the web, filtering irrelevant content, and scraping article text.\n2.  **Summarizer Agent**: Analyzes the scraped content and generates structured summaries in Korean.\n3.  **Curator Agent**: Compiles the summaries into a final, polished report with editorial insights.\n\n## ğŸ› ï¸ Prerequisites\n\n- Python 3.10+\n- [uv](https://github.com/astral-sh/uv) (recommended) or pip\n- API Keys:\n  - **Google Gemini API Key**: For the LLM.\n  - **Serper API Key**: For Google Search capabilities.\n\n## ğŸ“¦ Installation\n\n1.  **Clone the repository**:\n    ```bash\n    git clone <repository-url>\n    cd 2_news-reader-agent\n    ```\n\n2.  **Install dependencies**:\n    Using `uv` (recommended):\n    ```bash\n    uv sync\n    ```\n    Or using pip:\n    ```bash\n    pip install crewai crewai-tools python-dotenv\n    ```\n\n## âš™ï¸ Configuration\n\n1.  Create a `.env` file in the root directory:\n    ```bash\n    touch .env\n    ```\n\n2.  Add your API keys to `.env`:\n    ```env\n    GEMINI_API_KEY=your_gemini_api_key_here\n    SERPER_API_KEY=your_serper_api_key_here\n    ```\n\n## ğŸƒ Usage\n\nRun the news reader agent:\n\n```bash\npython news_reader.py\n```\n\nBy default, it searches for news about **\"near protocol news\"**. You can change the topic in `news_reader.py`:\n\n```python\nresult = NewsReaderCrew().crew().kickoff(\n    inputs={\n        \"topic\": \"your desired topic\",\n    }\n)\n```\n\n## ğŸ“‚ Output\n\nThe agent generates the following files in the `output/` directory:\n\n-   `content_harvest.md`: Raw collected articles with metadata.\n-   `summary.md`: Structured summaries for each article.\n-   `final_report.md`: The final, polished daily news briefing.\n",
    "codes": {
      "main.py": "import os\nimport dotenv\nfrom crewai import Crew, Agent, Task, LLM\nfrom crewai.project import CrewBase, agent, task, crew\nfrom tools import count_letters\n\ndotenv.load_dotenv()\n\n@CrewBase\nclass TranslatorCrew:\n    \"\"\"Translator Crew\"\"\"\n    agents_config = \"config/agents.yaml\"\n    tasks_config = \"config/tasks.yaml\"\n\n    def __init__(self):\n        self.llm = LLM(\n            model=\"gemini/gemini-2.5-flash-preview-09-2025\",\n            api_key=os.environ[\"GEMINI_API_KEY\"]\n        )\n\n    @agent\n    def translator_agent(self) -> Agent:\n        return Agent(\n            config=self.agents_config[\"translator_agent\"],\n            llm=self.llm,\n            verbose=True\n        )\n\n    @agent\n    def counter_agent(self) -> Agent:\n        return Agent(\n            config=self.agents_config[\"counter_agent\"],\n            tools=[count_letters],\n            llm=self.llm,\n            verbose=True\n        )\n\n    @task\n    def translate_task(self) -> Task:\n        return Task(\n            config=self.tasks_config[\"translate_task\"],\n        )\n\n    @task\n    def retranslate_task(self) -> Task:\n        return Task(\n            config=self.tasks_config[\"retranslate_task\"],\n        )\n\n    @task\n    def count_task(self) -> Task:\n        return Task(\n            config=self.tasks_config[\"count_task\"],\n        )\n\n    @crew\n    def assemble_crew(self) -> Crew:\n        return Crew(\n            agents=self.agents,\n            tasks=self.tasks,\n            verbose=True,\n        )\n\ndef main():\n    TranslatorCrew().assemble_crew().kickoff(\n        inputs={\n            \"sentence\": \"I'm Henry and I like to ride my bicicle in Napoli\",\n        }\n    )\n\nif __name__ == \"__main__\":\n    main()\n",
      "news_reader.py": "import os\nimport dotenv\nfrom crewai import Crew, Agent, Task, LLM\nfrom crewai.project import CrewBase, agent, task, crew\nfrom crewai.project import CrewBase, agent, task, crew\nfrom tools import search_tool, scrape_tool\n\ndotenv.load_dotenv()\n\n@CrewBase\nclass NewsReaderCrew:\n    \"\"\"News Reader Crew\"\"\"\n    agents_config = \"config/news_agents.yaml\"\n    tasks_config = \"config/news_tasks.yaml\"\n\n    def __init__(self):\n        self.llm = LLM(\n            model=\"gemini/gemini-2.0-flash\",\n            api_key=os.environ[\"GEMINI_API_KEY\"]\n        )\n\n    @agent\n    def news_hunter_agent(self) -> Agent:\n        return Agent(\n            config=self.agents_config[\"news_hunter_agent\"],\n            tools=[search_tool, scrape_tool],\n            llm=self.llm,\n            verbose=True\n        )\n\n    @agent\n    def summarizer_agent(self) -> Agent:\n        return Agent(\n            config=self.agents_config[\"summarizer_agent\"],\n            tools=[scrape_tool],\n            llm=self.llm,\n            verbose=True\n        )\n\n    @agent\n    def curator_agent(self) -> Agent:\n        return Agent(\n            config=self.agents_config[\"curator_agent\"],\n            llm=self.llm,\n            verbose=True\n        )\n\n    @task\n    def content_harvesting_task(self) -> Task:\n        return Task(\n            config=self.tasks_config[\"content_harvesting_task\"],\n        )\n\n    @task\n    def summarization_task(self) -> Task:\n        return Task(\n            config=self.tasks_config[\"summarization_task\"],\n        )\n\n    @task\n    def final_report_assembly_task(self) -> Task:\n        return Task(\n            config=self.tasks_config[\"final_report_assembly_task\"],\n        )\n\n    @crew\n    def crew(self) -> Crew:\n        return Crew(\n            agents=self.agents,\n            tasks=self.tasks,\n            verbose=True,\n        )\n\ndef main():\n    result = NewsReaderCrew().crew().kickoff(\n        inputs={\n            \"topic\": \"near protocol news\",\n        }\n    )\n\n    for task_output in result.tasks_output:\n        print(task_output)\n\nif __name__ == \"__main__\":\n    main()\n",
      "tools.py": "from crewai.tools import tool\nfrom crewai_tools import SerperDevTool, ScrapeWebsiteTool\n\nsearch_tool = SerperDevTool()\nscrape_tool = ScrapeWebsiteTool()\n\n@tool\ndef count_letters(sentence: str):\n    \"\"\"\n    This function is to count the amount of letters in a sentence.\n    The input is a `sentence` string.\n    The output is a number.\n    \"\"\"\n    print(\"tool called with input:\", sentence)\n    return len(sentence)\n"
    },
    "assets": [],
    "techStack": [
      "CrewAI",
      "Gemini",
      "Python"
    ]
  },
  {
    "id": "03",
    "title": "Job Hunter Agent",
    "description": "ì±„ìš© ê³µê³  ë¶„ì„ ë° ì´ë ¥ì„œ ìµœì í™” ì§€ì› ì—ì´ì „íŠ¸",
    "directory": "03_job-hunter-agent",
    "readmePath": "03_job-hunter-agent/README.md",
    "mainCodePath": "03_job-hunter-agent/main.py",
    "readme": "# Job Hunter Agent (AI ì·¨ì—… ì—ì´ì „íŠ¸)\n\nCrewAIì™€ Geminië¥¼ í™œìš©í•˜ì—¬ êµ¬ì§ ê³¼ì •ì„ ìë™í™”í•´ì£¼ëŠ” AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. ì±„ìš© ê³µê³  ê²€ìƒ‰ë¶€í„° ì í•©ë„ ë¶„ì„, ì´ë ¥ì„œ ìµœì í™”, ê¸°ì—… ì¡°ì‚¬, ë©´ì ‘ ì¤€ë¹„ê¹Œì§€ ë„ì™€ì¤ë‹ˆë‹¤.\n\n## ğŸ“– ëª©ì°¨\n- [ì£¼ìš” ê¸°ëŠ¥](#-ì£¼ìš”-ê¸°ëŠ¥)\n- [ì„¤ì¹˜ ë° ì‹¤í–‰](#-ì„¤ì¹˜-ë°-ì‹¤í–‰)\n  - [í•„ìˆ˜ ì¡°ê±´](#-í•„ìˆ˜-ì¡°ê±´)\n  - [ì €ì¥ì†Œ í´ë¡ ](#-ì €ì¥ì†Œ-í´ë¡ )\n  - [ì˜ì¡´ì„± ì„¤ì¹˜](#-ì˜ì¡´ì„±-ì„¤ì¹˜)\n  - [í™˜ê²½ ë³€ìˆ˜ ì„¤ì •](#-í™˜ê²½-ë³€ìˆ˜-ì„¤ì •)\n  - [ì´ë ¥ì„œ ì¤€ë¹„](#-ì´ë ¥ì„œ-ì¤€ë¹„)\n  - [ì‹¤í–‰](#-ì‹¤í–‰)\n- [ì»¤ìŠ¤í„°ë§ˆì´ì§•](#-ì»¤ìŠ¤í„°ë§ˆì´ì§•)\n- [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#-íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)\n- [í”„ë¡œì íŠ¸ êµ¬ì¡°](#-í”„ë¡œì íŠ¸-êµ¬ì¡°)\n- [ê²°ê³¼ë¬¼ (Output)](#-ê²°ê³¼ë¬¼-output)\n- [ì£¼ì˜ì‚¬í•­](#-ì£¼ì˜ì‚¬í•­)\n\n## ğŸš€ ì£¼ìš” ê¸°ëŠ¥\n\n1. **ì±„ìš© ê³µê³  ê²€ìƒ‰ (Job Search)**: ì‚¬ìš©ìì˜ í¬ë§ ì§ë¬´, ë ˆë²¨, ìœ„ì¹˜ì— ë§ëŠ” ê³µê³ ë¥¼ ì›¹ì—ì„œ ê²€ìƒ‰í•˜ê³  ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤.\n2. **ì§ë¬´ ë§¤ì¹­ ë¶„ì„ (Job Matching)**: ì‚¬ìš©ì ì´ë ¥ì„œì™€ ê³µê³ ë¥¼ ë¹„êµí•˜ì—¬ ì í•©ë„ë¥¼ ì ìˆ˜(1-5)ë¡œ ë§¤ê¸°ê³  ì´ìœ ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.\n3. **ìµœê³ ì˜ ê³µê³  ì„ ì • (Job Selection)**: ë¶„ì„ëœ ê³µê³  ì¤‘ ì‚¬ìš©ìì˜ ìŠ¤í‚¬ê³¼ ì„ í˜¸ë„ì— ê°€ì¥ ì˜ ë§ëŠ” ê³µê³ ë¥¼ ì„ ì •í•©ë‹ˆë‹¤.\n4. **ì´ë ¥ì„œ ìµœì í™” (Resume Optimization)**: ì„ ì •ëœ ê³µê³ ì— ë§ì¶° ì‚¬ìš©ì ì´ë ¥ì„œë¥¼ ì¬ì‘ì„±í•©ë‹ˆë‹¤. (ì‚¬ì‹¤ì„ ê¸°ë°˜ìœ¼ë¡œ ë‚´ìš©ì„ ê°•ì¡°í•˜ë©°, í—ˆìœ„ ì‚¬ì‹¤ì€ ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)\n5. **ê¸°ì—… ì¡°ì‚¬ (Company Research)**: ì„ ì •ëœ ê¸°ì—…ì˜ ë¯¸ì…˜, ê°€ì¹˜, ìµœê·¼ ë‰´ìŠ¤, ë©´ì ‘ ì˜ˆìƒ ì§ˆë¬¸ ë“±ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤.\n6. **ë©´ì ‘ ì¤€ë¹„ (Interview Prep)**: ì§ë¬´, ìµœì í™”ëœ ì´ë ¥ì„œ, ê¸°ì—… ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ ë©´ì ‘ ì¤€ë¹„ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n\n## ğŸ› ï¸ ì„¤ì¹˜ ë° ì‹¤í–‰\n\n### í•„ìˆ˜ ì¡°ê±´\n- **Python 3.13 ì´ìƒ**: ì´ í”„ë¡œì íŠ¸ëŠ” ìµœì‹  Python ê¸°ëŠ¥ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n- **uv (ê¶Œì¥)**: ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ íŒ¨í‚¤ì§€ ê´€ë¦¬ë¥¼ ìœ„í•´ `uv` ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n- **API Keys**:\n  - [Firecrawl](https://www.firecrawl.dev/) (ì›¹ ê²€ìƒ‰ ë° ìŠ¤í¬ë˜í•‘ìš©)\n  - [Google Gemini](https://aistudio.google.com/) (AI ëª¨ë¸ìš©)\n\n### 1. ì €ì¥ì†Œ í´ë¡ \n```bash\ngit clone <repository-url>\ncd 3_job-hunter-agent\n```\n\n### 2. ì˜ì¡´ì„± ì„¤ì¹˜\nì´ í”„ë¡œì íŠ¸ëŠ” `uv`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ì¡´ì„±ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.\n\n**uv ì‚¬ìš© ì‹œ (ê¶Œì¥):**\n```bash\n# ê°€ìƒí™˜ê²½ ìƒì„± ë° íŒ¨í‚¤ì§€ ë™ê¸°í™”\nuv sync\n```\n\n**pip ì‚¬ìš© ì‹œ:**\n```bash\npip install \"crewai[google-genai,tools]>=0.152.0\" \"firecrawl-py>=2.16.3\" \"google-generativeai>=0.8.5\" \"python-dotenv>=1.1.1\"\n```\n\n### 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\ní”„ë¡œì íŠ¸ ë£¨íŠ¸ì— `.env` íŒŒì¼ì„ ìƒì„±í•˜ê³  í•„ìš”í•œ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n\n```env\n# .env\nFIRECRAWL_API_KEY=fc_...\nGOOGLE_API_KEY=AIza...\n# CrewAI ëª¨ë¸ ì„¤ì • (Gemini ì‚¬ìš© ì‹œ í•„ìˆ˜)\nMODEL=gemini/gemini-1.5-pro\n```\n\n### 4. ì´ë ¥ì„œ ì¤€ë¹„\në³¸ì¸ì˜ ì´ë ¥ì„œ ë‚´ìš©ì„ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì¤€ë¹„í•´ì•¼ í•©ë‹ˆë‹¤.\n1. í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë˜ëŠ” `knowledge/` í´ë”ì— `resume.txt` íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.\n2. ì´ë ¥ì„œ ë‚´ìš©ì„ ë³µì‚¬í•˜ì—¬ ë¶™ì—¬ë„£ìŠµë‹ˆë‹¤. (PDFë‚˜ Word íŒŒì¼ì´ ì•„ë‹Œ **ìˆœìˆ˜ í…ìŠ¤íŠ¸**ì—¬ì•¼ í•©ë‹ˆë‹¤.)\n\n### 5. ì‹¤í–‰\n```bash\npython main.py\n```\nì‹¤í–‰ í›„ í„°ë¯¸ë„ì˜ ì•ˆë‚´ì— ë”°ë¼ ë‹¤ìŒ ì •ë³´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:\n- **Position**: í¬ë§ ì§ë¬´ (ì˜ˆ: Full Stack Developer)\n- **Level**: ê²½ë ¥ ë ˆë²¨ (ì˜ˆ: Senior, Junior)\n- **Location**: í¬ë§ ê·¼ë¬´ì§€ (ì˜ˆ: Remote, Seoul, Korea)\n\n## âš™ï¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•\n\nì´ í”„ë¡œì íŠ¸ëŠ” `config/` í´ë” ë‚´ì˜ YAML íŒŒì¼ì„ í†µí•´ ì‰½ê²Œ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n- **`config/agents.yaml`**: ì—ì´ì „íŠ¸ì˜ ì—­í• (Role), ëª©í‘œ(Goal), ë°°ê²½ ì´ì•¼ê¸°(Backstory)ë¥¼ ìˆ˜ì •í•˜ì—¬ ì—ì´ì „íŠ¸ì˜ í˜ë¥´ì†Œë‚˜ë¥¼ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n- **`config/tasks.yaml`**: ê° íƒœìŠ¤í¬ì˜ ì„¤ëª…(Description)ê³¼ ì˜ˆìƒ ì¶œë ¥(Expected Output)ì„ ìˆ˜ì •í•˜ì—¬ ì‘ì—…ì˜ êµ¬ì²´ì ì¸ ì§€ì‹œì‚¬í•­ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n## â“ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…\n\n**Q: `FileNotFoundError: Could not find resume.txt` ì˜¤ë¥˜ê°€ ë°œìƒí•´ìš”.**\nA: `resume.txt` íŒŒì¼ì´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë‚˜ `knowledge/` í´ë” ì•ˆì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. íŒŒì¼ëª… ì² ìê°€ ì •í™•í•œì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.\n\n**Q: API ê´€ë ¨ ì˜¤ë¥˜ê°€ ë°œìƒí•´ìš”.**\nA: `.env` íŒŒì¼ì— `FIRECRAWL_API_KEY`ì™€ `GOOGLE_API_KEY`ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ë˜í•œ, API ì‚¬ìš©ëŸ‰ í•œë„ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸í•´ ë³´ì„¸ìš”.\n\n**Q: `uv` ëª…ë ¹ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”.**\nA: `uv`ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´ `pip install uv`ë¡œ ì„¤ì¹˜í•˜ê±°ë‚˜, `pip`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ì¡´ì„±ì„ ì„¤ì¹˜í•´ ì£¼ì„¸ìš”.\n\n## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°\n```\n3_job-hunter-agent/\nâ”œâ”€â”€ config/\nâ”‚   â”œâ”€â”€ agents.yaml       # ì—ì´ì „íŠ¸ ì„¤ì •\nâ”‚   â””â”€â”€ tasks.yaml        # íƒœìŠ¤í¬ ì„¤ì •\nâ”œâ”€â”€ knowledge/\nâ”‚   â””â”€â”€ resume.txt        # (ì‚¬ìš©ì ì œê³µ) ì´ë ¥ì„œ íŒŒì¼\nâ”œâ”€â”€ output/               # (ìë™ ìƒì„±) ê²°ê³¼ë¬¼ ì €ì¥ì†Œ\nâ”‚   â”œâ”€â”€ ranked_jobs.json\nâ”‚   â”œâ”€â”€ chosen_job.json\nâ”‚   â”œâ”€â”€ rewritten_resume.md\nâ”‚   â”œâ”€â”€ company_research.md\nâ”‚   â””â”€â”€ interview_prep.md\nâ”œâ”€â”€ main.py               # ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸\nâ”œâ”€â”€ models.py             # Pydantic ë°ì´í„° ëª¨ë¸\nâ”œâ”€â”€ tools.py              # ì»¤ìŠ¤í…€ ë„êµ¬ (Firecrawl)\nâ”œâ”€â”€ pyproject.toml        # í”„ë¡œì íŠ¸ ì˜ì¡´ì„± ì„¤ì •\nâ””â”€â”€ README.md             # í”„ë¡œì íŠ¸ ë¬¸ì„œ\n```\n\n## ğŸ“¤ ê²°ê³¼ë¬¼ (Output)\nì‹¤í–‰ì´ ì™„ë£Œë˜ë©´ `output/` í´ë”ì— ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:\n- **ranked_jobs.json**: ê²€ìƒ‰ëœ ê³µê³ ë“¤ê³¼ ë§¤ì¹­ ì ìˆ˜ ëª©ë¡.\n- **chosen_job.json**: ìµœì¢… ì„ ì •ëœ ê³µê³ ì˜ ìƒì„¸ ì •ë³´.\n- **rewritten_resume.md**: ì„ ì •ëœ ê³µê³ ì— ë§ì¶° ìµœì í™”ëœ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ì´ë ¥ì„œ.\n- **company_research.md**: ê¸°ì—… ë¶„ì„ ë³´ê³ ì„œ (ê°œìš”, ë¯¸ì…˜, ë©´ì ‘ í† í”½ ë“±).\n- **interview_prep.md**: ìµœì¢… ë©´ì ‘ ì¤€ë¹„ ê°€ì´ë“œ ë¬¸ì„œ.\n\n## âš ï¸ ì£¼ì˜ì‚¬í•­\n- **ì´ë ¥ì„œ ë‚´ìš©**: `rewritten_resume.md`ëŠ” AIê°€ ìƒì„±í•œ ì´ˆì•ˆì…ë‹ˆë‹¤. ì‹¤ì œ ì œì¶œ ì „ ë°˜ë“œì‹œ ë‚´ìš©ì„ í™•ì¸í•˜ê³  ìˆ˜ì •í•˜ì„¸ìš”.\n- **API ë¹„ìš©**: Firecrawl ë° Gemini API ì‚¬ìš©ì— ë”°ë¥¸ ë¹„ìš©ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n---\nPowered by [CrewAI](https://crewai.com) & [Gemini](https://deepmind.google/technologies/gemini/)\n",
    "codes": {
      "main.py": "import os\nimport dotenv\nfrom pathlib import Path\n\ndotenv.load_dotenv()\n\nfrom crewai import Crew, Agent, Task\nfrom crewai.project import CrewBase, task, agent, crew\nfrom models import JobList, RankedJobList, ChosenJob\nfrom tools import web_search_tool\n\n@CrewBase\nclass JobHunterCrew:\n\n    @agent\n    def job_search_agent(self):\n        return Agent(\n            config=self.agents_config[\"job_search_agent\"],\n            tools=[web_search_tool],\n        )\n\n    @agent\n    def job_matching_agent(self):\n        return Agent(config=self.agents_config[\"job_matching_agent\"])\n\n    @agent\n    def resume_optimization_agent(self):\n        return Agent(config=self.agents_config[\"resume_optimization_agent\"])\n\n    @agent\n    def company_research_agent(self):\n        return Agent(config=self.agents_config[\"company_research_agent\"])\n\n    @agent\n    def interview_prep_agent(self):\n        return Agent(config=self.agents_config[\"interview_prep_agent\"])\n\n    @task\n    def job_extraction_task(self):\n        return Task(\n            config=self.tasks_config[\"job_extraction_task\"],\n            output_pydantic=JobList,\n        )\n\n    @task\n    def job_matching_task(self):\n        return Task(\n            config=self.tasks_config[\"job_matching_task\"],\n            output_pydantic=RankedJobList,\n            output_file=\"output/ranked_jobs.json\",\n        )\n\n    @task\n    def job_selection_task(self):\n        return Task(\n            config=self.tasks_config[\"job_selection_task\"],\n            output_pydantic=ChosenJob,\n            output_file=\"output/chosen_job.json\",\n        )\n\n    @task\n    def resume_rewriting_task(self):\n        return Task(\n            config=self.tasks_config[\"resume_rewriting_task\"],\n            context=[\n                self.job_selection_task(),\n            ],\n        )\n\n    @task\n    def company_research_task(self):\n        return Task(\n            config=self.tasks_config[\"company_research_task\"],\n            context=[\n                self.job_selection_task(),\n            ],\n        )\n\n    @task\n    def interview_prep_task(self):\n        return Task(\n            config=self.tasks_config[\"interview_prep_task\"],\n            context=[\n                self.job_selection_task(),\n                self.resume_rewriting_task(),\n                self.company_research_task(),\n            ],\n        )\n\n    @crew\n    def crew(self):\n        return Crew(\n            agents=self.agents,\n            tasks=self.tasks,\n            verbose=True,\n        )\n\n\ndef main():\n    print(\"ğŸš€ Job Hunter Agent - Powered by Gemini\")\n    print(\"=\" * 50)\n    \n    # Read resume content\n    try:\n        resume_path = Path(\"resume.txt\")\n        if not resume_path.exists():\n             # Fallback to knowledge/resume.txt if needed\n             resume_path = Path(\"knowledge/resume.txt\")\n        \n        if resume_path.exists():\n            resume_content = resume_path.read_text()\n            print(\"âœ… Resume loaded successfully!\")\n        else:\n            raise FileNotFoundError(\"Could not find resume.txt or knowledge/resume.txt\")\n            \n    except Exception as e:\n        print(f\"âŒ Error reading resume: {e}\")\n        return\n\n    # Get user inputs\n    position = input(\"\\nğŸ“‹ What position are you looking for? (e.g., Full Stack Developer): \").strip() or \"Full Stack Developer\"\n    level = input(\"ğŸ“Š What level? (e.g., Senior, Mid, Junior): \").strip() or \"Senior\"\n    location = input(\"ğŸ“ What location? (e.g., Remote, Copenhagen, Denmark): \").strip() or \"Remote\"\n    \n    print(f\"\\nğŸ” Searching for {level} {position} jobs in {location}...\")\n    print(\"=\" * 50)\n    print(\"\\nğŸ¤– Starting AI agents workflow...\")\n    print(\"=\" * 50)\n    \n    try:\n        # Execute the crew with inputs\n        result = JobHunterCrew().crew().kickoff(\n            inputs={\n                \"position\": position,\n                \"level\": level,\n                \"location\": location,\n                \"resume_content\": resume_content,\n            }\n        )\n        \n        print(\"\\n\" + \"=\" * 50)\n        print(\"âœ… Job Hunter Agent completed successfully!\")\n        print(\"=\" * 50)\n        print(\"\\nğŸ“ Output files generated:\")\n        print(\"  - output/ranked_jobs.json\")\n        print(\"  - output/chosen_job.json\")\n        print(\"  - output/rewritten_resume.md\")\n        print(\"  - output/company_research.md\")\n        print(\"  - output/interview_prep.md\")\n        print(\"\\nğŸ’¡ Check the output directory for your personalized job application materials!\")\n        \n    except Exception as e:\n        print(f\"\\nâŒ Error: {e}\")\n        raise\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "models.py": "from typing import List, Union\nfrom pydantic import BaseModel\nfrom datetime import date\n\n\nclass Job(BaseModel):\n\n    job_title: str\n    company_name: str\n    job_location: str\n    is_remote_friendly: bool | None = None\n    employment_type: str | None = None\n    compensation: str | None = None\n    job_posting_url: str\n    job_summary: str\n\n    key_qualifications: List[str] | None = None\n    job_responsibilities: List[str] | None = None\n    date_listed: Union[str, date, None] = None\n    required_technologies: List[str] | None = None\n    core_keywords: List[str] | None = None\n\n    role_seniority_level: str | None = None\n    years_of_experience_required: str | None = None\n    minimum_education: str | None = None\n    job_benefits: List[str] | None = None\n    includes_equity: bool | None = None\n    offers_visa_sponsorship: bool | None = None\n    hiring_company_size: str | None = None\n    hiring_industry: str | None = None\n    source_listing_url: str | None = None\n    full_raw_job_description: str | None = None\n\n\nclass JobList(BaseModel):\n    jobs: List[Job]\n\n\nclass RankedJob(BaseModel):\n    job: Job\n    match_score: int\n    reason: str\n\n\nclass RankedJobList(BaseModel):\n    ranked_jobs: List[RankedJob]\n\n\nclass ChosenJob(BaseModel):\n    job: Job\n    selected: bool\n    reason: str\n",
      "tools.py": "import os, re\n\nfrom crewai.tools import tool\nfrom firecrawl import FirecrawlApp, ScrapeOptions\n\n\n@tool\ndef web_search_tool(query: str):\n    \"\"\"Search the web for job listings and scrape full page content\"\"\"\n    app = FirecrawlApp(api_key=os.getenv(\"FIRECRAWL_API_KEY\"))\n\n    response = app.search(\n        query=query,\n        limit=5,\n        scrape_options=ScrapeOptions(\n            formats=[\"markdown\"],\n        ),\n    )\n\n    if not response.success:\n        return \"Error using tool.\"\n\n    cleaned_chunks = []\n\n    for result in response.data:\n\n        title = result[\"title\"]\n        url = result[\"url\"]\n        markdown = result[\"markdown\"]\n\n        # Clean markdown: remove backslashes and newlines\n        cleaned = re.sub(r\"\\\\+|\\n+\", \"\", markdown).strip()\n        # Remove markdown links and URLs\n        cleaned = re.sub(r\"\\[[^\\]]+\\]\\([^\\)]+\\)|https?://[^\\s]+\", \"\", cleaned)\n\n        cleaned_result = {\n            \"title\": title,\n            \"url\": url,\n            \"markdown\": cleaned,\n        }\n\n        cleaned_chunks.append(cleaned_result)\n\n    return cleaned_chunks\n\n"
    },
    "assets": [],
    "techStack": [
      "CrewAI",
      "Gemini",
      "Google ADK",
      "Python"
    ]
  },
  {
    "id": "04",
    "title": "Content Pipeline Agent",
    "description": "ìë™í™”ëœ ì½˜í…ì¸  ìƒì„± ë° ë°°í¬ íŒŒì´í”„ë¼ì¸",
    "directory": "04_content-pipeline-agent",
    "readmePath": "04_content-pipeline-agent/README.md",
    "mainCodePath": "04_content-pipeline-agent/main.py",
    "readme": "",
    "codes": {
      "main.py": "from typing import List\nimport json\nfrom crewai.flow.flow import Flow, listen, start, router, or_\nfrom crewai import LLM\nfrom pydantic import BaseModel\n# from tools import web_search_tool  # Removed as it's now used inside ResearcherCrew\nfrom crews.seo_crew import SeoCrew\nfrom crews.virality_crew import ViralityCrew\nfrom crews.researcher_crew import ResearcherCrew\n\n\nclass BlogPost(BaseModel):\n    title: str\n    subtitle: str\n    sections: List[str]\n\n\nclass Tweet(BaseModel):\n    content: str\n    hashtags: str\n\n\nclass LinkedInPost(BaseModel):\n    hook: str\n    content: str\n    call_to_action: str\n\n\nclass Score(BaseModel):\n    score: int = 0\n    reason: str = \"\"\n\n\nclass ContentPipelineState(BaseModel):\n    # Inputs\n    content_type: str = \"\"\n    topic: str = \"\"\n\n    # Internal\n    max_length: int = 0\n    research: str = \"\"\n    score: Score | str | None = None\n    retry_count: int = 0\n\n    # Content\n    blog_post: BlogPost | str | None = None\n    tweet: Tweet | None = None\n    linkedin_post: LinkedInPost | None = None\n\n\nclass ContentPipelineFlow(Flow[ContentPipelineState]):\n\n    @start()\n    def init_content_pipeline(self):\n        if self.state.content_type not in [\"tweet\", \"blog\", \"linkedin\"]:\n            raise ValueError(\"The content type is wrong.\")\n\n        if self.state.topic == \"\":\n            raise ValueError(\"The topic can't be blank.\")\n\n        if self.state.content_type == \"tweet\":\n            self.state.max_length = 150\n        elif self.state.content_type == \"blog\":\n            self.state.max_length = 800\n        elif self.state.content_type == \"linkedin\":\n            self.state.max_length = 500\n\n    @listen(init_content_pipeline)\n    def conduct_research(self):\n        print(\"Researching....\")\n        \n        crew = ResearcherCrew()\n        result = crew.crew().kickoff(\n            inputs={\n                \"topic\": self.state.topic\n            }\n        )\n        \n        # Store the research result (string)\n        self.state.research = result.raw\n        return result\n\n    @router(conduct_research)\n    def conduct_research_router(self):\n        content_type = self.state.content_type\n\n        if content_type == \"blog\":\n            return \"make_blog\"\n        elif content_type == \"tweet\":\n            return \"make_tweet\"\n        else:\n            return \"make_linkedin_post\"\n\n    @listen(or_(\"make_blog\", \"remake_blog\"))\n    def handle_make_blog(self):\n        print(\"Making blog post...\")\n        llm = LLM(\n            model=\"gemini/gemini-2.5-flash-preview-09-2025\",\n            temperature=0.7,\n        )\n        if self.state.blog_post is None or self.state.score is None:\n            result = llm.call(\n                f\"\"\"\n                Make a blog post with SEO practices on the topic {self.state.topic} using the following research:\n\n                <research>\n                ================\n                {self.state.research}\n                ================\n                </research>\n                \"\"\",\n                response_model=BlogPost,\n            )\n        else:\n            print(\"Remaking blog post...\")\n            \n            # Handle case where blog_post might be a string\n            blog_content = self.state.blog_post\n            if hasattr(self.state.blog_post, 'model_dump_json'):\n                blog_content = self.state.blog_post.model_dump_json()\n            \n            # Handle case where score might be a string or None\n            score_reason = \"\"\n            if hasattr(self.state.score, 'reason'):\n                 score_reason = self.state.score.reason\n            elif isinstance(self.state.score, str):\n                 try:\n                     score_data = json.loads(self.state.score)\n                     score_reason = score_data.get('reason', 'Unknown reason')\n                 except Exception:\n                     score_reason = \"Unknown reason\"\n\n            result = llm.call(\n                f\"\"\"\n                You wrote this blog post on {self.state.topic}, but it does not have a good SEO score because of {score_reason}\n\n                Improve it.\n\n                <blog post>\n                {blog_content}\n                </blog post>\n\n                Use the following research.\n\n                <research>\n                ================\n                {self.state.research}\n                ================\n                </research>\n                \"\"\",\n                response_model=BlogPost,\n            )\n            \n        self.state.blog_post = result\n\n    @listen(or_(\"make_tweet\", \"remake_tweet\"))\n    def handle_make_tweet(self):\n        print(\"Making tweet...\")\n        llm = LLM(\n            model=\"gemini/gemini-2.5-flash-preview-09-2025\",\n            temperature=0.7,\n        )\n        self.state.tweet = llm.call(\n            f\"\"\"\n            Write a tweet about {self.state.topic}.\n            \n            Use the following research.\n\n            <research>\n            ================\n            {self.state.research}\n            ================\n            </research>\n            \"\"\",\n            response_model=Tweet,\n        )\n\n    @listen(or_(\"make_linkedin_post\", \"remake_linkedin_post\"))\n    def handle_make_linkedin_post(self):\n        print(\"Making linkedin post...\")\n        llm = LLM(\n            model=\"gemini/gemini-2.5-flash-preview-09-2025\",\n            temperature=0.7,\n        )\n        self.state.linkedin_post = llm.call(\n            f\"\"\"\n            Write a linkedin post about {self.state.topic}.\n            \n            Use the following research.\n\n            <research>\n            ================\n            {self.state.research}\n            ================\n            </research>\n            \"\"\",\n            response_model=LinkedInPost,\n        )\n\n    @listen(handle_make_blog)\n    def check_seo(self):\n        print(\"Checking Blog SEO\")\n        \n        # Handle blog_post serialization safely\n        blog_content = self.state.blog_post\n        if hasattr(self.state.blog_post, 'model_dump_json'):\n            blog_content = self.state.blog_post.model_dump_json()\n            \n        crew = SeoCrew()\n        result = crew.crew().kickoff(\n            inputs={\n                \"blog_post\": blog_content,\n                \"topic\": self.state.topic,\n            }\n        )\n        # The result from crew.kickoff() with output_pydantic is usually the Pydantic object directly\n        # or we might need to access .pydantic or .raw depending on version\n        # Assuming it returns the Score object directly as per recent CrewAI updates\n        self.state.score = result.pydantic or result \n        \n        # Safe access for print\n        current_score = 0\n        current_reason = \"\"\n        if hasattr(self.state.score, 'score'):\n            current_score = self.state.score.score\n            current_reason = self.state.score.reason\n        elif isinstance(self.state.score, dict):\n            current_score = self.state.score.get('score')\n            current_reason = self.state.score.get('reason')\n            \n        print(f\"Score: {current_score}, Reason: {current_reason}\")\n\n    @listen(or_(handle_make_tweet, handle_make_linkedin_post))\n    def check_virality(self):\n        print(\"Checking virality...\")\n        \n        content = \"\"\n        if self.state.content_type == \"tweet\":\n            content = self.state.tweet\n        else:\n            content = self.state.linkedin_post\n            \n        if hasattr(content, 'model_dump_json'):\n            content = content.model_dump_json()\n            \n        crew = ViralityCrew()\n        result = crew.crew().kickoff(\n            inputs={\n                \"content\": content,\n                \"content_type\": self.state.content_type,\n                \"topic\": self.state.topic,\n            }\n        )\n        self.state.score = result.pydantic or result\n        \n        # Safe access for print\n        current_score = 0\n        current_reason = \"\"\n        if hasattr(self.state.score, 'score'):\n            current_score = self.state.score.score\n            current_reason = self.state.score.reason\n            \n        print(f\"Score: {current_score}, Reason: {current_reason}\")\n\n    @router(or_(check_seo, check_virality))\n    def score_router(self):\n\n        content_type = self.state.content_type\n        score = self.state.score\n        \n        # Safety check\n        current_score = 0\n        if hasattr(score, 'score'):\n            current_score = score.score\n        elif isinstance(score, dict):\n            current_score = score.get('score', 0)\n\n        if score is not None and current_score >= 8:\n            return \"check_passed\"\n        \n        if self.state.retry_count >= 3:\n             print(\"Max retries reached. Passing content anyway.\")\n             return \"check_passed\"\n        \n        self.state.retry_count += 1\n        print(f\"Score is low ({current_score}). Retrying... (Attempt {self.state.retry_count})\")\n\n        if content_type == \"blog\":\n            return \"remake_blog\"\n        elif content_type == \"linkedin\":\n            return \"remake_linkedin_post\"\n        else:\n            return \"remake_tweet\"\n\n    @listen(\"check_passed\")\n    def finalize_content(self):\n        print(\"Finalizing content\")\n        print(self.state.blog_post)\n\n\nflow = ContentPipelineFlow()\n\nflow.kickoff(\n    inputs={\n        \"content_type\": \"blog\",\n        \"topic\": \"AI Dog Training\",\n    },\n)\n\n# flow.plot()\n",
      "tools.py": "import os\nimport re\n\nfrom crewai.tools import tool\nfrom firecrawl import FirecrawlApp, ScrapeOptions\n\n\n@tool\ndef web_search_tool(query: str):\n    \"\"\"\n    Web Search Tool.\n    Args:\n        query: str\n            The query to search the web for.\n    Returns\n        A list of search results with the website content in Markdown format.\n    \"\"\"\n    app = FirecrawlApp(api_key=os.getenv(\"FIRECRAWL_API_KEY\"))\n\n    response = app.search(\n        query=query,\n        limit=5,\n        scrape_options=ScrapeOptions(\n            formats=[\"markdown\"],\n        ),\n    )\n\n    if not response.success:\n        return \"Error using tool.\"\n\n    cleaned_chunks = []\n\n    for result in response.data:\n\n        title = result[\"title\"]\n        url = result[\"url\"]\n        markdown = result[\"markdown\"]\n\n        cleaned = re.sub(r\"\\\\+|\\n+\", \"\", markdown).strip()\n        cleaned = re.sub(r\"\\[[^\\]]+\\]\\([^\\)]+\\)|https?://[^\\s]+\", \"\", cleaned)\n\n        cleaned_result = {\n            \"title\": title,\n            \"url\": url,\n            \"markdown\": cleaned,\n        }\n\n        cleaned_chunks.append(cleaned_result)\n\n    return cleaned_chunks\n"
    },
    "assets": [],
    "techStack": [
      "Python"
    ]
  },
  {
    "id": "05",
    "title": "Deep Research Clone",
    "description": "AutoGen ê¸°ë°˜ì˜ ì‹¬ì¸µ ì›¹ ë¦¬ì„œì¹˜ ë©€í‹° ì—ì´ì „íŠ¸ íŒ€",
    "directory": "05_deep-research-clone",
    "readmePath": "05_deep-research-clone/README.md",
    "mainCodePath": "05_deep-research-clone/main.py",
    "readme": "# Deep Research Clone\n\nThis project implements multi-agent AI teams using **AutoGen** and **Google Gemini** models. It features a \"Deep Research Team\" for conducting comprehensive web research and an \"Email Optimizer Team\" for refining communications.\n\n## Features\n\n- **Deep Research Team**: A squad of agents (Planner, Researcher, Analyst, Reviewer) that autonomously plans research, searches the web, and compiles detailed reports.\n- **Email Optimizer Team**: A team of agents (Clarity, Tone, Persuasion, Critic) that collaborates to improve email drafts.\n- **Google Gemini Integration**: Powered by `gemini-2.5-flash` for high-performance and cost-effective reasoning.\n- **Web Search**: Integrated with **Firecrawl** for advanced web scraping and search capabilities.\n- **Report Generation**: Automatically saves research findings to Markdown files.\n\n## Prerequisites\n\n- **Python**: >= 3.13\n- **Package Manager**: [uv](https://github.com/astral-sh/uv) (recommended)\n- **API Keys**:\n  - [Google Gemini API Key](https://aistudio.google.com/)\n  - [Firecrawl API Key](https://firecrawl.dev/)\n\n## Installation\n\n1. **Clone the repository:**\n   ```bash\n   git clone <repository-url>\n   cd deep-research-clone\n   ```\n\n2. **Install dependencies using `uv`:**\n   ```bash\n   uv sync\n   ```\n   Or using pip:\n   ```bash\n   pip install .\n   ```\n\n## Configuration\n\n1. Create a `.env` file in the root directory:\n   ```bash\n   touch .env\n   ```\n\n2. Add your API keys to `.env`:\n   ```env\n   GEMINI_API_KEY=your_gemini_api_key_here\n   FIRECRAWL_API_KEY=your_firecrawl_api_key_here\n   ```\n\n## Usage\n\n### Deep Research Team\n1. Open `deep-research-team.ipynb` in Jupyter Lab or VS Code.\n2. Run the cells to initialize the agents.\n3. The team will execute the research task defined in the last cell (default: \"Research about the new development in Nuclear Energy\").\n4. The final report will be saved to `report.md`.\n\n### Email Optimizer Team\n1. Open `email-optimizer-team.ipynb`.\n2. Run the cells to start the email optimization process.\n\n## Project Structure\n\n- `deep-research-team.ipynb`: Main notebook for the research agent team.\n- `email-optimizer-team.ipynb`: Notebook for the email optimization team.\n- `tools.py`: Custom tools for web search and file operations.\n- `report.md`: Output file for research reports.\n- `.project_rules.md`: Project-specific rules and conventions (e.g., commit messages).\n\n## License\n\n[Add License Here]\n",
    "codes": {
      "deep-research-team.ipynb": "from autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.agents import AssistantAgent, UserProxyAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.ui import Console\nfrom tools import web_search_tool, save_report_to_md\nimport os\n\n# ---\n\nmodel_client = OpenAIChatCompletionClient(\n    # model=\"gemini-2.5-flash\",\n    model=\"gemini-2.5-flash-preview-09-2025\",\n    api_key=os.getenv(\"GEMINI_API_KEY\"),\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n    max_retries=10,\n    model_info={\n        \"vision\": True,\n        \"function_calling\": True,\n        \"json_output\": True,\n        \"structured_output\": True,\n        \"family\": \"unknown\",\n    },\n)\n\n# ---\n\nresearch_planner = AssistantAgent(\n    \"research_planner\",\n    description=\"A strategic research coordinator that breaks down complex questions into research subtasks\",\n    model_client=model_client,\n    system_message=\"\"\"You are a research planning specialist. Your job is to create a focused research plan.\n\n    For each research question, create a FOCUSED research plan with:\n\n    1. **Core Topics**: 2-3 main areas to investigate\n    2. **Search Queries**: Create 3-5 specific search queries covering:\n    - Latest developments and news\n    - Key statistics or data\n    - Expert analysis or studies\n    - Future outlook\n\n    Keep the plan focused and achievable. Quality over quantity.\"\"\",\n)\n\nresearch_agent = AssistantAgent(\n    \"research_agent\",\n    description=\"A web research specialist that searches and extracts content\",\n    tools=[web_search_tool],\n    model_client=model_client,\n    system_message=\"\"\"You are a web research specialist. Your job is to conduct focused searches based on the research plan.\n\n    RESEARCH STRATEGY:\n    1. **Execute 3-5 searches** from the research plan\n    2. **Extract key information** from the results:\n    - Main facts and statistics\n    - Recent developments\n    - Expert opinions\n    - Important context\n\n    3. **Quality focus**:\n    - Prioritize authoritative sources\n    - Look for recent information (within 2 years)\n    - Note diverse perspectives\n\n    After completing the searches from the plan, summarize what you found. Your goal is to gather 5-10 quality sources.\"\"\",\n)\n\nresearch_analyst = AssistantAgent(\n    \"research_analyst\",\n    description=\"An expert analyst that creates research reports\",\n    model_client=model_client,\n    system_message=\"\"\"You are a research analyst. Create a comprehensive report from the gathered research.\n\n    CREATE A RESEARCH REPORT with:\n\n    ## Executive Summary\n    - Key findings and conclusions\n    - Main insights\n\n    ## Background & Current State\n    - Current landscape\n    - Recent developments\n    - Key statistics and data\n\n    ## Analysis & Insights\n    - Main trends\n    - Different perspectives\n    - Expert opinions\n\n    ## Future Outlook\n    - Emerging trends\n    - Predictions\n    - Implications\n\n    ## Sources\n    - List all sources used\n\n    Write a clear, well-structured report based on the research gathered. End with \"REPORT_COMPLETE\" when finished.\"\"\",\n)\n\nquality_reviewer = AssistantAgent(\n    \"quality_reviewer\",\n    description=\"A quality assurance specialist that evaluates research completeness and accuracy\",\n    tools=[save_report_to_md],\n    model_client=model_client,\n    system_message=\"\"\"You are a quality reviewer. Your job is to check if the research analyst has produced a complete research report.\n\n    Look for:\n    - A comprehensive research report from the research analyst that ends with \"REPORT_COMPLETE\"\n    - The research question is fully answered\n    - Sources are cited and reliable\n    - The report includes summary, key information, analysis, and sources\n\n    When you see a complete research report that ends with \"REPORT_COMPLETE\":\n    1. First, use the save_report_to_md tool to save the report to report.md\n    2. Then say: \"The research is complete. The report has been saved to report.md. Please review the report and let me know if you approve it or need additional research.\"\n\n    If the research analyst has NOT yet created a complete report, tell them to create one now.\"\"\",\n)\n\nresearch_enhancer = AssistantAgent(\n    \"research_enhancer\",\n    description=\"A specialist that identifies critical gaps only\",\n    model_client=model_client,\n    system_message=\"\"\"You are a research enhancement specialist. Your job is to identify ONLY CRITICAL gaps.\n\n    Review the research and ONLY suggest additional searches if there are MAJOR gaps like:\n    - Completely missing recent developments (last 6 months)\n    - No statistics or data at all\n    - Missing a crucial perspective that was specifically asked for\n\n    If the research covers the basics reasonably well, say: \"The research is sufficient to proceed with the report.\"\n\n    Only suggest 1-2 additional searches if absolutely necessary. We prioritize getting a good report done rather than perfect coverage.\"\"\",\n)\n\nuser_proxy = UserProxyAgent(\n    \"user_proxy\",\n    description=\"Human reviewer who can request additional research or approve final results\",\n    input_func=input,\n)\n\n\n# ---\n\nselector_prompt = \"\"\"\nChoose the best agent for the current task based on the conversation history:\n\n{roles}\n\nCurrent conversation:\n{history}\n\nAvailable agents:\n- research_planner: Plan the research approach (ONLY at the start)\n- research_agent: Search for and extract content from web sources (after planning)\n- research_enhancer: Identify CRITICAL gaps only (use sparingly)\n- research_analyst: Write the final research report\n- quality_reviewer: Check if a complete report exists\n- user_proxy: Ask the human for feedback\n\nWORKFLOW:\n1. If no planning done yet â†’ select research_planner\n2. If planning done but no research â†’ select research_agent  \n3. After research_agent completes initial searches â†’ select research_enhancer ONCE\n4. If enhancer says \"sufficient to proceed\" â†’ select research_analyst\n5. If enhancer suggests critical searches â†’ select research_agent ONCE more then research_analyst\n6. If research_analyst said \"REPORT_COMPLETE\" â†’ select quality_reviewer\n7. If quality_reviewer asked for user feedback â†’ select user_proxy\n\nIMPORTANT: After research_agent has searched 2 times maximum, proceed to research_analyst regardless.\n\nPick the agent that should work next based on this workflow.\"\"\"\n\n\n# ---\n\ntext_termination = TextMentionTermination(\"APPROVED\")\nmax_message_termination = MaxMessageTermination(max_messages=10)\ntermination_condition = text_termination | max_message_termination\n\nteam = SelectorGroupChat(\n    participants=[\n        research_agent,\n        research_analyst,\n        research_enhancer,\n        research_planner,\n        quality_reviewer,\n        user_proxy,\n    ],\n    selector_prompt=selector_prompt,\n    model_client=model_client,\n    # allow_repeated_speaker=True,\n    termination_condition=termination_condition,\n)\n\n\n# ---\n\nawait Console(\n    team.run_stream(task=\"Research about the best web3 blockchain\"),\n)",
      "email-optimizer-team.ipynb": "from autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.ui import Console\nimport os\n\n# ---\n\n# Configure Gemini using OpenAI compatible client\nmodel = OpenAIChatCompletionClient(\n    model=\"gemini-2.5-flash\",\n    api_key=os.getenv(\"GEMINI_API_KEY\"),\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n    max_retries=10,\n    model_info={\n        \"vision\": True,\n        \"function_calling\": True,\n        \"json_output\": True,\n        \"family\": \"unknown\",\n    },\n)\n\nclarity_agent = AssistantAgent(\n    \"ClarityAgent\",\n    model_client=model,\n    system_message=\"\"\"You are an expert editor focused on clarity and simplicity. \n    Your job is to eliminate ambiguity, redundancy, and make every sentence crisp and clear. \n    Don't worry about persuasion or tone â€” just make the message easy to read and understand.\"\"\",\n)\n\ntone_agent = AssistantAgent(\n    \"ToneAgent\",\n    model_client=model,\n    system_message=\"\"\"You are a communication coach focused on emotional tone and professionalism. \n    Your job is to make the email sound warm, confident, and human â€” while staying professional \n    and appropriate for the audience. Improve the emotional resonance, polish the phrasing, \n    and adjust any words that may come off as stiff, cold, or overly casual.\"\"\",\n)\n\npersuasion_agent = AssistantAgent(\n    \"PersuasionAgent\",\n    model_client=model,\n    system_message=\"\"\"You are a persuasion expert trained in marketing, behavioral psychology, \n    and copywriting. Your job is to enhance the email's persuasive power: improve call to action, structure arguments, and emphasize benefits. Remove weak or passive language.\"\"\",\n)\n\nsynthesizer_agent = AssistantAgent(\n    \"SynthesizerAgent\",\n    model_client=model,\n    system_message=\"\"\"You are an advanced email-writing specialist. Your role is to read all \n    prior agent responses and revisions, and then **synthesize the best ideas** into a unified, \n    polished draft of the email. Focus on: Integrating clarity, tone, and persuasion improvements; \n    Ensuring coherence, fluency, and a natural voice; Creating a version that feels professional, \n    effective, and readable.\"\"\",\n)\n\ncritic_agent = AssistantAgent(\n    \"CriticAgent\",\n    model_client=model,\n    system_message=\"\"\"You are an email quality evaluator. Your job is to perform a final review \n    of the synthesized email. Check for any remaining errors, awkward phrasing, or inconsistencies. \n    If the email is perfect, type 'APPROVE'. If not, provide specific feedback for improvement.\"\"\",\n)\n\n# ---\n\ntermination_condition = MaxMessageTermination(max_messages=10) | TextMentionTermination(\"APPROVE\")\n\nteam = RoundRobinGroupChat(\n    participants=[\n        clarity_agent,\n        tone_agent,\n        persuasion_agent,\n        synthesizer_agent,\n        critic_agent,\n    ],\n    termination_condition=termination_condition,\n)\n\nawait Console(\n    team.run_stream(\n        task=\"Hi! Im hungry, buy me lunch and invest in my business. Thanks.\"\n    )\n)",
      "main.py": "def main():\n    print(\"Hello from 5-deep-research-clone!\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "tools.py": "import os, re\nimport datetime\nfrom firecrawl import FirecrawlApp, ScrapeOptions\n\n\ndef web_search_tool(query: str):\n    \"\"\"\n    Web Search Tool.\n    Args:\n        query: str\n        The query to search the web for.\n    Returns\n        A list of search results with the website content in Markdown format.\n    \"\"\"\n    app = FirecrawlApp(api_key=os.getenv(\"FIRECRAWL_API_KEY\"))\n\n    response = app.search(\n        query=query,\n        limit=2,\n        scrape_options=ScrapeOptions(\n            formats=[\"markdown\"],\n        ),\n    )\n\n    if not response.success:\n        return \"Error using tool.\"\n\n    cleaned_chunks = []\n\n    for result in response.data:\n        title = result[\"title\"]\n        url = result[\"url\"]\n        markdown = result[\"markdown\"]\n\n        cleaned = re.sub(r\"\\\\+|\\n+\", \"\", markdown).strip()\n        cleaned = re.sub(r\"\\[[^\\]]+\\]\\([^\\)]+\\)|https?://[^\\s]+\", \"\", cleaned)\n\n        cleaned_result = {\n            \"title\": title,\n            \"url\": url,\n            \"markdown\": cleaned,\n        }\n\n        cleaned_chunks.append(cleaned_result)\n\n    return cleaned_chunks\n\n\ndef save_report_to_md(content: str) -> str:\n    \"\"\"Save report content to report.md file.\"\"\"\n    with open(\"report.md\", \"w\") as f:\n        f.write(content)\n    return \"report.md\"\n"
    },
    "assets": [],
    "techStack": [
      "Gemini",
      "AutoGen",
      "Python"
    ]
  },
  {
    "id": "06",
    "title": "Chatgpt Clone",
    "description": "ë‹¤ê¸°ëŠ¥ ë„êµ¬ë¥¼ ê°–ì¶˜ Streamlit ê¸°ë°˜ AI ë¹„ì„œ",
    "directory": "06_chatgpt-clone",
    "readmePath": "06_chatgpt-clone/README.md",
    "mainCodePath": "06_chatgpt-clone/main.py",
    "readme": "# AI Agent Masterclass: ChatGPT Clone\n\nA powerful AI assistant built with Streamlit, capable of Web Search, File Search, Code Execution, Image Generation, and MCP (Model Context Protocol) integration.\n\n## ğŸŒŸ Features\n\n- **Advanced Chat Interface**: Built with Streamlit for a smooth user experience.\n- **Multi-Modal Capabilities**:\n  - **Web Search**: Real-time internet search using DuckDuckGo.\n  - **File Search**: RAG (Retrieval-Augmented Generation) for uploaded documents.\n  - **Code Interpreter**: Writes and executes Python code in a sandboxed environment.\n  - **Image Generation**: Creates images using DALL-E 3.\n- **MCP Integration**:\n  - **Hosted MCP Tool (Client)**: Connects to external MCP servers (e.g., Context7) to extend knowledge.\n  - **Local MCP Server (Server)**: Exposes the agent's own tools to other MCP-compliant clients (like Claude Desktop).\n- **Session Management**: Persistent chat history with SQLite and session reset functionality.\n\n## ğŸ› ï¸ Prerequisites\n\n- Python 3.10+\n- [uv](https://github.com/astral-sh/uv) (for dependency management)\n- OpenAI API Key\n\n## ğŸš€ Installation\n\n1. **Clone the repository**\n   ```bash\n   git clone <repository-url>\n   cd chatgpt-clone\n   ```\n\n2. **Install dependencies**\n   ```bash\n   uv sync\n   ```\n\n3. **Configure Environment**\n   Create a `.env` file in the root directory:\n   ```env\n   OPENAI_API_KEY=sk-...\n   VECTOR_STORE_ID=vs_...  # Optional: For persistent file search\n   ```\n\n## ğŸ’» Usage\n\nThis application supports two distinct modes of operation.\n\n### 1. Web App Mode (Client)\nRun the interactive chat interface in your browser.\n\n```bash\nstreamlit run main.py\n```\n\n- **Functionality**: Chat with the agent, upload files, generate images, and run code.\n- **MCP Client**: The agent can connect to external MCP servers defined in the configuration.\n\n### 2. MCP Server Mode (Server)\nRun the agent as a local MCP server. This allows other applications to use this agent's tools.\n\n```bash\npython main.py mcp\n```\n\n- **Functionality**: Runs headlessly and communicates via Stdio.\n- **Integration**: Configure your MCP client (e.g., Claude Desktop) to run this command to access tools like Web Search and Code Interpreter.\n\n## ğŸ“‚ Project Structure\n\n- `main.py`: The entry point for both the Streamlit app and the MCP server.\n- `agents/`: Core logic for the AI agent, tools, and session management.\n- `chat-gpt-clone-memory-new.db`: SQLite database for storing chat history.\n- `.env`: Configuration file for API keys.\n\n## â“ Troubleshooting\n\n**\"Container is expired\" Error**\nIf you encounter this error, it means the OpenAI Code Interpreter session has timed out.\n- **Fix**: Click the **\"Reset memory\"** button in the sidebar to clear the session and start fresh.\n",
    "codes": {
      "dummy-agent.ipynb": "from agents import Agent, Runner, function_tool, SQLiteSession\n\nsession = SQLiteSession(\"user_1\", \"ai-memory.db\")\n\n\n@function_tool\ndef get_weather(city: str):\n    \"\"\"Get weather by city\"\"\"\n    return \"30 degrees\"\n\n\nagent = Agent(\n    name=\"Assistant Agent\",\n    instructions=\"You are a helpful assistant. Use tools when needed to answer questions\",\n    tools=[get_weather],\n)\n\nstream = Runner.run_streamed(\n    agent, \"Hello how are you? What is the weather in the capital of Spain?\"\n)\n\nasync for event in stream.stream_events():\n    if event.type == \"raw_response_event\":\n        continue\n    elif event.type == \"agent_updated_stream_event\":\n        print(\"Agent updated to\", event.new_agent.name)\n    elif event.type == \"run_item_stream_event\":\n        print(event.item.type)\n        print(\"=\" * 20)\n\n# ---\n\nresult = await Runner.run(\n    agent,\n    \"What was my name again?\",\n    session=session,\n)\nresult\n\n# ---\n\nfrom agents import Agent, Runner, function_tool, ItemHelpers\n\n\n@function_tool\ndef get_weather(city: str):\n    \"\"\"Get weather by city\"\"\"\n    return \"30 degrees\"\n\nagent = Agent(\n    name=\"Assistant Agent\",\n    instructions=\"You are a helpful assistant. Use tools when needed to answer questions\",\n    tools=[get_weather],\n)\n\nstream = Runner.run_streamed(\n    agent, \"Hello how are you? What is the weather in the capital of Spain?\"\n)\n\nasync for event in stream.stream_events():\n    if event.type == \"raw_response_event\":\n        print(event.data.type)\n    print(\"=\"*20)\n\n# ---\n\nfrom agents import Agent, Runner, function_tool, ItemHelpers\n\n\n@function_tool\ndef get_weather(city: str):\n    \"\"\"Get weather by city\"\"\"\n    return \"30 degrees\"\n\n\nagent = Agent(\n    name=\"Assistant Agent\",\n    instructions=\"You are a helpful assistant. Use tools when needed to answer questions\",\n    tools=[get_weather],\n)\n\nstream = Runner.run_streamed(\n    agent, \"Hello how are you? What is the weather in the capital of Spain?\"\n)\n\nmessage = \"\"\nargs = \"\"\n\nasync for event in stream.stream_events():\n\n    if event.type == \"raw_response_event\":\n        event_type = event.data.type\n        if event_type == \"response.output_text.delta\":\n            message += event.data.delta\n            print(message)\n        elif event_type == \"response.function_call_arguments.delta\":\n            args += event.data.delta\n            print(args)\n        elif event_type == \"response.completed\":\n            message = \"\"\n            args = \"\"",
      "handoff-agent.ipynb": "from agents import Agent, Runner, SQLiteSession\n\nsession = SQLiteSession(\"user_1\", \"ai-memory.db\")\n\n\ngeaography_agent = Agent(\n    name=\"Geo Expert Agent\",\n    instructions=\"You are a expert in geography, you answer questions related to them.\",\n    handoff_description=\"Use this to answer geography related questions.\",\n)\neconomics_agent = Agent(\n    name=\"Economics Expert Agent\",\n    instructions=\"You are a expert in economics, you answer questions related to them.\",\n    handoff_description=\"Use this to answer economics questions.\",\n)\n\nmain_agent = Agent(\n    name=\"Main Agent\",\n    instructions=\"You are a user facing agent. Transfer to the agent most capable of answering the user's question.\",\n    handoffs=[\n        economics_agent,\n        geaography_agent,\n    ],\n)\n\n# ---\n\nresult = await Runner.run(\n    main_agent,\n    \"Why do countries sell bonds?\",\n    session=session,\n)\n\nprint(result.last_agent.name)\nprint(result.final_output)",
      "main.py": "\nimport dotenv\ndotenv.load_dotenv()\nimport time\nfrom openai import OpenAI\nimport asyncio\nimport base64\nimport streamlit as st\nfrom agents import (\n    Agent,\n    Runner,\n    SQLiteSession,\n    WebSearchTool,\n    FileSearchTool,\n    ImageGenerationTool,\n    CodeInterpreterTool,\n    HostedMCPTool,\n)\nfrom agents.mcp.server import MCPServerStdio\nimport os\nimport sys\n\nclient = OpenAI()\n\nVECTOR_STORE_ID = os.getenv(\"VECTOR_STORE_ID\")\n\nagent = Agent(\n    name=\"ChatGPT Clone\",\n    instructions=\"\"\"\n        You are a helpful assistant.\n        You have access to the followign tools:\n        - Web Search Tool: Use this when the user asks a questions that isn't in your training data. Use this tool when the users asks about current or future events, when you think you don't know the answer, try searching for it in the web first.\n        - File Search Tool: Use this tool when the user asks a question about facts related to themselves. Or when they ask questions about specific files.\n        - Code Interpreter Tool: Use this tool when you need to write and run code to answer the user's question.\n        \"\"\",\n    tools=[\n        WebSearchTool(),\n        FileSearchTool(\n            vector_store_ids=[VECTOR_STORE_ID],\n            max_num_results=3,\n        ),\n        ImageGenerationTool(\n            tool_config={\n                \"type\": \"image_generation\",\n                \"quality\": \"low\",\n                \"output_format\": \"jpeg\",\n                \"partial_images\": 1,\n            }\n        ),\n        CodeInterpreterTool(\n            tool_config={\n                \"type\": \"code_interpreter\",\n                \"container\": {\n                    \"type\": \"auto\",\n                },\n            }\n        ),\n        HostedMCPTool(\n            tool_config={\n                \"server_url\": \"https://mcp.context7.com/mcp\",\n                \"type\": \"mcp\",\n                \"server_label\": \"Context7\",\n                \"server_description\": \"Use this to get the docs from software projects.\",\n                \"require_approval\": \"never\",\n            }\n        ),\n    ],\n)\n\nif \"agent\" not in st.session_state:\n    st.session_state[\"agent\"] = agent\nagent = st.session_state[\"agent\"]\n\nif \"session\" not in st.session_state:\n    st.session_state[\"session\"] = SQLiteSession(\n        \"chat-history\",\n        \"chat-gpt-clone-memory-new.db\",\n    )\nsession = st.session_state[\"session\"]\n\nasync def paint_history():\n    messages = await session.get_items()\n    for message in messages:\n        if \"role\" in message:\n            with st.chat_message(message[\"role\"]):\n                if message[\"role\"] == \"user\":\n                    content = message[\"content\"]\n                    if isinstance(content, str):\n                        st.write(content)\n                    elif isinstance(content, list):\n                        for part in content:\n                            if \"image_url\" in part:\n                                st.image(part[\"image_url\"])\n                            elif \"text\" in part:\n                                st.write(part[\"text\"])\n                else:\n                    if message[\"type\"] == \"message\":\n                        st.write(message[\"content\"][0][\"text\"].replace(\"$\", \"\\\\$\"))\n        if \"type\" in message:\n            message_type = message[\"type\"]\n            if message_type == \"web_search_call\":\n                with st.chat_message(\"ai\"):\n                    st.write(\"ğŸ” Searched the web...\")\n            elif message_type == \"file_search_call\":\n                with st.chat_message(\"ai\"):\n                    st.write(\"ğŸ—‚ï¸ Searched your files...\")\n            elif message_type == \"image_generation_call\":\n                image = base64.b64decode(message[\"result\"])\n                with st.chat_message(\"ai\"):\n                    st.image(image)\n            elif message_type == \"code_interpreter_call\":\n                with st.chat_message(\"ai\"):\n                    st.code(message[\"code\"])\n            elif message_type == \"mcp_list_tools\":\n                with st.chat_message(\"ai\"):\n                    st.write(f\"Listed {message['server_label']}'s tools\")\n            elif message_type == \"mcp_call\":\n                with st.chat_message(\"ai\"):\n                    st.write(\n                        f\"Called {message['server_label']}'s {message['name']} with args {message['arguments']}\"\n                    )\n\ndef update_status(status_container, event):\n    status_messages = {\n        \"response.web_search_call.completed\": (\"âœ… Web search completed.\", \"complete\"),\n        \"response.web_search_call.in_progress\": (\"ğŸ” Starting web search...\", \"running\"),\n        \"response.web_search_call.searching\": (\"ğŸ” Web search in progress...\", \"running\"),\n        \"response.file_search_call.completed\": (\"âœ… File search completed.\", \"complete\"),\n        \"response.file_search_call.in_progress\": (\"ğŸ—‚ï¸ Starting file search...\", \"running\"),\n        \"response.file_search_call.searching\": (\"ğŸ—‚ï¸ File search in progress...\", \"running\"),\n        \"response.image_generation_call.generating\": (\"ğŸ¨ Drawing image...\", \"running\"),\n        \"response.image_generation_call.in_progress\": (\"ğŸ¨ Drawing image...\", \"running\"),\n        \"response.code_interpreter_call_code.done\": (\"ğŸ¤– Ran code.\", \"complete\"),\n        \"response.code_interpreter_call.completed\": (\"ğŸ¤– Ran code.\", \"complete\"),\n        \"response.code_interpreter_call.in_progress\": (\"ğŸ¤– Running code...\", \"complete\"),\n        \"response.code_interpreter_call.interpreting\": (\"ğŸ¤– Running code...\", \"complete\"),\n        \"response.mcp_call.completed\": (\"âš’ï¸ Called MCP tool\", \"complete\"),\n        \"response.mcp_call.failed\": (\"âš’ï¸ Error calling MCP tool\", \"complete\"),\n        \"response.mcp_call.in_progress\": (\"âš’ï¸ Calling MCP tool...\", \"running\"),\n        \"response.mcp_list_tools.completed\": (\"âš’ï¸ Listed MCP tools\", \"complete\"),\n        \"response.mcp_list_tools.failed\": (\"âš’ï¸ Error listing MCP tools\", \"complete\"),\n        \"response.mcp_list_tools.in_progress\": (\"âš’ï¸ Listing MCP tools\", \"running\"),\n        \"response.completed\": (\" \", \"complete\"),\n    }\n\n    if event in status_messages:\n        label, state = status_messages[event]\n        status_container.update(label=label, state=state)\n\nasyncio.run(paint_history())\n\nasync def run_agent(message):\n    with st.chat_message(\"ai\"):\n        status_container = st.status(\"â³\", expanded=False)\n        code_placeholder = st.empty()\n        image_placeholder = st.empty()\n        text_placeholder = st.empty()\n        response = \"\"\n        code_response = \"\"\n\n        st.session_state[\"code_placeholder\"] = code_placeholder\n        st.session_state[\"image_placeholder\"] = image_placeholder\n        st.session_state[\"text_placeholder\"] = text_placeholder\n        \n        mcp_tool_calls = {}\n\n        stream = Runner.run_streamed(\n            agent,\n            message,\n            session=session,\n        )\n        async for event in stream.stream_events(): \n            if event.type == \"raw_response_event\":\n                update_status(status_container, event.data.type)\n                if event.data.type == \"response.output_item.added\":\n                    if event.data.item.type == \"mcp_call\":\n                        mcp_tool_calls[event.data.item.id] = {\n                            \"name\": event.data.item.name,\n                            \"server_label\": event.data.item.server_label,\n                            \"arguments\": \"\",\n                        }\n                elif event.data.type == \"response.mcp_call_arguments.delta\":\n                    if event.data.item_id in mcp_tool_calls:\n                        mcp_tool_calls[event.data.item_id][\"arguments\"] += event.data.delta\n                        tool_info = mcp_tool_calls[event.data.item_id]\n                        status_container.update(\n                            label=f\"âš’ï¸ Calling {tool_info['server_label']}'s {tool_info['name']} with args: {tool_info['arguments']}...\",\n                            state=\"running\",\n                        )\n                elif event.data.type == \"response.output_text.delta\":\n                    response += event.data.delta\n                    text_placeholder.write(response.replace(\"$\", \"\\\\$\"))\n                elif event.data.type == \"response.code_interpreter_call_code.delta\":\n                    code_response += event.data.delta\n                    code_placeholder.code(code_response)\n                elif event.data.type == \"response.image_generation_call.partial_image\":\n                    image = base64.b64decode(event.data.partial_image_b64)\n                    image_placeholder.image(image)\n\nprompt = st.chat_input(\n    \"Write a message for your assistant\",\n    accept_file=True,\n    file_type=[\n        \"txt\",\n        \"jpg\",\n        \"jpeg\",\n        \"png\",\n    ],\n)\n\nif prompt:\n    if \"code_placeholder\" in st.session_state:\n        st.session_state[\"code_placeholder\"].empty()\n    if \"image_placeholder\" in st.session_state:\n        st.session_state[\"image_placeholder\"].empty()\n    if \"text_placeholder\" in st.session_state:\n        st.session_state[\"text_placeholder\"].empty()\n\n    for file in prompt.files:\n        if file.type.startswith(\"text/\"):\n            with st.chat_message(\"ai\"):\n                with st.status(\"â³ Uploading file...\") as status:\n                    uploaded_file = client.files.create(\n                        file=(file.name, file.getvalue()),\n                        purpose=\"user_data\",\n                    )\n                    status.update(label=\"â³ Attaching file...\")\n                    client.vector_stores.files.create(\n                        vector_store_id=VECTOR_STORE_ID,\n                        file_id=uploaded_file.id,\n                    )\n                    status.update(label=\"âœ… File uploaded\", state=\"complete\")\n        elif file.type.startswith(\"image/\"):\n            with st.status(\"â³ Uploading image...\") as status:\n                file_bytes = file.getvalue()\n                base64_data = base64.b64encode(file_bytes).decode(\"utf-8\")\n                data_uri = f\"data:{file.type};base64,{base64_data}\"\n                asyncio.run(\n                    session.add_items(\n                        [\n                            {\n                                \"role\": \"user\",\n                                \"content\": [\n                                    {\n                                        \"type\": \"input_image\",\n                                        \"detail\": \"auto\",\n                                        \"image_url\": data_uri,\n                                    }\n                                ],\n                            }\n                        ]\n                    )\n                )\n                status.update(label=\"âœ… Image uploaded\", state=\"complete\")\n            with st.chat_message(\"human\"):\n                st.image(data_uri)\n\n    if prompt.text:\n        with st.chat_message(\"human\"):\n            st.write(prompt.text)\n        asyncio.run(run_agent(prompt.text))\n\nwith st.sidebar:\n    reset = st.button(\"Reset memory\")\n    if reset:\n        asyncio.run(session.clear_session())\n    st.write(asyncio.run(session.get_items()))\n\nif __name__ == \"__main__\":\n    if len(sys.argv) > 1 and sys.argv[1] == \"mcp\":\n        MCPServerStdio(agent).run()\n",
      "session-agent.ipynb": "from agents import Agent, Runner, function_tool, SQLiteSession\n\nsession = SQLiteSession(\"user_1\", \"ai-memory.db\")\n\n\n@function_tool\ndef get_weather(city: str):\n    \"\"\"Get weather by city\"\"\"\n    return \"30 degrees\"\n\n\nagent = Agent(\n    name=\"Assistant Agent\",\n    instructions=\"You are a helpful assistant. Use tools when needed to answer questions\",\n    tools=[get_weather],\n)\n\n# ---\n\nresult = await Runner.run(\n    agent,\n    \"what was my name again?\",\n    session=session,\n)\n\nprint(result.final_output)\n\n# ---\n\n# await session.add_items([\n#   {\"role\": \"user\", \"content\": \"my name is Alice?\"},\n# ])\n\n# await session.pop_item()\n\n# await session.clear_session()\n\nawait session.get_items()",
      "trace-agent.ipynb": "from agents import Agent, Runner, SQLiteSession, function_tool, trace\nfrom agents.extensions.visualization import draw_graph\nfrom pydantic import BaseModel\n\nsession = SQLiteSession(\"user_111111\", \"ai-memory.db\")\n\n\nclass Answer(BaseModel):\n    answer: str\n    background_explanation: str\n\n\n@function_tool\ndef get_weather():\n    return \"30\"\n\n\ngeaography_agent = Agent(\n    name=\"Geo Expert Agent\",\n    instructions=\"You are a expert in geography, you answer questions related to them.\",\n    handoff_description=\"Use this to answer geography related questions.\",\n    tools=[\n        get_weather,\n    ],\n    output_type=Answer,\n)\neconomics_agent = Agent(\n    name=\"Economics Expert Agent\",\n    instructions=\"You are a expert in economics, you answer questions related to them.\",\n    handoff_description=\"Use this to answer economics questions.\",\n)\n\nmain_agent = Agent(\n    name=\"Main Agent\",\n    instructions=\"You are a user facing agent. Transfer to the agent most capable of answering the user's question.\",\n    handoffs=[\n        economics_agent,\n        geaography_agent,\n    ],\n)\n\n# draw_graph(main_agent)\n\n# ---\n\nwith trace(\"user_111111\"):\n    result = await Runner.run(\n        main_agent,\n        \"What is the capital of Colombia's northen province.\",\n        session=session,\n    )\n    result = await Runner.run(\n        main_agent,\n        \"What is the capital of Cambodia's northen province.\",\n        session=session,\n    )\n    result = await Runner.run(\n        main_agent,\n        \"What is the capital of Thailand's northen province.\",\n        session=session,\n    )",
      "visual-agent.ipynb": "from agents import Agent, Runner, SQLiteSession, function_tool\nfrom agents.extensions.visualization import draw_graph\nfrom pydantic import BaseModel\n\nsession = SQLiteSession(\"user_1\", \"ai-memory.db\")\n\n\nclass Answer(BaseModel):\n    answer: str\n    background_explanation: str\n\n\n@function_tool\ndef get_weather():\n    return \"30\"\n\n\ngeaography_agent = Agent(\n    name=\"Geo Expert Agent\",\n    instructions=\"You are a expert in geography, you answer questions related to them.\",\n    handoff_description=\"Use this to answer geography related questions.\",\n    tools=[\n        get_weather,\n    ],\n    output_type=Answer,\n)\neconomics_agent = Agent(\n    name=\"Economics Expert Agent\",\n    instructions=\"You are a expert in economics, you answer questions related to them.\",\n    handoff_description=\"Use this to answer economics questions.\",\n)\n\nmain_agent = Agent(\n    name=\"Main Agent\",\n    instructions=\"You are a user facing agent. Transfer to the agent most capable of answering the user's question.\",\n    handoffs=[\n        economics_agent,\n        geaography_agent,\n    ],\n)\n\ndraw_graph(main_agent)\n\n# ---\n\nresult = await Runner.run(\n    main_agent,\n    \"What is the capital of Thailand's northen province. this is geography question.\",\n    session=session,\n)\n\nprint(result.last_agent.name)\nprint(result.final_output)"
    },
    "assets": [],
    "techStack": [
      "Streamlit",
      "Python",
      "OpenAI"
    ]
  },
  {
    "id": "07",
    "title": "Customer Support Agent",
    "description": "ë©”ëª¨ë¦¬ì™€ ê°€ë“œë ˆì¼ì´ í¬í•¨ëœ ê³ ê° ì§€ì› ì›Œí¬í”Œë¡œìš°",
    "directory": "07_customer-support-agent",
    "readmePath": "07_customer-support-agent/README.md",
    "mainCodePath": "07_customer-support-agent/main.py",
    "readme": "",
    "codes": {
      "main.py": "from openai import OpenAI\nimport asyncio\nimport os\nimport streamlit as st\nfrom dotenv import load_dotenv\nfrom agents import (\n    Runner,\n    SQLiteSession,\n    InputGuardrailTripwireTriggered,\n    OutputGuardrailTripwireTriggered,\n)\nfrom agents.voice import AudioInput, VoicePipeline\nfrom models import UserAccountContext\nfrom my_agents.triage_agent import triage_agent\nfrom my_agents.account_agent import account_agent\nfrom my_agents.billing_agent import billing_agent\nfrom my_agents.order_agent import order_agent\nfrom my_agents.technical_agent import technical_agent\nimport numpy as np\nimport wave, io\nfrom workflow import CustomWorkflow\nimport sounddevice as sd\n\nload_dotenv()\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\nuser_account_ctx = UserAccountContext(\n    customer_id=1,\n    name=\"henry\",\n    email=\"henry@example.com\",\n    tier=\"basic\",\n)\n\n\nif \"session\" not in st.session_state:\n    st.session_state[\"session\"] = SQLiteSession(\n        \"chat-history\",\n        \"customer-support-memory.db\",\n    )\nsession = st.session_state[\"session\"]\n\nif \"agent\" not in st.session_state:\n    st.session_state[\"agent\"] = triage_agent\n\n\ndef convert_audio(audio_input):\n    audio_data = audio_input.getvalue()\n    with wave.open(io.BytesIO(audio_data), \"rb\") as wav_file:\n        audio_frames = wav_file.readframes(-1)\n    return np.frombuffer(\n        audio_frames,\n        dtype=np.int16,\n    )\n\n\nasync def run_agent(audio_input):\n    with st.chat_message(\"ai\"):\n        status_container = st.status(\"â³ Processing voice message...\")\n        try:\n            audio_array = convert_audio(audio_input)\n            audio = AudioInput(buffer=audio_array)\n            workflow = CustomWorkflow(context=user_account_ctx)\n            pipeline = VoicePipeline(workflow=workflow)\n\n            status_container.update(label=\"Running workflow\", state=\"running\")\n\n            result = await pipeline.run(audio)\n\n            player = sd.OutputStream(\n                samplerate=24000,\n                channels=1,\n                dtype=np.int16,\n            )\n            player.start()\n\n            status_container.update(state=\"complete\")\n\n            async for event in result.stream():\n                if event.type == \"voice_stream_event_audio\":\n                    player.write(event.data)\n\n        except InputGuardrailTripwireTriggered:\n            st.write(\"I can't help you with that.\")\n        except OutputGuardrailTripwireTriggered:\n            st.write(\"Cant show you that answer.\")\n            st.session_state[\"text_placeholder\"].empty()\n\n\naudio_input = st.audio_input(\n    \"Record your message\",\n)\n\nif audio_input:\n    with st.chat_message(\"human\"):\n        st.audio(audio_input)\n    asyncio.run(run_agent(audio_input))\n\n\nwith st.sidebar:\n    reset = st.button(\"Reset memory\")\n    if reset:\n        asyncio.run(session.clear_session())\n    st.write(asyncio.run(session.get_items()))",
      "models.py": "from pydantic import BaseModel\nfrom typing import Optional\n\n\nclass UserAccountContext(BaseModel):\n\n    customer_id: int\n    name: str\n    tier: str = \"basic\"\n    email: Optional[str] = None  # premium entreprise\n\n\nclass InputGuardRailOutput(BaseModel):\n\n    is_off_topic: bool\n    reason: str\n\n\nclass TechnicalOutputGuardRailOutput(BaseModel):\n\n    contains_off_topic: bool\n    contains_billing_data: bool\n    contains_account_data: bool\n    reason: str\n\nclass HandoffData(BaseModel):\n\n    to_agent_name: str\n    issue_type: str\n    issue_description: str\n    reason: str\n\n",
      "output_guardrails.py": "from agents import (\n    Agent,\n    output_guardrail,\n    Runner,\n    RunContextWrapper,\n    GuardrailFunctionOutput,\n)\nfrom models import TechnicalOutputGuardRailOutput, UserAccountContext\n\n\ntechnical_output_guardrail_agent = Agent(\n    name=\"Technical Support Guardrail\",\n    instructions=\"\"\"\n    Analyze the technical support response to check if it inappropriately contains:\n\n    - Billing information (payments, refunds, charges, subscriptions)\n    - Order information (shipping, tracking, delivery, returns)\n    - Account management info (passwords, email changes, account settings)\n\n    Technical agents should ONLY provide technical troubleshooting, diagnostics, and product support.\n    Return true for any field that contains inappropriate content for a technical support response.\n    \"\"\",\n    output_type=TechnicalOutputGuardRailOutput,\n)\n\n\n@output_guardrail\nasync def technical_output_guardrail(\n    wrapper: RunContextWrapper[UserAccountContext],\n    agent: Agent,\n    output: str,\n):\n    result = await Runner.run(\n        technical_output_guardrail_agent,\n        output,\n        context=wrapper.context,\n    )\n\n    validation = result.final_output\n\n    triggered = (\n        validation.contains_off_topic\n        or validation.contains_billing_data\n        or validation.contains_account_data\n    )\n\n    return GuardrailFunctionOutput(\n        output_info=validation,\n        tripwire_triggered=triggered,\n    )\n",
      "tools.py": "import streamlit as st\nfrom agents import function_tool, AgentHooks, Agent, Tool, RunContextWrapper\nfrom models import UserAccountContext\nimport random\nfrom datetime import datetime, timedelta\n\n\n# =============================================================================\n# TECHNICAL SUPPORT TOOLS\n# =============================================================================\n\n\n@function_tool\ndef run_diagnostic_check(\n    context: UserAccountContext, product_name: str, issue_description: str\n) -> str:\n    \"\"\"\n    Run a diagnostic check on the customer's product to identify potential issues.\n\n    Args:\n        product_name: Name of the product experiencing issues\n        issue_description: Description of the problem\n    \"\"\"\n    diagnostics = [\n        \"âœ… Server connectivity: Normal\",\n        \"âœ… API endpoints: Responsive\",\n        \"âš ï¸  Cache memory: 85% full (recommend clearing)\",\n        \"âœ… Database connections: Stable\",\n        \"âš ï¸  Last update: 7 days ago (update available)\",\n    ]\n\n    return f\"ğŸ” Diagnostic results for {product_name}:\\n\" + \"\\n\".join(diagnostics)\n\n\n@function_tool\ndef provide_troubleshooting_steps(context: UserAccountContext, issue_type: str) -> str:\n    \"\"\"\n    Provide step-by-step troubleshooting instructions for common issues.\n\n    Args:\n        issue_type: Type of issue (connection, login, performance, crash, etc.)\n    \"\"\"\n    steps_map = {\n        \"connection\": [\n            \"1. Check internet connectivity\",\n            \"2. Clear browser cache and cookies\",\n            \"3. Disable browser extensions temporarily\",\n            \"4. Try incognito/private browsing mode\",\n            \"5. Restart your router/modem\",\n        ],\n        \"login\": [\n            \"1. Verify username and password\",\n            \"2. Check caps lock is off\",\n            \"3. Clear browser cache\",\n            \"4. Try password reset if needed\",\n            \"5. Disable VPN temporarily\",\n        ],\n        \"performance\": [\n            \"1. Close unnecessary browser tabs\",\n            \"2. Clear browser cache\",\n            \"3. Check available RAM and storage\",\n            \"4. Update your browser\",\n            \"5. Restart the application\",\n        ],\n        \"crash\": [\n            \"1. Update to latest version\",\n            \"2. Restart the application\",\n            \"3. Check system requirements\",\n            \"4. Disable conflicting software\",\n            \"5. Run in safe mode\",\n        ],\n    }\n\n    steps = steps_map.get(\n        issue_type.lower(),\n        [\n            \"1. Restart the application\",\n            \"2. Check for updates\",\n            \"3. Contact support with error details\",\n        ],\n    )\n\n    return f\"ğŸ› ï¸ Troubleshooting steps for {issue_type}:\\n\" + \"\\n\".join(steps)\n\n\n@function_tool\ndef escalate_to_engineering(\n    context: UserAccountContext, issue_summary: str, priority: str = \"medium\"\n) -> str:\n    \"\"\"\n    Escalate a technical issue to the engineering team.\n\n    Args:\n        issue_summary: Brief summary of the technical issue\n        priority: Priority level (low, medium, high, critical)\n    \"\"\"\n    ticket_id = f\"ENG-{random.randint(10000, 99999)}\"\n    is_premium = context.tier != \"basic\"\n\n    return f\"\"\"\nğŸš€ Issue escalated to Engineering Team\nğŸ“‹ Ticket ID: {ticket_id}\nâš¡ Priority: {priority.upper()}\nğŸ“ Summary: {issue_summary}\nğŸ• Expected response: {2 if is_premium else 4} hours\n    \"\"\".strip()\n\n\n# =============================================================================\n# BILLING SUPPORT TOOLS\n# =============================================================================\n\n\n@function_tool\ndef lookup_billing_history(context: UserAccountContext, months_back: int = 6) -> str:\n    \"\"\"\n    Look up customer's billing history and payment records.\n\n    Args:\n        months_back: Number of months to look back (default 6)\n    \"\"\"\n    payments = []\n    for i in range(months_back):\n        date = datetime.now() - timedelta(days=30 * i)\n        amount = random.choice([29.99, 49.99, 99.99])\n        status = random.choice([\"Paid\", \"Paid\", \"Paid\", \"Failed\"])\n        payments.append(f\"â€¢ {date.strftime('%b %Y')}: ${amount} - {status}\")\n\n    return f\"ğŸ’³ Billing History (Last {months_back} months):\\n\" + \"\\n\".join(payments)\n\n\n@function_tool\ndef process_refund_request(\n    context: UserAccountContext, refund_amount: float, reason: str\n) -> str:\n    \"\"\"\n    Process a refund request for the customer.\n\n    Args:\n        refund_amount: Amount to refund\n        reason: Reason for the refund\n    \"\"\"\n    is_premium = context.tier != \"basic\"\n    processing_days = 3 if is_premium else 5\n    refund_id = f\"REF-{random.randint(100000, 999999)}\"\n\n    return f\"\"\"\nâœ… Refund request processed\nğŸ”— Refund ID: {refund_id}\nğŸ’° Amount: ${refund_amount}\nğŸ“ Reason: {reason}\nâ±ï¸ Processing time: {processing_days} business days\nğŸ’³ Refund will appear on original payment method\n    \"\"\".strip()\n\n\n@function_tool\ndef update_payment_method(context: UserAccountContext, payment_type: str) -> str:\n    \"\"\"\n    Help customer update their payment method.\n\n    Args:\n        payment_type: Type of payment method (credit_card, paypal, bank_transfer)\n    \"\"\"\n    return f\"\"\"\nğŸ’³ Payment method update initiated\nğŸ“‹ Type: {payment_type.replace('_', ' ').title()}\nğŸ”’ Secure link sent to: {context.email}\nâ° Link expires in: 24 hours\nâœ… No interruption to current service\n    \"\"\".strip()\n\n\n@function_tool\ndef apply_billing_credit(\n    context: UserAccountContext, credit_amount: float, reason: str\n) -> str:\n    \"\"\"\n    Apply account credit for billing issues or compensation.\n\n    Args:\n        credit_amount: Amount of credit to apply\n        reason: Reason for the credit\n    \"\"\"\n    return f\"\"\"\nğŸ Account credit applied\nğŸ’° Credit amount: ${credit_amount}\nğŸ“ Reason: {reason}\nâš¡ Applied to account: {context.customer_id}\nğŸ“§ Confirmation sent to: {context.email}\n    \"\"\".strip()\n\n\n# =============================================================================\n# ORDER MANAGEMENT TOOLS\n# =============================================================================\n\n\n@function_tool\ndef lookup_order_status(context: UserAccountContext, order_number: str) -> str:\n    \"\"\"\n    Look up the current status and details of an order.\n\n    Args:\n        order_number: Customer's order number\n    \"\"\"\n    statuses = [\"processing\", \"shipped\", \"in_transit\", \"delivered\"]\n    current_status = random.choice(statuses)\n\n    tracking_number = f\"1Z{random.randint(100000, 999999)}\"\n    estimated_delivery = datetime.now() + timedelta(days=random.randint(1, 5))\n\n    return f\"\"\"\nğŸ“¦ Order Status: {order_number}\nğŸ·ï¸ Status: {current_status.title()}\nğŸšš Tracking: {tracking_number}\nğŸ“… Estimated delivery: {estimated_delivery.strftime('%B %d, %Y')}\nğŸ“ Shipping to: {context.email}\n    \"\"\".strip()\n\n\n@function_tool\ndef initiate_return_process(\n    context: UserAccountContext, order_number: str, return_reason: str, items: str\n) -> str:\n    \"\"\"\n    Start the return process for an order.\n\n    Args:\n        order_number: Order number to return\n        return_reason: Reason for return\n        items: Items being returned\n    \"\"\"\n    return_id = f\"RET-{random.randint(100000, 999999)}\"\n    is_premium = context.tier != \"basic\"\n    return_label_fee = 0 if is_premium else 5.99\n\n    return f\"\"\"\nğŸ“¦ Return initiated\nğŸ”— Return ID: {return_id}\nğŸ“‹ Order: {order_number}\nğŸ“ Items: {items}\nğŸ’° Return label fee: ${return_label_fee}\nğŸ“§ Return label sent to: {context.email}\nâ° Return window: 30 days\n    \"\"\".strip()\n\n\n@function_tool\ndef schedule_redelivery(\n    context: UserAccountContext, tracking_number: str, preferred_date: str\n) -> str:\n    \"\"\"\n    Schedule a redelivery for a failed delivery attempt.\n\n    Args:\n        tracking_number: Package tracking number\n        preferred_date: Customer's preferred delivery date\n    \"\"\"\n    return f\"\"\"\nğŸšš Redelivery scheduled\nğŸ“¦ Tracking: {tracking_number}\nğŸ“… New delivery date: {preferred_date}\nğŸ  Address confirmed: {context.email}\nğŸ“ Driver will call 30 minutes before delivery\n    \"\"\".strip()\n\n\n@function_tool\ndef expedite_shipping(context: UserAccountContext, order_number: str) -> str:\n    \"\"\"\n    Upgrade shipping speed for an order (premium customers only).\n\n    Args:\n        order_number: Order to expedite\n    \"\"\"\n    is_premium = context.tier != \"basic\"\n    if not is_premium:\n        return \"âŒ Expedited shipping upgrade requires Premium membership\"\n\n    return f\"\"\"\nâš¡ Shipping expedited\nğŸ“¦ Order: {order_number}\nğŸš€ Upgraded to: Next-day delivery\nğŸ’° No additional charge (Premium benefit)\nğŸ“§ Updated tracking sent to: {context.email}\n    \"\"\".strip()\n\n\n# =============================================================================\n# ACCOUNT MANAGEMENT TOOLS\n# =============================================================================\n\n\n@function_tool\ndef reset_user_password(context: UserAccountContext, email: str) -> str:\n    \"\"\"\n    Send password reset instructions to the customer's email.\n\n    Args:\n        email: Email address to send reset instructions\n    \"\"\"\n    reset_token = f\"RST-{random.randint(100000, 999999)}\"\n\n    return f\"\"\"\nğŸ” Password reset initiated\nğŸ“§ Reset link sent to: {email}\nğŸ”— Reset token: {reset_token}\nâ° Link expires in: 1 hour\nğŸ›¡ï¸ For security, link is single-use only\n    \"\"\".strip()\n\n\n@function_tool\ndef enable_two_factor_auth(context: UserAccountContext, method: str = \"app\") -> str:\n    \"\"\"\n    Help customer set up two-factor authentication.\n\n    Args:\n        method: 2FA method (app, sms, email)\n    \"\"\"\n    setup_code = f\"2FA-{random.randint(100000, 999999)}\"\n\n    return f\"\"\"\nğŸ”’ Two-Factor Authentication Setup\nğŸ“± Method: {method.upper()}\nğŸ”‘ Setup code: {setup_code}\nğŸ“§ Instructions sent to: {context.email}\nâš¡ Enhanced security activated\n    \"\"\".strip()\n\n\n@function_tool\ndef update_account_email(\n    context: UserAccountContext, old_email: str, new_email: str\n) -> str:\n    \"\"\"\n    Process account email address change.\n\n    Args:\n        old_email: Current email address\n        new_email: New email address\n    \"\"\"\n    verification_code = f\"VER-{random.randint(100000, 999999)}\"\n\n    return f\"\"\"\nğŸ“§ Email update requested\nğŸ“¤ From: {old_email}\nğŸ“¥ To: {new_email}\nğŸ” Verification code: {verification_code}\nâ° Code expires in: 30 minutes\nâœ… Change will be activated after verification\n    \"\"\".strip()\n\n\n@function_tool\ndef deactivate_account(\n    context: UserAccountContext, reason: str, feedback: str = \"\"\n) -> str:\n    \"\"\"\n    Process account deactivation request.\n\n    Args:\n        reason: Reason for account deactivation\n        feedback: Optional feedback from customer\n    \"\"\"\n    return f\"\"\"\nâš ï¸ Account deactivation initiated\nğŸ‘¤ Account: {context.customer_id}\nğŸ“ Reason: {reason}\nğŸ’¬ Feedback: {feedback if feedback else 'None provided'}\nâ° Account will be deactivated in 24 hours\nğŸ”„ Can be reactivated within 30 days\nğŸ“§ Confirmation sent to: {context.email}\n    \"\"\".strip()\n\n\n@function_tool\ndef export_account_data(context: UserAccountContext, data_types: str) -> str:\n    \"\"\"\n    Generate export of customer's account data.\n\n    Args:\n        data_types: Types of data to export (profile, orders, billing, etc.)\n    \"\"\"\n    export_id = f\"EXP-{random.randint(100000, 999999)}\"\n\n    return f\"\"\"\nğŸ“Š Data export requested\nğŸ”— Export ID: {export_id}\nğŸ“‹ Data types: {data_types}\nâ±ï¸ Processing time: 2-4 hours\nğŸ“§ Download link will be sent to: {context.email}\nğŸ”’ Link expires in: 7 days\n    \"\"\".strip()\n\n\nclass AgentToolUsageLoggingHooks(AgentHooks):\n\n    async def on_tool_start(\n        self,\n        context: RunContextWrapper[UserAccountContext],\n        agent: Agent[UserAccountContext],\n        tool: Tool,\n    ):\n        with st.sidebar:\n            st.write(f\"ğŸ”§ **{agent.name}** starting tool: `{tool.name}`\")\n\n    async def on_tool_end(\n        self,\n        context: RunContextWrapper[UserAccountContext],\n        agent: Agent[UserAccountContext],\n        tool: Tool,\n        result: str,\n    ):\n        with st.sidebar:\n            st.write(f\"ğŸ”§ **{agent.name}** used tool: `{tool.name}`\")\n            st.code(result)\n\n    async def on_handoff(\n        self,\n        context: RunContextWrapper[UserAccountContext],\n        agent: Agent[UserAccountContext],\n        source: Agent[UserAccountContext],\n    ):\n        with st.sidebar:\n            st.write(f\"ğŸ”„ Handoff: **{source.name}** â†’ **{agent.name}**\")\n\n    async def on_start(\n        self,\n        context: RunContextWrapper[UserAccountContext],\n        agent: Agent[UserAccountContext],\n    ):\n        with st.sidebar:\n            st.write(f\"ğŸš€ **{agent.name}** activated\")\n\n    async def on_end(\n        self,\n        context: RunContextWrapper[UserAccountContext],\n        agent: Agent[UserAccountContext],\n        output,\n    ):\n        with st.sidebar:\n            st.write(f\"ğŸ **{agent.name}** completed\")\n",
      "workflow.py": "from agents.voice import VoiceWorkflowBase, VoiceWorkflowHelper\nfrom agents import Runner\nimport streamlit as st\n\n\nclass CustomWorkflow(VoiceWorkflowBase):\n\n    def __init__(self, context):\n        self.context = context\n\n    async def run(self, transcription):\n        result = Runner.run_streamed(\n            st.session_state[\"agent\"],\n            transcription,\n            session=st.session_state[\"session\"],\n            context=self.context,\n        )\n\n        async for chunk in VoiceWorkflowHelper.stream_text_from(result):\n            yield chunk\n\n        st.session_state[\"agent\"] = result.last_agent\n"
    },
    "assets": [],
    "techStack": [
      "Python"
    ]
  },
  {
    "id": "08",
    "title": "Financial Analyst",
    "description": "ì£¼ì‹ ë° ì¬ë¬´ ë°ì´í„° ë¶„ì„ íˆ¬ì ì¡°ë¬¸ ì—ì´ì „íŠ¸",
    "directory": "08_financial-analyst",
    "readmePath": "08_financial-analyst/README.md",
    "mainCodePath": "08_financial-analyst/main.py",
    "readme": "# Financial Advisor Agent\n\n**Financial Advisor Agent**ëŠ” ì‚¬ìš©ìê°€ ì§€ì •í•œ ì£¼ì‹ í‹°ì»¤(Ticker)ì— ëŒ€í•´ í¬ê´„ì ì¸ íˆ¬ì ì¡°ì–¸ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” AI ê¸°ë°˜ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. Googleì˜ Gemini ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ë°ì´í„° ë¶„ì„, ì¬ë¬´ ë¶„ì„, ë‰´ìŠ¤ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ì´ë¥¼ ì¢…í•©í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n\n## ì£¼ìš” ê¸°ëŠ¥\n\nì´ í”„ë¡œì íŠ¸ëŠ” 3ê°œì˜ ì „ë¬¸ ì„œë¸Œ ì—ì´ì „íŠ¸ì™€ í˜‘ë ¥í•˜ì—¬ ì‘ë™í•©ë‹ˆë‹¤:\n\n1.  **Data Analyst (`data_analyst`)**: ì£¼ê°€ ë°ì´í„° ë° ê¸°ìˆ ì  ì§€í‘œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.\n2.  **Financial Analyst (`financial_analyst`)**: íšŒì‚¬ì˜ ì¬ë¬´ ì œí‘œ ë° ê±´ì „ì„±ì„ ë¶„ì„í•©ë‹ˆë‹¤.\n3.  **News Analyst (`news_analyst`)**: ìµœì‹  ë‰´ìŠ¤ ë° ì‹œì¥ ë™í–¥ì„ ë¶„ì„í•˜ì—¬ ì •ì„±ì ì¸ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n\nìµœì¢…ì ìœ¼ë¡œ **Financial Advisor**ëŠ” ì´ë“¤ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ íˆ¬ì ì¡°ì–¸ ë³´ê³ ì„œ(`{ticker}_investment_advice.md`)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n\n## ì„¤ì¹˜ ë° ì„¤ì •\n\nì´ í”„ë¡œì íŠ¸ëŠ” Python í™˜ê²½ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤. ì˜ì¡´ì„± ê´€ë¦¬ë¥¼ ìœ„í•´ `uv`ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n\n### í•„ìˆ˜ ìš”êµ¬ ì‚¬í•­\n\n- Python 3.10 ì´ìƒ\n- Google Gemini API Key\n- Firecrawl API Key (ì›¹ ê²€ìƒ‰ìš©)\n\n### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n\ní”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì— `.env` íŒŒì¼ì„ ìƒì„±í•˜ê³  ë‹¤ìŒ ë³€ìˆ˜ë“¤ì„ ì„¤ì •í•´ì£¼ì„¸ìš”:\n\n```bash\nGOOGLE_API_KEY=your_google_api_key\nFIRECRAWL_API_KEY=your_firecrawl_api_key\n```\n\n### íŒ¨í‚¤ì§€ ì„¤ì¹˜\n\n`uv`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ì¡´ì„±ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤:\n\n```bash\nuv sync\n```\n\në˜ëŠ” `pip`ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n```bash\npip install -r requirements.txt\n```\n\n## ì‚¬ìš© ë°©ë²•\n\nì—ì´ì „íŠ¸ëŠ” `financial_advisor` íŒ¨í‚¤ì§€ ë‚´ì— ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤. `main.py`ë¥¼ í†µí•´ ì‹¤í–‰í•˜ê±°ë‚˜ ë‹¤ë¥¸ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì„í¬íŠ¸í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```python\nfrom financial_advisor.agent import financial_advisor\n\n# ì—ì´ì „íŠ¸ ì‹¤í–‰ ì˜ˆì‹œ (êµ¬ì²´ì ì¸ ì‹¤í–‰ ì½”ë“œëŠ” êµ¬í˜„ í•„ìš”)\n# ...\n```\n\n### ADK CLI ì‚¬ìš©\n\n`adk` CLI ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰í•˜ë©´ ì—ì´ì „íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•©ë‹ˆë‹¤.\n\n**1. ì›¹ ì¸í„°í˜ì´ìŠ¤ë¡œ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ ë° ì‹œì—°ìš©):**\n\n```bash\nadk web\n```\n\n**2. API ì„œë²„ë¡œ ì‹¤í–‰ (ì™¸ë¶€ ì—°ë™ìš©):**\n\n```bash\nadk api_server\n```\n\n## í”„ë¡œì íŠ¸ êµ¬ì¡°\n\n```\n.\nâ”œâ”€â”€ financial_advisor/          # Financial Advisor ì—ì´ì „íŠ¸ íŒ¨í‚¤ì§€\nâ”‚   â”œâ”€â”€ agent.py                # ë©”ì¸ ì—ì´ì „íŠ¸ ì •ì˜ (FinancialAdvisor)\nâ”‚   â”œâ”€â”€ prompt.py               # ì—ì´ì „íŠ¸ í”„ë¡¬í”„íŠ¸\nâ”‚   â””â”€â”€ sub_agents/             # ì„œë¸Œ ì—ì´ì „íŠ¸ (Data, Financial, News)\nâ”œâ”€â”€ tools.py                    # ìœ í‹¸ë¦¬í‹° ë„êµ¬ (ì›¹ ê²€ìƒ‰ ë“±)\nâ”œâ”€â”€ main.py                     # ì§„ì…ì  íŒŒì¼\nâ”œâ”€â”€ .env                        # í™˜ê²½ ë³€ìˆ˜ íŒŒì¼\nâ””â”€â”€ README.md                   # í”„ë¡œì íŠ¸ ë¬¸ì„œ\n```",
    "codes": {
      "main.py": "def main():\n    print(\"Hello from 8-financial-analyst!\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "tools.py": "import dotenv\n\ndotenv.load_dotenv()\nimport re\nimport os\nfrom firecrawl import FirecrawlApp, ScrapeOptions\n\n\ndef web_search_tool(query: str):\n    \"\"\"\n    Web Search Tool.\n    Args:\n        query: str\n            The query to search the web for.\n    Returns\n        A list of search results with the website content in Markdown format.\n    \"\"\"\n    app = FirecrawlApp(api_key=os.getenv(\"FIRECRAWL_API_KEY\"))\n\n    response = app.search(\n        query=query,\n        limit=5,\n        scrape_options=ScrapeOptions(\n            formats=[\"markdown\"],\n        ),\n    )\n\n    if not response.success:\n        return \"Error using tool.\"\n\n    cleaned_chunks = []\n\n    for result in response.data:\n\n        title = result[\"title\"]\n        url = result[\"url\"]\n        markdown = result[\"markdown\"]\n\n        cleaned = re.sub(r\"\\\\+|\\n+\", \"\", markdown).strip()\n        cleaned = re.sub(r\"\\[[^\\]]+\\]\\([^\\)]+\\)|https?://[^\\s]+\", \"\", cleaned)\n\n        cleaned_result = {\n            \"title\": title,\n            \"url\": url,\n            \"markdown\": cleaned,\n        }\n\n        cleaned_chunks.append(cleaned_result)\n\n    return cleaned_chunks\n\n\n\n"
    },
    "assets": [],
    "techStack": [
      "Gemini",
      "Python"
    ]
  },
  {
    "id": "09",
    "title": "Youtube Shorts Maker",
    "description": "ìë™ ëŒ€ë³¸ ì‘ì„±ë¶€í„° ì˜ìƒ í¸ì§‘ê¹Œì§€ ìˆ˜í–‰í•˜ëŠ” ì—ì´ì „íŠ¸",
    "directory": "09_youtube-shorts-maker",
    "readmePath": "09_youtube-shorts-maker/README.md",
    "mainCodePath": "09_youtube-shorts-maker/main.py",
    "readme": "# YouTube Shorts Maker Agent ğŸ¬\n\n**YouTube Shorts Maker**ëŠ” ì£¼ì œë§Œ ì…ë ¥í•˜ë©´ ëŒ€ë³¸ ì‘ì„±ë¶€í„° ì´ë¯¸ì§€ ìƒì„±, ë”ë¹™, ì˜ìƒ í¸ì§‘ê¹Œì§€ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•˜ì—¬ YouTube Shorts ì˜ìƒì„ ë§Œë“¤ì–´ì£¼ëŠ” AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.\n\nGoogle Gen AI Agent Development Kit (ADK)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ë˜ì—ˆìœ¼ë©°, Gemini 2.5 Flash ëª¨ë¸ê³¼ OpenAIì˜ DALL-E 3, TTS ëª¨ë¸ì„ í™œìš©í•©ë‹ˆë‹¤.\n\n## âœ¨ ì£¼ìš” ê¸°ëŠ¥\n\n*   **ğŸ“ ì½˜í…ì¸  ê¸°íš (Content Planner):** ì£¼ì œì— ë§ì¶° ìˆí¼ì— ìµœì í™”ëœ ëŒ€ë³¸ê³¼ ì¥ë©´ë³„ ì‹œê°ì  ë¬˜ì‚¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n*   **ğŸ¨ ì´ë¯¸ì§€ ìƒì„± (Image Generator):** ê° ì¥ë©´ì— ì–´ìš¸ë¦¬ëŠ” ê³ í’ˆì§ˆ ì„¸ë¡œí˜•(9:16) ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n*   **ğŸ™ï¸ ìŒì„± ìƒì„± (Voice Generator):** ëŒ€ë³¸ì„ ìì—°ìŠ¤ëŸ¬ìš´ AI ì„±ìš°ì˜ ëª©ì†Œë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n*   **ğŸ¬ ì˜ìƒ ì¡°ë¦½ (Video Assembler):** ìƒì„±ëœ ì´ë¯¸ì§€ì™€ ì˜¤ë””ì˜¤ë¥¼ FFmpegë¥¼ ì‚¬ìš©í•˜ì—¬ í•˜ë‚˜ì˜ MP4 ì˜ìƒìœ¼ë¡œ í•©ì¹©ë‹ˆë‹¤.\n*   **ğŸ›¡ï¸ ì•ˆì „ ì¥ì¹˜ (Guardrails):** ë¶€ì ì ˆí•œ ì…ë ¥(ì˜ˆ: 'hummus')ì„ í•„í„°ë§í•˜ëŠ” ì˜ˆì œ ì½œë°±ì´ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n\n## ğŸ› ï¸ ì‚¬ì „ ìš”êµ¬ ì‚¬í•­\n\n*   **Python 3.12+**\n*   **FFmpeg**: ì˜ìƒ ì²˜ë¦¬ë¥¼ ìœ„í•´ í•„ìˆ˜ì…ë‹ˆë‹¤.\n    *   Mac: `brew install ffmpeg`\n*   **API Keys**: `.env` íŒŒì¼ì— ë‹¤ìŒ í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n    *   `GEMINI_API_KEY`\n    *   `OPENAI_API_KEY`\n\n## ğŸš€ ì„¤ì¹˜ ë°©ë²•\n\nì´ í”„ë¡œì íŠ¸ëŠ” `uv`ë¥¼ ì‚¬ìš©í•˜ì—¬ íŒ¨í‚¤ì§€ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.\n\n```bash\n# í”„ë¡œì íŠ¸ í´ë¡  ë° ì´ë™\ngit clone <repository-url>\ncd 9_youtube-shorts-maker\n\n# ì˜ì¡´ì„± ì„¤ì¹˜\nuv sync\n```\n\n## ğŸ’» ì‚¬ìš© ë°©ë²•\n\n### 1. Web UI (ê¶Œì¥)\n\nê°€ì¥ í¸ë¦¬í•˜ê²Œ ì—ì´ì „íŠ¸ì™€ ëŒ€í™”í•˜ê³  ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n\n```bash\nadk web\n```\n*   ë¸Œë¼ìš°ì €ê°€ ì—´ë¦¬ë©´ ì±„íŒ…ì°½ì— \"ì»¤í”¼ì˜ ì—­ì‚¬ì— ëŒ€í•œ ì‡¼ì¸  ë§Œë“¤ì–´ì¤˜\"ì™€ ê°™ì´ ì…ë ¥í•˜ì„¸ìš”.\n*   ì‘ì—…ì´ ì™„ë£Œë˜ë©´ ì±„íŒ…ì°½ì— **ì˜ìƒ ë‹¤ìš´ë¡œë“œ ë§í¬**ê°€ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.\n\n### 2. CLI (í„°ë¯¸ë„ ëª¨ë“œ)\n\ní„°ë¯¸ë„ì—ì„œ ì§ì ‘ ì—ì´ì „íŠ¸ì™€ ëŒ€í™”í•˜ê³  ì‹¶ë‹¤ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n\n```bash\nuv run python main.py\n```\n\n### 3. ìˆ˜ë™ ìƒì„± (ë””ë²„ê¹…/ë³µêµ¬ìš©)\n\nì—ì´ì „íŠ¸ ëŒ€í™” ê³¼ì •ì„ ê±´ë„ˆë›°ê³ , ì¦‰ì‹œ ì˜ìƒì„ ìƒì„±í•˜ê³  ì‹¶ê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ ì‹œ ë³µêµ¬í•˜ê³  ì‹¶ë‹¤ë©´ ì´ ìœ í‹¸ë¦¬í‹°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n\n```bash\nuv run python manual_generation.py\n```\n*   ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë¯¸ë¦¬ ì •ì˜ëœ ì‹œë‚˜ë¦¬ì˜¤(ì»¤í”¼ì˜ ì—­ì‚¬)ë¡œ ì˜ìƒì„ ì¦‰ì‹œ ìƒì„±í•˜ì—¬ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— `final_video.mp4`ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n*   ì´ë¯¸ ìƒì„±ëœ ì´ë¯¸ì§€/ì˜¤ë””ì˜¤ê°€ ìˆë‹¤ë©´ ì¬ì‚¬ìš©í•˜ì—¬ ì†ë„ê°€ ë¹ ë¦…ë‹ˆë‹¤.\n\n## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°\n\n```\n9_youtube-shorts-maker/\nâ”œâ”€â”€ main.py                 # CLI ì§„ì…ì \nâ”œâ”€â”€ manual_generation.py    # ìˆ˜ë™ ìƒì„± ìœ í‹¸ë¦¬í‹°\nâ”œâ”€â”€ youtube_shorts_maker/\nâ”‚   â”œâ”€â”€ agent.py            # ë©”ì¸ ì—ì´ì „íŠ¸ (ShortsProducerAgent) ì •ì˜\nâ”‚   â””â”€â”€ sub_agents/\nâ”‚       â”œâ”€â”€ content_planner/ # ê¸°íš ì—ì´ì „íŠ¸\nâ”‚       â”œâ”€â”€ asset_generator/ # ìì‚° ìƒì„± (ì´ë¯¸ì§€/ì˜¤ë””ì˜¤) ì—ì´ì „íŠ¸\nâ”‚       â””â”€â”€ video_assembler/ # ì˜ìƒ ì¡°ë¦½ ì—ì´ì „íŠ¸ (FFmpeg ë„êµ¬ í¬í•¨)\nâ”œâ”€â”€ final_video.mp4         # ìƒì„±ëœ ê²°ê³¼ë¬¼ (ë¡œì»¬ ì €ì¥ ì‹œ)\nâ””â”€â”€ .env                    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n```\n\n## âš ï¸ ì£¼ì˜ ì‚¬í•­\n\n*   **ë¹„ìš©:** OpenAI DALL-E 3 ë° TTS API ì‚¬ìš© ì‹œ ë¹„ìš©ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n*   **ì‹œê°„:** ì˜ìƒ ìƒì„±(íŠ¹íˆ ì´ë¯¸ì§€ 5ì¥ ìƒì„±)ì—ëŠ” ì•½ 2~5ë¶„ì˜ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "codes": {
      "main.py": "from youtube_shorts_maker.agent import shorts_producer_agent\nfrom rich.console import Console\n\nconsole = Console()\n\n\ndef main():\n    console.print(\"[bold green]YouTube Shorts Maker Agent[/bold green]\")\n    console.print(\"Type 'exit' to quit\")\n\n    while True:\n        user_input = console.input(\"[bold blue]You:[/bold blue] \")\n        if user_input.lower() == \"exit\":\n            break\n\n        response = shorts_producer_agent.run(user_input)\n        console.print(f\"[bold green]Agent:[/bold green] {response.content}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "manual_generation.py": "import asyncio\nimport os\nimport sys\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n# Add project root to path\nsys.path.append(os.getcwd())\n\nfrom google.adk.tools.tool_context import ToolContext\nfrom youtube_shorts_maker.sub_agents.asset_generator.voice_generator.tools import generate_narrations\nfrom youtube_shorts_maker.sub_agents.asset_generator.image_generator.image_builder.tools import generate_images\nfrom youtube_shorts_maker.sub_agents.video_assembler.tools import assemble_video\nimport google.genai.types as types\n\n# Mock ToolContext to handle state and artifacts locally\nclass LocalToolContext(ToolContext):\n    def __init__(self, state):\n        self._state = state\n        self.artifacts = {}\n\n    @property\n    def state(self):\n        return self._state\n\n    async def list_artifacts(self):\n        return list(self.artifacts.keys())\n\n    async def load_artifact(self, filename):\n        return self.artifacts.get(filename)\n\n    async def save_artifact(self, filename, artifact):\n        print(f\"ğŸ’¾ Saving artifact: {filename}\")\n        self.artifacts[filename] = artifact\n        # Also save to disk for inspection\n        if artifact.inline_data:\n            with open(filename, \"wb\") as f:\n                f.write(artifact.inline_data.data)\n        return True\n\nasync def main():\n    print(\"ğŸš€ Starting Manual Video Generation for 'History of Coffee'...\")\n\n    # 1. Define Content Plan (Hardcoded for reliability)\n    content_plan = {\n        \"scenes\": [\n            {\n                \"id\": 1,\n                \"duration\": 4,\n                \"narration\": \"Every perfect cup has a legend. Travel back to 9th-century Ethiopia and meet Kaldi, the unsuspecting goat herder.\",\n                \"visual_description\": \"Misty Ethiopian mountainside. Kaldi watches his flock.\",\n                \"embedded_text\": \"The Mythical Bean\"\n            },\n            {\n                \"id\": 2,\n                \"duration\": 3,\n                \"narration\": \"He noticed his goats, after eating strange red berries, began to DANCE. Uncontrollable energy filled the flock.\",\n                \"visual_description\": \"Close-up of goats excitedly jumping and twitching while chewing red berries.\",\n                \"embedded_text\": \"Goats Discovered Coffee\"\n            },\n            {\n                \"id\": 3,\n                \"duration\": 4,\n                \"narration\": \"Kaldi took these stimulating fruits to a local monastery, sharing the secret of their energetic power.\",\n                \"visual_description\": \"Kaldi presents coffee cherries to an elder monk in a dimly lit, ancient stone structure.\",\n                \"embedded_text\": \"Ancient Power\"\n            },\n            {\n                \"id\": 4,\n                \"duration\": 5,\n                \"narration\": \"The monks, fearing the devil's work, cast the berries into the fire. But as they burned, a divine, intoxicating aroma filled the air.\",\n                \"visual_description\": \"Dramatic shot of red berries being thrown onto glowing embers, creating smoke.\",\n                \"embedded_text\": \"The Divine Roast\"\n            },\n            {\n                \"id\": 5,\n                \"duration\": 4,\n                \"narration\": \"And thus, the ritual was born. Remember the dancing goats every time you savor the magic of your morning brew.\",\n                \"visual_description\": \"Close-up of steam swirling dramatically off a freshly poured dark espresso.\",\n                \"embedded_text\": \"Savor the Legend\"\n            }\n        ]\n    }\n\n    # Prepare state for tools\n    # Voice Generator needs: content_planner_output (for context) and voice/instructions\n    # Image Builder needs: prompt_builder_output (which contains optimized prompts)\n    # Video Assembler needs: content_planner_output\n    \n    # We need to mock the \"Prompt Builder\" output since we are skipping that agent\n    # We'll just use the visual descriptions as the \"optimized prompts\"\n    optimized_prompts = []\n    voice_instructions = []\n    \n    for scene in content_plan[\"scenes\"]:\n        optimized_prompts.append({\n            \"scene_id\": scene[\"id\"],\n            \"enhanced_prompt\": f\"Vertical 9:16 aspect ratio, photorealistic, 4k. {scene['visual_description']}. Text overlay: '{scene['embedded_text']}'\"\n        })\n        voice_instructions.append({\n            \"scene_id\": scene[\"id\"],\n            \"input\": scene[\"narration\"],\n            \"instructions\": \"Narration\"\n        })\n\n    state = {\n        \"content_planner_output\": content_plan,\n        \"prompt_builder_output\": {\"optimized_prompts\": optimized_prompts}\n    }\n\n    ctx = LocalToolContext(state)\n\n    # 2. Generate Images\n    print(\"\\nğŸ¨ Generating Images...\")\n    # Check if images exist\n    images_exist = True\n    for scene in content_plan[\"scenes\"]:\n        if not os.path.exists(f\"scene_{scene['id']}_image.jpeg\"):\n            images_exist = False\n            break\n    \n    if images_exist:\n        print(\"âœ… Images already exist locally. Skipping generation.\")\n        # Load them into context so assembler can find them\n        for scene in content_plan[\"scenes\"]:\n            filename = f\"scene_{scene['id']}_image.jpeg\"\n            with open(filename, \"rb\") as f:\n                data = f.read()\n            ctx.artifacts[filename] = types.Part(inline_data=types.Blob(mime_type=\"image/jpeg\", data=data))\n    else:\n        try:\n            await generate_images(ctx)\n            print(\"âœ… Images generated successfully.\")\n        except Exception as e:\n            print(f\"âŒ Image generation failed: {e}\")\n            return\n\n    # 3. Generate Audio\n    print(\"\\nğŸ™ï¸ Generating Audio...\")\n    # Check if audio exists\n    audio_exist = True\n    for scene in content_plan[\"scenes\"]:\n        if not os.path.exists(f\"scene_{scene['id']}_narration.mp3\"):\n            audio_exist = False\n            break\n            \n    if audio_exist:\n        print(\"âœ… Audio already exists locally. Skipping generation.\")\n        # Load them into context so assembler can find them\n        for scene in content_plan[\"scenes\"]:\n            filename = f\"scene_{scene['id']}_narration.mp3\"\n            with open(filename, \"rb\") as f:\n                data = f.read()\n            ctx.artifacts[filename] = types.Part(inline_data=types.Blob(mime_type=\"audio/mpeg\", data=data))\n    else:\n        try:\n            await generate_narrations(ctx, voice=\"alloy\", voice_instructions=voice_instructions)\n            print(\"âœ… Audio generated successfully.\")\n        except Exception as e:\n            print(f\"âŒ Audio generation failed: {e}\")\n            return\n\n    # 4. Assemble Video\n    print(\"\\nğŸ¬ Assembling Video...\")\n    # Note: VideoAssembler tool signature is assemble_video(tool_context)\n    try:\n        result = await assemble_video(ctx)\n        print(f\"\\nâœ… {result}\")\n    except Exception as e:\n        print(f\"âŒ Video assembly failed: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"
    },
    "assets": [
      "assets/09_youtube-shorts-maker/scene_1_image.jpeg",
      "assets/09_youtube-shorts-maker/scene_2_image.jpeg",
      "assets/09_youtube-shorts-maker/scene_3_image.jpeg",
      "assets/09_youtube-shorts-maker/scene_4_image.jpeg",
      "assets/09_youtube-shorts-maker/scene_5_image.jpeg"
    ],
    "techStack": [
      "Gemini",
      "Python",
      "OpenAI"
    ]
  },
  {
    "id": "10",
    "title": "Email Refiner Agent",
    "description": "ì´ë©”ì¼ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ìµœì í™” ì—ì´ì „íŠ¸",
    "directory": "10_email-refiner-agent",
    "readmePath": "10_email-refiner-agent/README.md",
    "mainCodePath": "10_email-refiner-agent/main.py",
    "readme": "# Email Refiner Agent\n\nGoogle ADKë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„í•œ AI ì—ì´ì „íŠ¸ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤. ì´ë©”ì¼ ìµœì í™” ì—ì´ì „íŠ¸ì™€ ì—¬í–‰ ì¡°ì–¸ ì—ì´ì „íŠ¸ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n\n## í”„ë¡œì íŠ¸ êµ¬ì„±\n\n### Agents\n\n1. **Email Refiner Agent** (`email_refiner/`)\n   - ì´ë©”ì¼ì„ ìµœì í™”í•˜ê³  ê°œì„ í•˜ëŠ” ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ\n   - ì—¬ëŸ¬ ì „ë¬¸ê°€ ì—ì´ì „íŠ¸ë“¤ì´ í˜‘ë ¥í•˜ì—¬ ì´ë©”ì¼ì„ ê°œì„ \n\n2. **Travel Advisor Agent** (`travel_advisor_agent/`)\n   - ì—¬í–‰ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì—ì´ì „íŠ¸\n   - ë‚ ì”¨, í™˜ìœ¨, ëª…ì†Œ ì •ë³´ ì œê³µ\n\n### ì‹¤í–‰ ëª¨ë“œ\n\n- **API Mode** (`api_mode.ipynb`): FastAPI ì„œë²„ë¥¼ í†µí•œ HTTP ìš”ì²­ ë°©ì‹\n- **Code Mode** (`code_mode.ipynb`): ì§ì ‘ ì½”ë“œë¡œ ì—ì´ì „íŠ¸ ì‹¤í–‰\n\n## ê¸°ìˆ  ìŠ¤íƒ\n\n- **Google ADK**: Googleì˜ Agent Development Kit\n- **Gemini 2.5 Flash**: Googleì˜ Gemini ëª¨ë¸\n- **Vertex AI**: Google Cloudì˜ ML í”Œë«í¼ (ë°°í¬ìš©)\n- **Python 3.13+**\n\n## ì„¤ì¹˜\n\n### ìš”êµ¬ì‚¬í•­\n\n- Python 3.13 ì´ìƒ\n- `uv` íŒ¨í‚¤ì§€ ë§¤ë‹ˆì €\n\n### ì„¤ì¹˜ ë°©ë²•\n\n```bash\n# ì˜ì¡´ì„± ì„¤ì¹˜\nuv sync\n\n# ê°œë°œ ì˜ì¡´ì„± í¬í•¨ ì„¤ì¹˜\nuv sync --group dev\n```\n\n## í™˜ê²½ ì„¤ì •\n\n`.env` íŒŒì¼ì„ ìƒì„±í•˜ê³  í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”:\n\n```env\n# Google Cloud ì„¤ì • (ë°°í¬ ì‹œ í•„ìš”)\nPROJECT_ID=your-project-id\nLOCATION=asia-southeast1\nBUCKET=gs://your-bucket-name\n```\n\n## ì‚¬ìš© ë°©ë²•\n\n### 1. API ëª¨ë“œ\n\nFastAPI ì„œë²„ë¥¼ ì‹¤í–‰í•˜ì—¬ HTTP APIë¡œ ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```bash\n# ì„œë²„ ì‹¤í–‰\nadk api_server\n\n# api_mode.ipynb ë…¸íŠ¸ë¶ì—ì„œ API í…ŒìŠ¤íŠ¸\n```\n\n### 2. ì½”ë“œ ëª¨ë“œ\n\nJupyter ë…¸íŠ¸ë¶ì—ì„œ ì§ì ‘ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```bash\n# Jupyter ë…¸íŠ¸ë¶ ì‹¤í–‰\njupyter notebook code_mode.ipynb\n```\n\n### 3. Vertex AI ë°°í¬\n\nì—ì´ì „íŠ¸ë¥¼ Google Cloud Vertex AIì— ë°°í¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```bash\n# ë°°í¬ ì‹¤í–‰\npython deploy.py\n\n# ë°°í¬ ê´€ë¦¬ (ì¡°íšŒ, ì‚­ì œ ë“±)\npython remote.py\n```\n\n**ì£¼ì˜**: `deploy.py`ì™€ `remote.py`ì˜ í”„ë¡œì íŠ¸ IDì™€ ë¦¬ì „ì„ ë³¸ì¸ì˜ ì„¤ì •ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”.\n\n## í”„ë¡œì íŠ¸ êµ¬ì¡°\n\n```\n.\nâ”œâ”€â”€ email_refiner/          # ì´ë©”ì¼ ë¦¬íŒŒì´ë„ˆ ì—ì´ì „íŠ¸\nâ”‚   â”œâ”€â”€ agent.py           # ì—ì´ì „íŠ¸ ì •ì˜\nâ”‚   â””â”€â”€ prompt.py          # í”„ë¡¬í”„íŠ¸ ì •ì˜\nâ”œâ”€â”€ travel_advisor_agent/  # ì—¬í–‰ ì¡°ì–¸ ì—ì´ì „íŠ¸\nâ”‚   â”œâ”€â”€ agent.py           # ì—ì´ì „íŠ¸ ì •ì˜\nâ”‚   â””â”€â”€ prompt.py          # í”„ë¡¬í”„íŠ¸ ì •ì˜\nâ”œâ”€â”€ api_mode.ipynb         # API ëª¨ë“œ ì˜ˆì œ\nâ”œâ”€â”€ code_mode.ipynb        # ì½”ë“œ ëª¨ë“œ ì˜ˆì œ\nâ”œâ”€â”€ deploy.py              # Vertex AI ë°°í¬ ìŠ¤í¬ë¦½íŠ¸\nâ”œâ”€â”€ remote.py              # ë°°í¬ ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸\nâ”œâ”€â”€ main.py                # ë©”ì¸ ì‹¤í–‰ íŒŒì¼\nâ””â”€â”€ pyproject.toml         # í”„ë¡œì íŠ¸ ì„¤ì • ë° ì˜ì¡´ì„±\n```\n\n## ì˜ì¡´ì„±\n\nì£¼ìš” ì˜ì¡´ì„±:\n\n- `google-adk[eval]>=1.12.0,<1.16.0`: Google Agent Development Kit\n- `google-cloud-aiplatform[adk,agent-engines]>=1.111.0`: Vertex AI í”Œë«í¼\n- `google-genai>=1.31.0`: Google Gemini API\n- `litellm>=1.76.0`: LLM í†µí•© ë¼ì´ë¸ŒëŸ¬ë¦¬\n- `requests>=2.32.5`: HTTP ìš”ì²­ ë¼ì´ë¸ŒëŸ¬ë¦¬\n- `sseclient-py>=1.8.0`: Server-Sent Events í´ë¼ì´ì–¸íŠ¸\n\n## ë¼ì´ì„ ìŠ¤\n\nì´ í”„ë¡œì íŠ¸ëŠ” êµìœ¡ ë° í•™ìŠµ ëª©ì ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "codes": {
      "api_mode.ipynb": "import requests\n\n\nBASE_URL = \"http://127.0.0.1:8000\"\nAPP_NAME = \"travel_advisor_agent\"\nUSER_ID = \"u_123\"\nSESSION_ID = \"a3e4785a-5040-4d8c-bad0-f7d4cad12edf\"\n\n\n# ---\n\nresponse = requests.post(f\"{BASE_URL}/apps/{APP_NAME}/users/{USER_ID}/sessions\")\n\nprint(response.json())\n\n\n# ---\n\nmessage = {\n    \"appName\": APP_NAME,\n    \"userId\": USER_ID,\n    \"sessionId\": SESSION_ID,\n    \"newMessage\": {\n        \"parts\": [{\"text\": \"Yes, I want to know the currency exchange rate\"}],\n        \"role\": \"user\",\n    },\n}\nresponse = requests.post(f\"{BASE_URL}/run\", json=message)\n\nprint(response.json())\n\n\n# ---\n\ndata = response.json()\n\nfor event in data:\n    content = event.get(\"content\")\n    parts = content.get(\"parts\")\n    for part in parts:\n        function_call = part.get(\"functionCall\", None)\n        if function_call:\n            print(function_call.get(\"name\"))\n        text = part.get(\"text\", None)\n        if text:\n            print(text)\n    print(\"=\" * 60)\n\n\n# ---\n\nimport sseclient\nimport json\n\nmessage = {\n    \"appName\": APP_NAME,\n    \"userId\": USER_ID,\n    \"sessionId\": SESSION_ID,\n    \"newMessage\": {\n        \"parts\": [{\"text\": \"What is the weather there?\"}],\n        \"role\": \"user\",\n    },\n    \"streaming\": True,\n}\nresponse = requests.post(\n    f\"{BASE_URL}/run_sse\",\n    json=message,\n    stream=True,\n)\n\nclient = sseclient.SSEClient(response)\n\nfor event in client.events():\n    data = json.loads(event.data)\n    content = data.get(\"content\")\n    parts = content.get(\"parts\")\n    for part in parts:\n        function_call = part.get(\"functionCall\", None)\n        if function_call:\n            print(function_call.get(\"name\"))\n        text = part.get(\"text\", None)\n        if text:\n            print(text)\n    print(\"=\" * 60)",
      "code_mode.ipynb": "from google.genai import types\nfrom google.adk.agents import Agent\nfrom google.adk.models import Gemini\nfrom prompt import (\n    TRAVEL_ADVISOR_DESCRIPTION,\n    TRAVEL_ADVISOR_INSTRUCTION,\n)\nfrom google.adk.tools.tool_context import ToolContext\nfrom google.adk.sessions import DatabaseSessionService\nfrom google.adk.artifacts import InMemoryArtifactService\nfrom google.adk.runners import Runner\n\n\nin_memory_service_py = InMemoryArtifactService()\n\nsession_service = DatabaseSessionService(db_url=\"sqlite:///./session.db\")\n\nsession = await session_service.create_session(\n    app_name=\"travel_advisor_agent\",\n    user_id=\"u_123\",\n    state={\n        \"user_name\": \"henry\",\n    },\n)\n\n\nMODEL = Gemini(model=\"gemini-2.5-flash\")\n\n\nasync def get_weather(tool_context: ToolContext, location: str):\n    \"\"\"Get current weather information for a location.\"\"\"\n    # Dummy implementation - returns mock data\n    return {\n        \"location\": location,\n        \"temperature\": \"22Â°C\",\n        \"condition\": \"Partly cloudy\",\n        \"humidity\": \"65%\",\n        \"wind\": \"12 km/h\",\n        \"forecast\": \"Mild weather with occasional clouds expected throughout the day\",\n    }\n\n\nasync def get_exchange_rate(\n    tool_context: ToolContext, from_currency: str, to_currency: str, amount: float\n):\n    \"\"\"Get exchange rate between two currencies.\n    Args should always be from_currency str, to_currency str, amount flot\n    \"\"\"\n    # Dummy implementation - returns mock data\n    mock_rates = {\n        (\"USD\", \"EUR\"): 0.92,\n        (\"USD\", \"GBP\"): 0.79,\n        (\"USD\", \"JPY\"): 149.50,\n        (\"USD\", \"KRW\"): 1325.00,\n        (\"EUR\", \"USD\"): 1.09,\n        (\"EUR\", \"GBP\"): 0.86,\n        (\"GBP\", \"USD\"): 1.27,\n        (\"JPY\", \"USD\"): 0.0067,\n        (\"KRW\", \"USD\"): 0.00075,\n    }\n\n    rate = mock_rates.get((from_currency, to_currency), 1.0)\n    converted_amount = amount * rate\n\n    return {\n        \"from_currency\": from_currency,\n        \"to_currency\": to_currency,\n        \"amount\": amount,\n        \"exchange_rate\": rate,\n        \"converted_amount\": converted_amount,\n        \"timestamp\": \"2024-03-15 10:30:00 UTC\",\n    }\n\n\nasync def get_local_attractions(\n    tool_context: ToolContext, location: str, category: str\n):\n    \"\"\"Get popular attractions and points of interest for a location.\"\"\"\n    # Dummy implementation - returns mock data\n    attractions = {\n        \"Paris\": [\n            {\n                \"name\": \"Eiffel Tower\",\n                \"type\": \"landmark\",\n                \"rating\": 4.8,\n                \"description\": \"Iconic iron lattice tower\",\n            },\n            {\n                \"name\": \"Louvre Museum\",\n                \"type\": \"museum\",\n                \"rating\": 4.7,\n                \"description\": \"World's largest art museum\",\n            },\n            {\n                \"name\": \"Arc de Triomphe\",\n                \"type\": \"monument\",\n                \"rating\": 4.6,\n                \"description\": \"Historic triumphal arch\",\n            },\n            {\n                \"name\": \"Notre-Dame\",\n                \"type\": \"cathedral\",\n                \"rating\": 4.5,\n                \"description\": \"Medieval Catholic cathedral\",\n            },\n            {\n                \"name\": \"SacrÃ©-CÅ“ur\",\n                \"type\": \"basilica\",\n                \"rating\": 4.4,\n                \"description\": \"Romano-Byzantine basilica\",\n            },\n        ],\n        \"Tokyo\": [\n            {\n                \"name\": \"Tokyo Tower\",\n                \"type\": \"landmark\",\n                \"rating\": 4.5,\n                \"description\": \"Communications and observation tower\",\n            },\n            {\n                \"name\": \"Senso-ji\",\n                \"type\": \"temple\",\n                \"rating\": 4.6,\n                \"description\": \"Ancient Buddhist temple\",\n            },\n            {\n                \"name\": \"Shibuya Crossing\",\n                \"type\": \"landmark\",\n                \"rating\": 4.4,\n                \"description\": \"Busiest pedestrian crossing\",\n            },\n            {\n                \"name\": \"Meiji Shrine\",\n                \"type\": \"shrine\",\n                \"rating\": 4.7,\n                \"description\": \"Shinto shrine dedicated to Emperor Meiji\",\n            },\n            {\n                \"name\": \"Tokyo Skytree\",\n                \"type\": \"tower\",\n                \"rating\": 4.6,\n                \"description\": \"Broadcasting and observation tower\",\n            },\n        ],\n        \"default\": [\n            {\n                \"name\": \"City Center\",\n                \"type\": \"area\",\n                \"rating\": 4.2,\n                \"description\": \"Main downtown area\",\n            },\n            {\n                \"name\": \"Historical Museum\",\n                \"type\": \"museum\",\n                \"rating\": 4.3,\n                \"description\": \"Local history and culture\",\n            },\n            {\n                \"name\": \"Central Park\",\n                \"type\": \"park\",\n                \"rating\": 4.1,\n                \"description\": \"Main public park\",\n            },\n            {\n                \"name\": \"Old Town\",\n                \"type\": \"district\",\n                \"rating\": 4.4,\n                \"description\": \"Historic district with traditional architecture\",\n            },\n            {\n                \"name\": \"Local Market\",\n                \"type\": \"market\",\n                \"rating\": 4.0,\n                \"description\": \"Traditional local marketplace\",\n            },\n        ],\n    }\n\n    location_attractions = attractions.get(location, attractions[\"default\"])\n\n    if category != \"all\":\n        location_attractions = [\n            a for a in location_attractions if a[\"type\"] == category\n        ]\n\n    return {\n        \"location\": location,\n        \"category\": category,\n        \"attractions\": location_attractions,\n        \"total_count\": len(location_attractions),\n    }\n\n\ntravel_advisor_agent = Agent(\n    name=\"TravelAdvisorAgent\",\n    description=TRAVEL_ADVISOR_DESCRIPTION,\n    instruction=TRAVEL_ADVISOR_INSTRUCTION,\n    tools=[\n        get_weather,\n        get_exchange_rate,\n        get_local_attractions,\n    ],\n    output_key=\"travel_advice\",\n    model=MODEL,\n)\n\n\nrunner = Runner(\n    agent=travel_advisor_agent,\n    session_service=session_service,\n    app_name=\"travel_advisor_agent\",\n    artifact_service=in_memory_service_py,\n)\n\nmessage = types.Content(\n    role=\"user\",\n    parts=[\n        types.Part(text=\"Im going to Vietnam, tell me all about it.\"),\n    ],\n)\n\n\nasync for event in runner.run_async(\n    user_id=\"u_123\", session_id=session.id, new_message=message\n):\n\n    if event.is_final_response():\n        print(event.content.parts[0].text)\n    else:\n        print(event.get_function_calls())\n        print(event.get_function_responses())",
      "deploy.py": "import dotenv\n\ndotenv.load_dotenv()\n\nimport os\nimport vertexai\nimport vertexai.agent_engines\nfrom vertexai.preview import reasoning_engines\nfrom travel_advisor_agent.agent import travel_advisor_agent\n\nPROJECT_ID = \"gen-lang-client-0577338238\"\nLOCATION = \"asia-southeast1\"\nBUCKET = \"gs://travel_advisor_agent\"\n\nvertexai.init(\n    project=PROJECT_ID,\n    location=LOCATION,\n    staging_bucket=BUCKET,\n)\n\napp = reasoning_engines.AdkApp(\n    agent=travel_advisor_agent,\n    enable_tracing=True,\n)\n\nremote_app = vertexai.agent_engines.create(\n    display_name=\"Travel Advisor Agent\",\n    agent_engine=app,\n    requirements=[\n        \"cloudpickle>=3.1.1\",\n        \"google-cloud-aiplatform[adk,agent_engines]>=1.111.0\",\n        \"google-genai>=1.31.0\",\n        \"google-adk[eval]>=1.12.0,<1.16.0\",\n        \"litellm>=1.76.0\",\n        \"requests>=2.32.5\",\n        \"sseclient-py>=1.8.0\",\n    ],\n    extra_packages=[\"travel_advisor_agent\"],\n)\n\n",
      "main.py": "def main():\n    print(\"Hello from 10-email-refiner-agent!\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "prompt.py": "# Agent Descriptions\nTRAVEL_ADVISOR_DESCRIPTION = \"Travel advisor agent that helps users with location-based questions, providing weather, currency exchange, and local attraction information.\"\n\n# Agent Instructions\nTRAVEL_ADVISOR_INSTRUCTION = \"\"\"\nYou are a helpful travel advisor agent that assists users with questions about locations and travel planning.\n\nYou call the user by their name:\n\nTheir name is {user_name}\n\nYou have access to the following tools:\n1. **get_weather** - Get current weather information for any location\n2. **get_exchange_rate** - Convert currencies and get exchange rates\n3. **get_local_attractions** - Find popular attractions and points of interest\n\nWhen users ask questions about a location, you should:\n- Provide relevant information using the appropriate tools\n- Be helpful and informative in your responses\n- Offer additional suggestions when relevant\n- Combine multiple tools when needed for comprehensive answers\n\nFor example:\n- If asked about visiting Paris, you might check the weather, suggest attractions, and provide currency exchange rates\n- If asked about weather, provide current conditions and the forecast\n- If asked about money/currency, provide exchange rates and practical conversion amounts\n\nAlways aim to be helpful, accurate, and provide practical travel advice based on the information from your tools.\n\"\"\"",
      "remote.py": "import vertexai\nfrom vertexai import agent_engines\n\nPROJECT_ID = \"gen-lang-client-0577338238\"\nLOCATION = \"asia-southeast1\"\n\nvertexai.init(\n    project=PROJECT_ID,\n    location=LOCATION,\n)\n\n# deployments = agent_engines.list()\n\n# for deployment in deployments:\n#     print(deployment)\n\nDEPLOYMENT_ID = \"projects/577338238006/locations/asia-southeast1/reasoningEngines/2153529862441140224\"\n\nSESSION_ID = \"5724511082748313600\"\n\nremote_app = agent_engines.get(DEPLOYMENT_ID)\n\nremote_app.delete(force=True)\n\n\n# # remote_session = remote_app.create_session(user_id=\"u_123\")\n\n# # print(remote_session[\"id\"])\n\n# # for event in remote_app.stream_query(\n# #     user_id=\"u_123\",\n# #     session_id=SESSION_ID,\n# #     message=\"I'm going to Laos, any tips?\",\n# # ):\n# #     print(event, \"\\n\", \"=\" * 50)\n\n"
    },
    "assets": [],
    "techStack": [
      "Gemini",
      "Google ADK",
      "FastAPI",
      "Python"
    ]
  },
  {
    "id": "11",
    "title": "Travel Advisor Agent",
    "description": "ë§ì¶¤í˜• ì—¬í–‰ ì¼ì • ë° ì •ë³´ ì œê³µ ì–´ë“œë°”ì´ì €",
    "directory": "11_travel-advisor-agent",
    "readmePath": "11_travel-advisor-agent/README.md",
    "mainCodePath": "11_travel-advisor-agent/main.py",
    "readme": "",
    "codes": {
      "api_mode.ipynb": "import requests\n\n\nBASE_URL = \"http://127.0.0.1:8000\"\nAPP_NAME = \"travel_advisor_agent\"\nUSER_ID = \"u_123\"\nSESSION_ID = \"ce085ce3-9637-4eca-b7a1-b0be58fa39f1\"\n\n# ---\n\nresponse = requests.post(f\"{BASE_URL}/apps/{APP_NAME}/users/{USER_ID}/sessions\")\n\nprint(response.json())\n\n# ---\n\nmessage = {\n    \"appName\": APP_NAME,\n    \"userId\": USER_ID,\n    \"sessionId\": SESSION_ID,\n    \"newMessage\": {\n        \"parts\": [{\"text\": \"Yes, I want to know the currency exchange rate\"}],\n        \"role\": \"user\",\n    },\n}\nresponse = requests.post(f\"{BASE_URL}/run\", json=message)\n\nprint(response.json())\n\n# ---\n\ndata = response.json()\n\nfor event in data:\n    content = event.get(\"content\")\n    parts = content.get(\"parts\")\n    for part in parts:\n        function_call = part.get(\"functionCall\", None)\n        if function_call:\n            print(function_call.get(\"name\"))\n        text = part.get(\"text\", None)\n        if text:\n            print(text)\n    print(\"=\" * 60)",
      "main.py": "def main():\n    print(\"Hello from 11-travel-advisor-agent!\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    "assets": [],
    "techStack": [
      "Python"
    ]
  },
  {
    "id": "12",
    "title": "Hello Langgraph",
    "description": "LangGraph í•µì‹¬ ê¸°ëŠ¥(State, Routing, Caching ë“±) í•™ìŠµ íŠœí† ë¦¬ì–¼",
    "directory": "12_hello-langgraph",
    "readmePath": "12_hello-langgraph/README.md",
    "mainCodePath": "12_hello-langgraph/main.py",
    "readme": "# 12. Hello LangGraph\n\nì´ í”„ë¡œì íŠ¸ëŠ” LangGraphì˜ í•µì‹¬ ê°œë…ê³¼ ì£¼ìš” ê¸°ëŠ¥ë“¤ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ê¸°ì´ˆ íŠœí† ë¦¬ì–¼ ì˜ˆì œ ëª¨ìŒì…ë‹ˆë‹¤. ê° ì£¼ìš” ê¸°ëŠ¥ë³„ë¡œ ë³„ë„ì˜ Jupyter Notebook ì˜ˆì œê°€ êµ¬ì„±ë˜ì–´ ìˆì–´ LangGraphì˜ ì‘ë™ ì›ë¦¬ë¥¼ ë‹¨ê³„ë³„ë¡œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n## ì£¼ìš” ê¸°ëŠ¥ ë° ì˜ˆì œ\n\nê° ì˜ˆì œëŠ” LangGraphì˜ íŠ¹ì • ê¸°ëŠ¥ì„ ì¤‘ì ì ìœ¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤.\n\n### 1. ê¸°ë³¸ ìƒíƒœ ê´€ë¦¬ (`main_state.ipynb`)\n- `StateGraph`, `START`, `END` ë“±ì˜ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì‚¬ìš©ë²•\n- `InputState`, `OutputState`, `PrivateState` ì •ì˜ ë° í™œìš©\n- ê·¸ë˜í”„ì˜ ìƒíƒœê°€ ë…¸ë“œë¥¼ ê±°ì³ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ í•™ìŠµ\n\n### 2. ì¡°ê±´ë¶€ ì—£ì§€ (`main_conditional_edge.ipynb`)\n- `add_conditional_edges`ë¥¼ ì‚¬ìš©í•œ ë™ì  ê²½ë¡œ ì„¤ì •\n- ìƒíƒœ ë°ì´í„°ì— ë”°ë¥¸ ì¡°ê±´ë¶€ ë¡œì§ êµ¬í˜„ (ì˜ˆ: íŠ¹ì • ê°’ì— ë”°ë¥¸ ë…¸ë“œ ë¶„ê¸°)\n\n### 3. ì»¤ë§¨ë“œ API (`main_command.ipynb`)\n- `Command` ì˜¤ë¸Œì íŠ¸ë¥¼ ì´ìš©í•œ ë…¸ë“œ ì „í™˜ ë° ìƒíƒœ ë™ì‹œ ì—…ë°ì´íŠ¸\n- ë³µì¡í•œ ë¼ìš°íŒ… ë¡œì§(Triage) êµ¬í˜„ ë°©ë²•\n\n### 4. ìºì‹± ì‹œìŠ¤í…œ (`main_caching.ipynb`)\n- `CachePolicy`ì™€ `ttl`ì„ ì´ìš©í•œ ë…¸ë“œ ì‹¤í–‰ ê²°ê³¼ ìºì‹±\n- `InMemoryCache`ë¥¼ í™œìš©í•œ ê·¸ë˜í”„ ì‹¤í–‰ ìµœì í™”\n\n### 5. ìƒíƒœ ë¦¬ë“€ì„œ (`main_reduce.ipynb`)\n- `Annotated`ì™€ ë¦¬ë“€ì„œ í•¨ìˆ˜(`operator.add` ë“±)ë¥¼ ì´ìš©í•œ ìƒíƒœ ì—…ë°ì´íŠ¸ ì œì–´\n- ì—¬ëŸ¬ ë…¸ë“œì—ì„œ ë°œìƒí•˜ëŠ” ë©”ì‹œì§€ ë°ì´í„°ë¥¼ ì¤‘ì²© ì—†ì´ ëˆ„ì í•˜ëŠ” ë°©ë²•\n\n### 6. Send API (Fan-out) (`main_sendApi.ipynb`)\n- `Send` APIë¥¼ í™œìš©í•œ ë™ì  ë…¸ë“œ ë³µì œ ë° ë³‘ë ¬ ì‹¤í–‰ (Map-Reduce íŒ¨í„´)\n- ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ë°ì´í„°ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ì›Œí¬í”Œë¡œìš° êµ¬í˜„\n\n## ì‹œì‘í•˜ê¸°\n\n### í™˜ê²½ ì„¤ì •\në³¸ í”„ë¡œì íŠ¸ëŠ” `uv`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ì¡´ì„±ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.\n\n```bash\n# ì˜ì¡´ì„± ì„¤ì¹˜\nuv sync\n```\n\n### ì‹¤í–‰ ë°©ë²•\nê° `.ipynb` íŒŒì¼ì„ ì§€ì›í•˜ëŠ” ì—ë””í„°(VS Code ë“±)ì—ì„œ ì—´ì–´ ì…€ì„ ì§ì ‘ ì‹¤í–‰í•˜ë©° ê²°ê³¼ì™€ ê·¸ë˜í”„ ì‹œê°í™” ìë£Œë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nê¸°ë³¸ ì‹¤í–‰ í™•ì¸ì„ ìœ„í•œ `main.py`ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì‹¤í–‰í•©ë‹ˆë‹¤.\n```bash\npython main.py\n```\n\n---\nì´ íŠœí† ë¦¬ì–¼ì„ í†µí•´ LangGraphì˜ ìƒíƒœ ì¤‘ì‹¬ ì„¤ê³„ì™€ ì—ì´ì „í‹± ì›Œí¬í”Œë¡œìš° ëª¨ë¸ë§ì„ ì²´ë“í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "codes": {
      "main.py": "def main():\n    print(\"Hello from 12-hello-langgraph!\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "main_caching.ipynb": "from langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\nfrom langgraph.types import CachePolicy\nfrom langgraph.cache.memory import InMemoryCache\nfrom datetime import datetime\n\n\n# ---\n\nclass State(TypedDict):\n    time: str\n\n\ngraph_builder = StateGraph(State)\n\n\n# ---\n\ndef node_one(state: State):\n    # print(\"node_one ->\", state)\n    return {}\n\n\ndef node_two(state: State):\n    # print(\"node_two ->\", state)\n    return {\"time\": f\"{datetime.now()}\"}\n\n\ndef node_three(state: State):\n    # print(\"node_three ->\", state)\n    return {}\n\n\n# ---\n\ngraph_builder.add_node(\"node_one\", node_one)\ngraph_builder.add_node(\n    \"node_two\",\n    node_two,\n    cache_policy=CachePolicy(ttl=20),\n)\ngraph_builder.add_node(\"node_three\", node_three)\n\ngraph_builder.add_edge(START, \"node_one\")\ngraph_builder.add_edge(\"node_one\", \"node_two\")\ngraph_builder.add_edge(\"node_two\", \"node_three\")\ngraph_builder.add_edge(\"node_three\", END)\n\n\n# ---\n\nimport time\n\ngraph = graph_builder.compile(cache=InMemoryCache())\n\nprint(graph.invoke({}))\ntime.sleep(5)\nprint(graph.invoke({}))\ntime.sleep(5)\nprint(graph.invoke({}))\ntime.sleep(5)\nprint(graph.invoke({}))\ntime.sleep(5)\nprint(graph.invoke({}))\ntime.sleep(5)\nprint(graph.invoke({}))\ntime.sleep(5)\n",
      "main_command.ipynb": "from langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\nfrom langgraph.types import Command\n\n\n# ---\n\nclass State(TypedDict):\n    transfer_reason: str\n\n\ngraph_builder = StateGraph(State)\n\n\n# ---\n\nfrom typing import Literal\n\n\ndef triage_node(state: State) -> Command[Literal[\"account_support\", \"tech_support\"]]:\n    return Command(\n        goto=\"account_support\",\n        update={\n            \"transfer_reason\": \"The user wants to change password.\",\n        },\n    )\n\n\ndef tech_support(state: State):\n    return {}\n\n\ndef account_support(state: State):\n    print(\"account_support running\")\n    return {}\n\n\n# ---\n\ngraph_builder.add_node(\"triage_node\", triage_node)\ngraph_builder.add_node(\"tech_support\", tech_support)\ngraph_builder.add_node(\"account_support\", account_support)\n\n\ngraph_builder.add_edge(START, \"triage_node\")\n\ngraph_builder.add_edge(\"tech_support\", END)\ngraph_builder.add_edge(\"account_support\", END)\n\n\n# ---\n\ngraph = graph_builder.compile()\n\ngraph.invoke({})\n\n\n# ---\n\ngraph\n",
      "main_conditional_edge.ipynb": "from langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\nfrom typing import Literal\n\n\n# ---\n\nclass State(TypedDict):\n    seed: int\n\n\ngraph_builder = StateGraph(State)\n\n\n# ---\n\ndef node_one(state: State):\n    print(\"node_one ->\", state)\n    return {}\n\n\ndef node_two(state: State):\n    print(\"node_two ->\", state)\n    return {}\n\n\ndef node_three(state: State):\n    print(\"node_three ->\", state)\n    return {}\n\n\ndef node_four(state: State):\n    print(\"node_four ->\", state)\n    return {}\n\n\n# ---\n\ngraph_builder.add_node(\"node_one\", node_one)\ngraph_builder.add_node(\"node_two\", node_two)\ngraph_builder.add_node(\"node_three\", node_three)\ngraph_builder.add_node(\"node_four\", node_four)\n\n\n# def decide_path(state: State) -> Literal[\"node_three\", \"node_four\"]:\n#     if state[\"seed\"] % 2 == 0:\n#         return \"node_three\"\n#     else:\n#         return \"node_four\"\n\n\ndef decide_path(state: State):\n    return state[\"seed\"] % 2 == 0\n\n\ngraph_builder.add_conditional_edges(\n    START,\n    decide_path,\n    {\n        True: \"node_one\",\n        False: \"node_two\",\n        \"hello\": END,\n    },\n)\ngraph_builder.add_edge(\"node_one\", \"node_two\")\ngraph_builder.add_conditional_edges(\n    \"node_two\",\n    decide_path,\n    {\n        True: \"node_three\",\n        False: \"node_four\",\n        \"hello\": END,\n    },\n)\n\ngraph_builder.add_edge(\"node_four\", END)\ngraph_builder.add_edge(\"node_three\", END)\n\n\n# ---\n\ngraph = graph_builder.compile()\n\ngraph\n\n\n# ---\n\n# í…ŒìŠ¤íŠ¸: seedê°€ ì§ìˆ˜ì¼ ë•Œ\ngraph.invoke({\"seed\": 4})\n\n\n# ---\n\n# í…ŒìŠ¤íŠ¸: seedê°€ í™€ìˆ˜ì¼ ë•Œ\ngraph.invoke({\"seed\": 5})\n",
      "main_reduce.ipynb": "from langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\nfrom typing import Annotated\nimport operator\n\n\n# ---\n\ndef update_function(old, new):\n    return old + new\n\n\nclass State(TypedDict):\n    # messages: Annotated[list[str], update_function]\n    messages: Annotated[list[str], operator.add]\n\n\ngraph_builder = StateGraph(State)\n\n\n# ---\n\ndef node_one(state: State):\n    # print(\"node_one ->\", state)\n    return {\n        \"messages\": [\"Hello, nice to meet you!\"],\n    }\n\n\ndef node_two(state: State):\n    # print(\"node_two ->\", state)\n    return {\n        \"messages\": [\"what is your name?\"],\n    }\n\n\ndef node_three(state: State):\n    # print(\"node_three ->\", state)\n    return {}\n\n\n# ---\n\ngraph_builder.add_node(\"node_one\", node_one)\ngraph_builder.add_node(\"node_two\", node_two)\ngraph_builder.add_node(\"node_three\", node_three)\n\ngraph_builder.add_edge(START, \"node_one\")\ngraph_builder.add_edge(\"node_one\", \"node_two\")\ngraph_builder.add_edge(\"node_two\", \"node_three\")\ngraph_builder.add_edge(\"node_three\", END)\n\n\n# ---\n\ngraph = graph_builder.compile()\n\ngraph.invoke(\n    {\"messages\": [\"Hello!\"]},\n)\n",
      "main_sendApi.ipynb": "from langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict, Annotated\nfrom langgraph.types import Send\nimport operator\n\n\n# ---\n\nfrom typing import Union\n\n\nclass State(TypedDict):\n    words: list[str]\n    output: Annotated[list[dict[str, Union[str, int]]], operator.add]\n\n\ngraph_builder = StateGraph(State)\n\n\n# ---\n\ndef node_one(state: State):\n    print(f\"I want to count {len(state['words'])} words in my state.\")\n\n\ndef node_two(word: str):\n    return {\n        \"output\": [\n            {\n                \"word\": word,\n                \"letters\": len(word),\n            }\n        ]\n    }\n\n\n# ---\n\ngraph_builder.add_node(\"node_one\", node_one)\ngraph_builder.add_node(\"node_two\", node_two)\n\n\ndef dispatcher(state: State):\n    return [Send(\"node_two\", word) for word in state[\"words\"]]\n\n\ngraph_builder.add_edge(START, \"node_one\")\ngraph_builder.add_conditional_edges(\"node_one\", dispatcher, [\"node_two\"])\ngraph_builder.add_edge(\"node_two\", END)\n\n\n# ---\n\ngraph = graph_builder.compile()\n\ngraph.invoke(\n    {\n        \"words\": [\"hello\", \"world\", \"how\", \"are\", \"you\", \"doing\"],\n    }\n)\n",
      "main_state.ipynb": "from langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\n\n\n# ---\n\nclass PrivateState(TypedDict):\n    a: int\n    b: int\n\n\nclass InputState(TypedDict):\n    hello: str\n\n\nclass OutputState(TypedDict):\n    bye: str\n\n\nclass MegaPrivate(TypedDict):\n    secret: bool\n\n\ngraph_builder = StateGraph(\n    PrivateState,\n    input_schema=InputState,\n    output_schema=OutputState,\n)\n\n\n# ---\n\ndef node_one(state: InputState) -> InputState:\n    print(\"node_one ->\", state)\n    return {\"hello\": \"world\"}\n\n\ndef node_two(state: PrivateState) -> PrivateState:\n    print(\"node_two ->\", state)\n    return {\n        \"a\": 1,\n    }\n\n\ndef node_three(state: PrivateState) -> PrivateState:\n    print(\"node_three ->\", state)\n    return {\"b\": 1}\n\n\ndef node_four(state: PrivateState) -> OutputState:\n    print(\"node_four ->\", state)\n    return {\n        \"bye\": \"world\",\n    }\n\n\ndef node_five(state: OutputState):\n    return {\"secret\": True}\n\n\ndef node_six(state: MegaPrivate):\n    print(state)\n\n\n# ---\n\ngraph_builder.add_node(\"node_one\", node_one)\ngraph_builder.add_node(\"node_two\", node_two)\ngraph_builder.add_node(\"node_three\", node_three)\ngraph_builder.add_node(\"node_four\", node_four)\ngraph_builder.add_node(\"node_five\", node_five)\ngraph_builder.add_node(\"node_six\", node_six)\n\ngraph_builder.add_edge(START, \"node_one\")\ngraph_builder.add_edge(\"node_one\", \"node_two\")\ngraph_builder.add_edge(\"node_two\", \"node_three\")\ngraph_builder.add_edge(\"node_three\", \"node_four\")\ngraph_builder.add_edge(\"node_four\", \"node_five\")\ngraph_builder.add_edge(\"node_five\", \"node_six\")\ngraph_builder.add_edge(\"node_six\", END)\n\n# ---\n\ngraph = graph_builder.compile()\n\n\ngraph.invoke(\n    {\"hello\": \"world\"},\n)\n\n# ---\n\ngraph"
    },
    "assets": [],
    "techStack": [
      "LangGraph",
      "Python"
    ]
  },
  {
    "id": "13",
    "title": "Langgraph Chatbot",
    "description": "Human-in-the-loop ê¸°ëŠ¥ì´ í¬í•¨ëœ ì‹œ ì‘ì„± ë´‡",
    "directory": "13_langgraph-chatbot",
    "readmePath": "13_langgraph-chatbot/README.md",
    "mainCodePath": "13_langgraph-chatbot/main.py",
    "readme": "# LangGraph Chatbot (Mr. Poet)\n\nì´ í”„ë¡œì íŠ¸ëŠ” **LangGraph**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§Œë“  \"ì‹œ(Poem) ì‘ì„± ë´‡\"ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì— ë”°ë¼ ì‹œë¥¼ ì‘ì„±í•˜ê³ , **Human-in-the-loop** ê¸°ëŠ¥ì„ í†µí•´ ì‚¬ìš©ìì˜ í”¼ë“œë°±ì„ ë°›ì•„ ìˆ˜ì •í•©ë‹ˆë‹¤.\n\n## ì£¼ìš” ê¸°ëŠ¥\n\n*   **ì‹œ ì‘ì„±**: ì£¼ì œë¥¼ ì£¼ë©´ Gemini ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‹œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.\n*   **í”¼ë“œë°± ë°˜ì˜**: `interrupt` ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ì¤‘ê°„ì— ë©ˆì¶”ê³  ì‚¬ìš©ìì˜ í”¼ë“œë°±ì„ ìš”ì²­í•©ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ë§Œì¡±í•  ë•Œê¹Œì§€ ì‹œë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤.\n*   **LangGraph Studio ì§€ì›**: `langgraph.json` ì„¤ì •ì´ í¬í•¨ë˜ì–´ ìˆì–´ LangGraph CLI ë° Studioì—ì„œ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n\n## ì‹¤í–‰ ë°©ë²•\n\n### 1. í™˜ê²½ ì„¤ì •\n\ní•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ê³  `.env` íŒŒì¼ì— `GOOGLE_API_KEY`ê°€ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n\n```bash\n# íŒ¨í‚¤ì§€ ì„¤ì¹˜ (uv ì‚¬ìš© ì‹œ)\nuv sync\n```\n\n### 2. ì„œë²„ ì‹¤í–‰ (LangGraph CLI)\n\ní„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ ë°±ì—”ë“œ ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\n\n```bash\nlanggraph dev\n```\n\nì´ ëª…ë ¹ì–´ëŠ” ë¡œì»¬ API ì„œë²„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤ (ê¸°ë³¸ ì£¼ì†Œ: `http://127.0.0.1:2024`).\n\n### 3. UI ì—°ê²° ë° í…ŒìŠ¤íŠ¸\n\nì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ ìƒíƒœì—ì„œ **Agent Chat UI**ë¥¼ ì‚¬ìš©í•˜ì—¬ ë´‡ê³¼ ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n1.  ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ [https://agentchat.vercel.app/](https://agentchat.vercel.app/) ì ‘ì†\n2.  ì„¤ì • ì…ë ¥:\n    *   **Deployment URL**: `http://127.0.0.1:2024`\n    *   **Assistant/Graph ID**: `mr_poet`\n    *   **LangSmith API Key**: (ë¡œì»¬ ì‹¤í–‰ ì‹œ ë¹„ì›Œë‘ì–´ë„ ë¨)\n3.  **Continue** ë²„íŠ¼ í´ë¦­ í›„ ëŒ€í™” ì‹œì‘\n\n## í”„ë¡œì íŠ¸ êµ¬ì¡°\n\n*   `main.py`: ë´‡ì˜ í•µì‹¬ ë¡œì§ (ê·¸ë˜í”„ ì •ì˜, íˆ´ ì„¤ì •)\n*   `langgraph.json`: LangGraph CLI ì„¤ì • íŒŒì¼\n*   `main_checkpoint.ipynb`: (ì°¸ê³ ìš©) ì´ì „ ì£¼í”¼í„° ë…¸íŠ¸ë¶ ë²„ì „\n",
    "codes": {
      "main.py": "import sqlite3\nfrom dotenv import load_dotenv\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph import MessagesState\nfrom langgraph.prebuilt import ToolNode, tools_condition\nfrom langchain_core.tools import tool\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nfrom langgraph.types import interrupt\n\nload_dotenv()\n\n@tool\ndef get_human_feedback(poem: str):\n    \"\"\"\n    Get human feedback on a poem.\n    Use this to get feedback on a poem.\n    The user will tell you if the poem is ready or if it needs more work.\n    \"\"\"\n    response = interrupt({\"poem\": poem})\n    return response[\"feedback\"]\n\n\ntools = [get_human_feedback]\n\n# Using the user's preferred model instead of OpenAI from the commit\nllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\nllm_with_tools = llm.bind_tools(tools)\n\n\nclass State(MessagesState):\n    pass\n\n\ndef chatbot(state: State) -> State:\n    response = llm_with_tools.invoke(\n        f\"\"\"\nYou are an expert at making poems.\n\nYou are given a topic and need to write a poem about it.\n\nUse the `get_human_feedback` tool to get feedback on your poem.\n\nOnly after the user says the poem is ready, you should return the poem.\n\nHere is the conversation history:\n{state['messages']}\n\"\"\"\n    )\n    return {\n        \"messages\": [response],\n    }\n\n\ntool_node = ToolNode(\n    tools=tools,\n)\n\ngraph_builder = StateGraph(State)\n\ngraph_builder.add_node(\"chatbot\", chatbot)\ngraph_builder.add_node(\"tools\", tool_node)\n\ngraph_builder.add_edge(START, \"chatbot\")\ngraph_builder.add_conditional_edges(\"chatbot\", tools_condition)\ngraph_builder.add_edge(\"tools\", \"chatbot\")\ngraph_builder.add_edge(\"chatbot\", END)\n\n# conn = sqlite3.connect(\"memory.db\", check_same_thread=False)\n# memory = SqliteSaver(conn)\n\ngraph = graph_builder.compile(name=\"mr_poet\")\n",
      "main_async.ipynb": "import sqlite3\nimport aiosqlite\nfrom dotenv import load_dotenv\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import MessagesState\nfrom langgraph.prebuilt import ToolNode, tools_condition\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nfrom langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\nfrom langchain_core.tools import tool\nfrom langchain.chat_models import init_chat_model\n\n# Load environment variables\nload_dotenv()\n\n# Initialize the LLM\nllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n\n# Initialize SQLite connection\nconn = await aiosqlite.connect(\"memory.db\", check_same_thread=False)\n\n# ---\n\nclass State(MessagesState):\n    custom_stuff: str\n\n\ngraph_builder = StateGraph(State)\n\n# ---\n\n@tool\ndef get_weather(city: str):\n    \"\"\"Gets weather in city\"\"\"\n    return f\"The weather in {city} is sunny.\"\n\n\nllm_with_tools = llm.bind_tools(tools=[get_weather])\n\n\ndef chatbot(state: State):\n    response = llm_with_tools.invoke(state[\"messages\"])\n    return {\"messages\": [response]}\n\n# ---\n\ntool_node = ToolNode(\n    tools=[get_weather],\n)\n\ngraph_builder.add_node(\"chatbot\", chatbot)\ngraph_builder.add_node(\"tools\", tool_node)\n\n\ngraph_builder.add_edge(START, \"chatbot\")\ngraph_builder.add_conditional_edges(\"chatbot\", tools_condition)\ngraph_builder.add_edge(\"tools\", \"chatbot\")\n\ncheckpointer = AsyncSqliteSaver(conn)\nawait checkpointer.setup()\ngraph = graph_builder.compile(checkpointer=checkpointer)\n\n# ---\n\nasync for event in graph.astream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"what is the weather in berlin, budapest and bratislava.\",\n            },\n        ]\n    },\n    stream_mode=\"updates\",\n    config={\n        \"configurable\": {\n            \"thread_id\": \"2\",\n        },\n    },\n):\n    print(event)\n\n# ---\n\nasync for state in graph.aget_state_history(\n    {\n        \"configurable\": {\n            \"thread_id\": \"2\",\n        },\n    }\n):\n    print(state.next)",
      "main_checkpoint.ipynb": "import sqlite3\nfrom langgraph.graph import StateGraph, START, END\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph.message import MessagesState\nfrom langgraph.checkpoint.sqlite import SqliteSaver\n\nllm = init_chat_model(\"openai:gpt-4o-mini\")\n\nconn = sqlite3.connect(\n    \"memory.db\",\n    check_same_thread=False,\n)\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"0_x\",\n    },\n}\n\n# ---\n\nclass State(MessagesState):\n    pass\n\n\ngraph_builder = StateGraph(State)\n\n# ---\n\ndef chatbot(state: State):\n    response = llm.invoke(state[\"messages\"])\n    return {\n        \"messages\": [response],\n    }\n\n# ---\n\ngraph_builder.add_node(\"chatbot\", chatbot)\n\n\ngraph_builder.add_edge(START, \"chatbot\")\ngraph_builder.add_edge(\"chatbot\", END)\n\ngraph = graph_builder.compile(\n    checkpointer=SqliteSaver(conn),\n)\n\n# ---\n\nresult = graph.invoke(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"hello I am a user.\",\n                # I live in Europe now. And the city I live in is Valencia\n                # I live in Europe now. And the city I live in is Zagreb\n            },\n        ]\n    },\n    config=config,\n)\n\n# ---\n\nfor message in result[\"messages\"]:\n    message.pretty_print()\n\n# ---\n\nstate_history = graph.get_state_history(config)\n\n# ---\n\nfor state_snapshot in list(state_history):\n    print(state_snapshot.next)\n    print(state_snapshot.values[\"messages\"])\n    print(\"=========\\n\")\n\n# ---\n\nstate_history = graph.get_state_history(config)\n\nto_fork = list(state_history)[-11]\n\nto_fork.config\n\n# ---\n\nfrom langchain_core.messages import HumanMessage\n\ngraph.update_state(\n    to_fork.config,\n    {\n        \"messages\": [\n            HumanMessage(\n                content=\"I live in Europe now. And the city I live in is Zagreb.\",\n                id=\"86831eb9-a146-4608-8a84-b3a9366be540\",\n            )\n        ]\n    },\n)\n\n# ---\n\nforked_state = graph.get_state_history(\n    {\n        \"configurable\": {\n            \"thread_id\": \"0_x\",\n            \"checkpoint_ns\": \"\",\n            \"checkpoint_id\": \"1f0cf086-6bae-6340-800a-2f289497ccd1\",\n        }\n    }\n)\n\nlist(forked_state)\n\n# ---\n\nresult = graph.invoke(\n    None,\n    {\n        \"configurable\": {\n            \"thread_id\": \"0_x\",\n            \"checkpoint_ns\": \"\",\n            \"checkpoint_id\": \"1f0cf086-6bae-6340-800a-2f289497ccd1\",\n        }\n    },\n)\n\nfor message in result[\"messages\"]:\n    message.pretty_print()",
      "main_interrupt.ipynb": "import sqlite3\nfrom dotenv import load_dotenv\nfrom langgraph.graph import StateGraph, START, END\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph.message import MessagesState\nfrom langgraph.prebuilt import ToolNode, tools_condition\nfrom langchain_core.tools import tool\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nfrom langgraph.types import interrupt, Command\n\n# Load environment variables\nload_dotenv()\n\n# Initialize the LLM (Adapted for Gemini)\nllm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n\nconn = sqlite3.connect(\n    \"memory.db\",\n    check_same_thread=False,\n)\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"4\",\n    },\n}\n\n# ---\n\nclass State(MessagesState):\n    custom_stuff: str\n\n\ngraph_builder = StateGraph(State)\n\n# ---\n\n@tool\ndef get_human_feedback(poem: str):\n    \"\"\"\n    Asks the user for feedback on the poem.\n    Use this before returning the final response.\n    \"\"\"\n    feedback = interrupt(f\"Here is the poem, tell me what you think\\n{poem}\")\n    return feedback\n\n\nllm_with_tools = llm.bind_tools(\n    tools=[\n        get_human_feedback,\n    ]\n)\n\n\ndef chatbot(state: State):\n    response = llm_with_tools.invoke(\n        f\"\"\"\n        You are an expert in making poems.\n\n        Use the `get_human_feedback` tool to get feedback on your poem.\n\n        Only after you receive positive feedback you can return the final poem.\n\n        ALWAYS ASK FOR FEEDBACK FIRST.\n\n        Here is the conversation history:\n\n        {state[\"messages\"]}\n    \"\"\"\n    )\n    return {\n        \"messages\": [response],\n    }\n\n# ---\n\ntool_node = ToolNode(\n    tools=[\n        get_human_feedback,\n    ],\n)\n\ngraph_builder.add_node(\"chatbot\", chatbot)\ngraph_builder.add_node(\"tools\", tool_node)\n\n\ngraph_builder.add_edge(START, \"chatbot\")\ngraph_builder.add_conditional_edges(\"chatbot\", tools_condition)\ngraph_builder.add_edge(\"tools\", \"chatbot\")\n\ngraph = graph_builder.compile(\n    checkpointer=SqliteSaver(conn),\n)\n\n# ---\n\n# Run the graph (First turn)\nresult = graph.invoke(\n    {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Please make a poem about Python code.\"},\n        ]\n    },\n    config=config,\n)\n\n# ---\n\n# Check the output (It should stop at the interrupt)\nfor message in result[\"messages\"]:\n    message.pretty_print()\n\n# ---\n\n# Check the current state\nsnapshot = graph.get_state(config)\nprint(\"Next step:\", snapshot.next)\n\n# ---\n\n# Provide feedback and resume\nresponse = Command(resume=\"It looks great!\")\n\nresult = graph.invoke(\n    response,\n    config=config,\n)\nfor message in result[\"messages\"]:\n    message.pretty_print()\n\n# ---\n\ngraph"
    },
    "assets": [],
    "techStack": [
      "LangGraph",
      "Gemini"
    ]
  },
  {
    "id": "14",
    "title": "Youtube Thumnail",
    "description": "ë¹„ë””ì˜¤ ë¶„ì„ì„ í†µí•œ ìë™ ì¸ë„¤ì¼ ìƒì„±ê¸°",
    "directory": "14_youtube-thumnail",
    "readmePath": "14_youtube-thumnail/README.md",
    "mainCodePath": "14_youtube-thumnail/main.ipynb",
    "readme": "# YouTube ì¸ë„¤ì¼ ë©”ì´ì»¤\n\nYouTube ë¹„ë””ì˜¤ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³ , ì˜¤ë””ì˜¤ë¥¼ ì¶”ì¶œí•˜ì—¬ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•œ í›„, AIë¥¼ í™œìš©í•˜ì—¬ ì¸ë„¤ì¼ì„ ìë™ìœ¼ë¡œ ìƒì„±í•˜ëŠ” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.\n\n## í•„ìˆ˜ ìš”êµ¬ì‚¬í•­\n\n- Python 3.13 ì´ìƒ\n- FFmpeg (ì˜¤ë””ì˜¤ ì¶”ì¶œìš©)\n- OpenAI API í‚¤ (ì´ë¯¸ì§€ ìƒì„±ìš©)\n- Google Gemini API í‚¤ (ìŒì„± ì „ì‚¬ ë° ìš”ì•½ìš©)\n\n## ì„¤ì¹˜ ë°©ë²•\n\n1. í”„ë¡œì íŠ¸ ì˜ì¡´ì„± ì„¤ì¹˜:\n```bash\nuv sync\n```\n\n2. FFmpeg ì„¤ì¹˜:\n```bash\n# macOS\nbrew install ffmpeg\n\n# Ubuntu/Debian\nsudo apt-get install ffmpeg\n\n# Windows\n# https://ffmpeg.org/download.html ì—ì„œ ë‹¤ìš´ë¡œë“œ\n```\n\n3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •:\n```bash\n# .env íŒŒì¼ ìƒì„± ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\nexport OPENAI_API_KEY=\"your-api-key\"      # ì´ë¯¸ì§€ ìƒì„±ìš© (í•„ìˆ˜)\nexport GEMINI_API_KEY=\"your-api-key\"      # ìŒì„± ì „ì‚¬ ë° ìš”ì•½ìš© (í•„ìˆ˜)\n```\n\në˜ëŠ” `.env` íŒŒì¼ì„ ìƒì„±í•˜ì—¬ ì„¤ì •:\n```bash\nOPENAI_API_KEY=your-api-key\nGEMINI_API_KEY=your-api-key\n```\n\n## ì‚¬ìš© ë°©ë²•\n\n### ë°©ë²• 1: Jupyter ë…¸íŠ¸ë¶ ì‚¬ìš© (main.ipynb)\n\n1. **YouTube ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ**\n```bash\nuv run python download_video.py\n```\n\n2. **Jupyter ë…¸íŠ¸ë¶ ì‹¤í–‰**\n```bash\njupyter notebook main.ipynb\n```\n\në˜ëŠ” VS Codeì—ì„œ `.ipynb` íŒŒì¼ì„ ì—´ì–´ ì‹¤í–‰í•˜ì„¸ìš”.\n\n3. **ì…€ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰**\n   - Cell 0: ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì„¤ì • ë¡œë“œ\n   - Cell 1: í•¨ìˆ˜ ì •ì˜\n   - Cell 2: (ì„ íƒ) ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ í™•ì¸\n   - Cell 3: ê·¸ë˜í”„ êµ¬ì„±\n   - Cell 4: ê²°ê³¼ ì¶œë ¥ (ì„ íƒ)\n   - Cell 5: ì›Œí¬í”Œë¡œìš° ì‹¤í–‰\n   - Cell 6: ì‚¬ìš©ì í”¼ë“œë°± ì œê³µ (ì¸í„°ëŸ½íŠ¸ í›„)\n\n### ë°©ë²• 2: LangGraph API ë°°í¬ (graph.py)\n\nLangGraph APIë¥¼ í†µí•´ ì›Œí¬í”Œë¡œìš°ë¥¼ ë°°í¬í•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n1. **LangGraph CLI ì„¤ì¹˜** (í•„ìš”í•œ ê²½ìš°)\n```bash\npip install langgraph-cli\n```\n\n2. **LangGraph APIë¡œ ë°°í¬**\n```bash\nlanggraph dev\n```\n\n3. **ë¡œì»¬ì—ì„œ ì‹¤í–‰** (ì„ íƒ)\n```python\nfrom graph import graph\nfrom langgraph.types import Command\n\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\n# ì²« ë²ˆì§¸ ì‹¤í–‰ (ì¸í„°ëŸ½íŠ¸ê¹Œì§€)\nresult = graph.invoke({\"video_file\": \"temp.mp4\"}, config=config)\n\n# ì¸í„°ëŸ½íŠ¸ í›„ ì¬ê°œ (ì‚¬ìš©ì í”¼ë“œë°± ì œê³µ)\nresponse = {\n    \"chosen_thumbnail\": \"1\",  # 1-3 ì¤‘ ì„ íƒ\n    \"feedback\": \"ë¯¸ì†Œë¥¼ ë” ë°ê²Œ, ë¡œê³  ì œê±°\",\n}\nresult = graph.invoke(Command(resume=response), config=config)\n```\n\n### ë°©ë²• 3: OpenAI Whisper ì‚¬ìš© (main.py)\n\n```bash\nuv run python main.py\n```\n\n> **ì°¸ê³ **: `main.py`ëŠ” ê¸°ë³¸ì ì¸ ìŒì„± ì „ì‚¬ë§Œ ìˆ˜í–‰í•˜ë©°, ì¸ë„¤ì¼ ìƒì„± ê¸°ëŠ¥ì€ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n## ì›Œí¬í”Œë¡œìš°\n\ní”„ë¡œì íŠ¸ëŠ” LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤:\n\n1. **ë¹„ë””ì˜¤ íŒŒì¼ ì…ë ¥** â†’ `video_file` ìƒíƒœì— ì €ì¥\n2. **ì˜¤ë””ì˜¤ ì¶”ì¶œ** â†’ FFmpegë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ (2ë°°ì† ì²˜ë¦¬)\n3. **ìŒì„± ì „ì‚¬** â†’ Google Gemini APIë¡œ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\n4. **í…ìŠ¤íŠ¸ ìš”ì•½** â†’ ì „ì‚¬ëœ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ë³‘ë ¬ ìš”ì•½ í›„ í†µí•©\n5. **ì¸ë„¤ì¼ í”„ë¡¬í”„íŠ¸ ìƒì„±** â†’ ìµœì¢… ìš”ì•½ì„ ê¸°ë°˜ìœ¼ë¡œ 3ê°œì˜ ì¸ë„¤ì¼ í”„ë¡¬í”„íŠ¸ ìƒì„±\n6. **ì¸ë„¤ì¼ ì´ë¯¸ì§€ ìƒì„±** â†’ OpenAI gpt-image-1 ëª¨ë¸ë¡œ ì¸ë„¤ì¼ ì´ë¯¸ì§€ ìƒì„± (base64 ë””ì½”ë”©)\n7. **ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘** â†’ ìƒì„±ëœ ì¸ë„¤ì¼ ì¤‘ ì„ íƒ ë° í”¼ë“œë°± ì…ë ¥\n8. **ìµœì¢… ê³ í™”ì§ˆ ì¸ë„¤ì¼ ìƒì„±** â†’ ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°˜ì˜í•œ ê³ í™”ì§ˆ ì¸ë„¤ì¼ ìƒì„±\n\n## ì£¼ìš” ê¸°ëŠ¥\n\n### download_video.py\n- YouTube ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ (ìµœëŒ€ 1080p í’ˆì§ˆ)\n- ìë™ìœ¼ë¡œ MP4 í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n\n### main.ipynb\n- Google Gemini APIë¥¼ ì‚¬ìš©í•œ ìŒì„± ì „ì‚¬\n- í…ìŠ¤íŠ¸ ìš”ì•½ ë° ì¸ë„¤ì¼ í”„ë¡¬í”„íŠ¸ ìƒì„±\n- OpenAI gpt-image-1 ëª¨ë¸ì„ ì‚¬ìš©í•œ ì¸ë„¤ì¼ ì´ë¯¸ì§€ ìƒì„±\n- ì‚¬ìš©ì í”¼ë“œë°±ì„ í†µí•œ ì¸ë„¤ì¼ ê°œì„ \n- LangGraphë¥¼ í†µí•œ ì›Œí¬í”Œë¡œìš° ê´€ë¦¬\n- ì¬ì‹œë„ ë¡œì§ ë° ì—ëŸ¬ ì²˜ë¦¬\n\n### graph.py\n- LangGraph API ë°°í¬ë¥¼ ìœ„í•œ Python ìŠ¤í¬ë¦½íŠ¸\n- `main.ipynb`ì˜ ëª¨ë“  ê¸°ëŠ¥ì„ í¬í•¨\n- `langgraph.json`ê³¼ í•¨ê»˜ LangGraph APIì— ë°°í¬ ê°€ëŠ¥\n\n### langgraph.json\n- LangGraph API ë°°í¬ ì„¤ì • íŒŒì¼\n- ê·¸ë˜í”„ ì´ë¦„: `mr_thumbs`\n- ì˜ì¡´ì„± ë° í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n\n## íŒŒì¼ êµ¬ì¡°\n\n```\n.\nâ”œâ”€â”€ download_video.py      # YouTube ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸\nâ”œâ”€â”€ main.py                # OpenAI Whisperë¥¼ ì‚¬ìš©í•œ ê¸°ë³¸ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸\nâ”œâ”€â”€ main.ipynb            # Google Geminië¥¼ ì‚¬ìš©í•œ Jupyter ë…¸íŠ¸ë¶ (ì¸ë„¤ì¼ ìƒì„± í¬í•¨)\nâ”œâ”€â”€ graph.py               # LangGraph API ë°°í¬ìš© Python ìŠ¤í¬ë¦½íŠ¸\nâ”œâ”€â”€ langgraph.json        # LangGraph API ë°°í¬ ì„¤ì • íŒŒì¼\nâ”œâ”€â”€ pyproject.toml        # í”„ë¡œì íŠ¸ ì˜ì¡´ì„± ì„¤ì •\nâ””â”€â”€ README.md             # í”„ë¡œì íŠ¸ ë¬¸ì„œ\n```\n\n## ì´ë¯¸ì§€ ìƒì„± ìŠ¤íƒ€ì¼\n\n`main.ipynb` ë˜ëŠ” `graph.py`ì—ì„œ `THUMBNAIL_IMAGE_STYLE` ë³€ìˆ˜ë¥¼ ìˆ˜ì •í•˜ì—¬ ì¸ë„¤ì¼ ìŠ¤íƒ€ì¼ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n- `\"photorealistic\"`: ì‚¬ì§„ì²˜ëŸ¼ ì‚¬ì‹¤ì ì¸ ìŠ¤íƒ€ì¼\n- `\"cinematic\"`: ì˜í™”ì  ìŠ¤íƒ€ì¼ (ê¸°ë³¸ê°’)\n- `\"anime\"`: ì• ë‹ˆë©”ì´ì…˜ ìŠ¤íƒ€ì¼\n- `\"cartoon\"`: ë§Œí™” ìŠ¤íƒ€ì¼\n- `\"watercolor\"`: ìˆ˜ì±„í™” ìŠ¤íƒ€ì¼\n- `\"digital art\"`: ë””ì§€í„¸ ì•„íŠ¸ ìŠ¤íƒ€ì¼\n- `\"3D render\"`: 3D ë Œë”ë§ ìŠ¤íƒ€ì¼\n- `\"comic book\"`: ì½”ë¯¹ë¶ ìŠ¤íƒ€ì¼\n\n## ì£¼ì˜ì‚¬í•­\n\n- `main.py`ëŠ” ê¸°ë³¸ì ì¸ ìŒì„± ì „ì‚¬ë§Œ ìˆ˜í–‰í•˜ë©°, ì¸ë„¤ì¼ ìƒì„± ê¸°ëŠ¥ì€ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.\n- ì˜¤ë””ì˜¤ ì¶”ì¶œ ì‹œ 2ë°°ì†ìœ¼ë¡œ ì²˜ë¦¬ë˜ë¯€ë¡œ, ì›ë³¸ ì†ë„ê°€ í•„ìš”í•˜ë©´ `atempo=2.0` ë¶€ë¶„ì„ `atempo=1.0`ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”.\n- Gemini APIëŠ” í•œêµ­ì–´ ìš”ì•½ì„ ì§€ì›í•©ë‹ˆë‹¤.\n- OpenAI API í‚¤ì™€ Gemini API í‚¤ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤.\n- ìƒì„±ëœ ì´ë¯¸ì§€ íŒŒì¼(`thumbnail_*.jpg`)ê³¼ í”„ë¡¬í”„íŠ¸ íŒŒì¼(`thumbnail_*_prompt.txt`)ì€ í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤.\n\n## ë¬¸ì œ í•´ê²°\n\n### FFmpeg ì˜¤ë¥˜\n- FFmpegê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸: `ffmpeg -version`\n- PATHì— FFmpegê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n\n### API í‚¤ ì˜¤ë¥˜\n- í™˜ê²½ ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸: `echo $OPENAI_API_KEY` ë˜ëŠ” `echo $GEMINI_API_KEY`\n- `.env` íŒŒì¼ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° `python-dotenv`ë¡œ ë¡œë“œë©ë‹ˆë‹¤\n\n### ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨\n- YouTube URLì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸\n- ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸\n- `uv run python download_video.py`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸\n- `uv sync`ë¡œ ì˜ì¡´ì„±ì´ ì˜¬ë°”ë¥´ê²Œ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸\n\n### ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ (ModuleNotFoundError)\n- `python` ëŒ€ì‹  `uv run python`ì„ ì‚¬ìš©í•˜ì„¸ìš”\n- ì˜ˆ: `uv run python download_video.py`\n- ë˜ëŠ” ê°€ìƒí™˜ê²½ì„ í™œì„±í™”: `source .venv/bin/activate` í›„ `python download_video.py`\n\n### API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì˜¤ë¥˜\n- Gemini APIì™€ OpenAI API ëª¨ë‘ í• ë‹¹ëŸ‰ ì œí•œì´ ìˆìŠµë‹ˆë‹¤\n- ì½”ë“œì— ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ë˜ì–´ ìˆì§€ë§Œ, ê³¼ë„í•œ ì‚¬ìš© ì‹œ ëŒ€ê¸° ì‹œê°„ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n- API í‚¤ì˜ ì‚¬ìš©ëŸ‰ ì œí•œì„ í™•ì¸í•˜ì„¸ìš”\n\n### ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨\n- OpenAI API í‚¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸\n- `gpt-image-1` ëª¨ë¸ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸\n- í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ìë™ìœ¼ë¡œ ì²˜ë¦¬ë¨)\n",
    "codes": {
      "download_video.py": "import yt_dlp\nimport os\n\n\ndef download_youtube_video(url: str, output_path: str = \"./temp.mp4\"):\n    \"\"\"\n    ìœ íŠœë¸Œ ì˜ìƒì„ ë‹¤ìš´ë¡œë“œí•˜ëŠ” í•¨ìˆ˜\n    \n    Args:\n        url: ìœ íŠœë¸Œ ì˜ìƒ URL\n        output_path: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: ./temp.mp4)\n    \"\"\"\n    ydl_opts = {\n        \"format\": \"bestvideo[height<=1080]+bestaudio/best[height<=1080]\",  # ìµœê³  í’ˆì§ˆ ë¹„ë””ì˜¤+ì˜¤ë””ì˜¤\n        \"outtmpl\": output_path.replace(\".mp4\", \".%(ext)s\"),  # ì¶œë ¥ íŒŒì¼ëª… í…œí”Œë¦¿\n        \"merge_output_format\": \"mp4\",  # ë³‘í•© ì‹œ mp4ë¡œ ì¶œë ¥\n        \"postprocessors\": [\n            {\n                \"key\": \"FFmpegVideoConvertor\",\n                \"preferedformat\": \"mp4\",  # mp4ë¡œ ë³€í™˜\n            }\n        ],\n    }\n\n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n        info = ydl.extract_info(url, download=True)\n        title = info.get(\"title\", \"Unknown\")\n        print(f\"{title} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\")\n        return output_path\n\n\nif __name__ == \"__main__\":\n    # ì‚¬ìš© ì˜ˆì œ\n    url = input(\"ìœ íŠœë¸Œ ì˜ìƒ URLì„ ì…ë ¥í•˜ì„¸ìš”: \")\n    output_file = download_youtube_video(url)\n    print(f\"íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}\")\n\n",
      "graph.py": "from langgraph.graph import END, START, StateGraph\nfrom langgraph.types import Send, interrupt, Command\nfrom typing import TypedDict\nfrom typing_extensions import Annotated\nimport subprocess\nimport google.genai as genai  # type: ignore\nfrom google.genai import types\nfrom google.genai.errors import ServerError\nfrom openai import OpenAI\nimport os\nimport textwrap\nimport operator\nimport base64\nimport time\nfrom dotenv import load_dotenv\n\n# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\nload_dotenv()\n\n# ì´ë¯¸ì§€ ìƒì„± ìŠ¤íƒ€ì¼ ì„¤ì • (gpt-image-1ìš©)\n# gpt-image-1ëŠ” style íŒŒë¼ë¯¸í„°ê°€ ì—†ì§€ë§Œ í”„ë¡¬í”„íŠ¸ì— ìŠ¤íƒ€ì¼ì„ ëª…ì‹œí•˜ë©´ ë©ë‹ˆë‹¤\n# ì‚¬ìš© ê°€ëŠ¥í•œ ìŠ¤íƒ€ì¼ ì˜µì…˜:\n# - \"photorealistic\": ì‚¬ì§„ì²˜ëŸ¼ ì‚¬ì‹¤ì ì¸ ìŠ¤íƒ€ì¼\n# - \"cinematic\": ì˜í™”ì  ìŠ¤íƒ€ì¼ (ê¸°ë³¸ê°’)\n# - \"anime\": ì• ë‹ˆë©”ì´ì…˜ ìŠ¤íƒ€ì¼\n# - \"cartoon\": ë§Œí™” ìŠ¤íƒ€ì¼\n# - \"watercolor\": ìˆ˜ì±„í™” ìŠ¤íƒ€ì¼\n# - \"digital art\": ë””ì§€í„¸ ì•„íŠ¸ ìŠ¤íƒ€ì¼\n# - \"3D render\": 3D ë Œë”ë§ ìŠ¤íƒ€ì¼\n# - \"comic book\": ì½”ë¯¹ë¶ ìŠ¤íƒ€ì¼\nTHUMBNAIL_IMAGE_STYLE = \"cinematic\"  # ì›í•˜ëŠ” ìŠ¤íƒ€ì¼ë¡œ ë³€ê²½í•˜ì„¸ìš”\n\n\nclass State(TypedDict):\n    video_file: str\n    audio_file: str\n    transcription: str\n    summaries: Annotated[list[str], operator.add]  # ë³‘ë ¬ ìš”ì•½ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸\n    thumbnail_prompts: Annotated[list[str], operator.add]  # ì¸ë„¤ì¼ í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸\n    thumbnail_sketches: Annotated[list[str], operator.add]  # ìƒì„±ëœ ì¸ë„¤ì¼ íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸\n    final_summary: str  # ìµœì¢… í†µí•© ìš”ì•½\n    user_feedback: str  # ì‚¬ìš©ì í”¼ë“œë°±\n    chosen_prompt: str  # ì„ íƒëœ í”„ë¡¬í”„íŠ¸\n\n\ndef extract_audio(state: State):\n    output_file = state[\"video_file\"].replace(\"mp4\", \"mp3\")\n    command = [\n        \"ffmpeg\",\n        \"-i\",\n        state[\"video_file\"],\n        \"-filter:a\",\n        \"atempo=2.0\",\n        \"-y\",\n        output_file,\n    ]\n    subprocess.run(command)\n    return {\n        \"audio_file\": output_file,\n    }\n\n\ndef transcribe_audio(state: State):\n    # Google API í‚¤ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´ (GEMINI_API_KEY)\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    # ì˜¤ë””ì˜¤ íŒŒì¼ì„ Gemini APIì— ì—…ë¡œë“œ\n    audio_file = client.files.upload(\n        file=state[\"audio_file\"],\n        config=types.UploadFileConfig(\n            display_name=\"Audio for transcription\",\n            mime_type=\"audio/mpeg\"\n        )\n    )\n    \n    # íŒŒì¼ì´ ì²˜ë¦¬ë  ë•Œê¹Œì§€ ëŒ€ê¸°\n    while audio_file.state == \"PROCESSING\":\n        time.sleep(1)\n        audio_file = client.files.get(name=audio_file.name)\n    \n    # Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ íŒŒì¼ ì „ì‚¬ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)\n    max_retries = 10\n    retry_delay = 5\n    \n    for attempt in range(max_retries):\n        try:\n            response = client.models.generate_content(\n                model=\"gemini-2.5-flash\",\n                contents=[\n                    \"Please transcribe this audio file accurately.\",\n                    audio_file\n                ]\n            )\n            transcription = response.text\n            print(f\"âœ… ì˜¤ë””ì˜¤ ì „ì‚¬ ì™„ë£Œ!\")\n            break\n        except ServerError as e:\n            if attempt < max_retries - 1:\n                wait_time = retry_delay * (attempt + 1)\n                print(f\"âš ï¸ API ê³¼ë¶€í•˜ ì˜¤ë¥˜ ë°œìƒ. {wait_time}ì´ˆ í›„ ì¬ì‹œë„ ì¤‘... (ì‹œë„ {attempt + 1}/{max_retries})\")\n                time.sleep(wait_time)\n            else:\n                print(f\"âŒ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\")\n                raise Exception(f\"API í˜¸ì¶œ ì‹¤íŒ¨: {e}\")\n    \n    return {\n        \"transcription\": transcription,\n    }\n\n\ndef dispatch_summarizers(state: State):\n    \"\"\"\n    ì „ì‚¬ëœ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê³  ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ Send ê°ì²´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\n    ìµœëŒ€ 3ê°œì˜ ì²­í¬ë§Œ ìƒì„±\n    \"\"\"\n    transcription = state[\"transcription\"]\n    chunks = []\n    # í…ìŠ¤íŠ¸ë¥¼ ë” í° ì²­í¬ë¡œ ë‚˜ëˆ” (1000ì ë‹¨ìœ„) - ìµœëŒ€ 3ê°œë§Œ ìƒì„±\n    wrapped_chunks = textwrap.wrap(transcription, 1000)\n    # ìµœëŒ€ 3ê°œë§Œ ì‚¬ìš©\n    for i, chunk in enumerate(wrapped_chunks[:3]):\n        chunks.append({\"id\": i + 1, \"chunk\": chunk})\n    # ê° ì²­í¬ì— ëŒ€í•´ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ Send ê°ì²´ ìƒì„±\n    return [Send(\"summarize_chunk\", chunk) for chunk in chunks]\n\n\ndef summarize_chunk(chunk):\n    \"\"\"\n    ê° í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ë¡œ ìš”ì•½ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)\n    \"\"\"\n    chunk_id = chunk[\"id\"]\n    chunk_text = chunk[\"chunk\"]\n    \n    # Google Gemini API í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    # ìš”ì•½ ìš”ì²­ (í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ë„ë¡ ëª…ì‹œ, ì¬ì‹œë„ ë¡œì§ í¬í•¨)\n    max_retries = 5\n    retry_delay = 2\n    \n    for attempt in range(max_retries):\n        try:\n            response = client.models.generate_content(\n                model=\"gemini-2.5-flash\",\n                contents=f\"\"\"\n                ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.\n\n                í…ìŠ¤íŠ¸: {chunk_text}\n                \"\"\"\n            )\n            summary = f\"[Chunk {chunk_id}] {response.text}\"\n            break\n        except ServerError as e:\n            if attempt < max_retries - 1:\n                wait_time = retry_delay * (attempt + 1)\n                print(f\"ì²­í¬ {chunk_id} API ê³¼ë¶€í•˜ ì˜¤ë¥˜. {wait_time}ì´ˆ í›„ ì¬ì‹œë„ ì¤‘... (ì‹œë„ {attempt + 1}/{max_retries})\")\n                time.sleep(wait_time)\n            else:\n                raise Exception(f\"ì²­í¬ {chunk_id} API í˜¸ì¶œ ì‹¤íŒ¨: {e}\")\n    \n    return {\n        \"summaries\": [summary],\n    }\n\n\ndef mega_summary(state: State):\n    \"\"\"\n    ëª¨ë“  ì²­í¬ ìš”ì•½ì„ í†µí•©í•˜ì—¬ ìµœì¢… ìš”ì•½ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)\n    \"\"\"\n    all_summaries = \"\\n\".join(state[\"summaries\"])\n    \n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    prompt = f\"\"\"\n        You are given multiple summaries of different chunks from a video transcription.\n\n        Please create a comprehensive final summary that combines all the key points.\n\n        Individual summaries:\n\n        {all_summaries}\n    \"\"\"\n    \n    max_retries = 5\n    retry_delay = 2\n    \n    for attempt in range(max_retries):\n        try:\n            response = client.models.generate_content(\n                model=\"gemini-2.5-flash\",\n                contents=prompt\n            )\n            final_summary = response.text\n            break\n        except ServerError as e:\n            if attempt < max_retries - 1:\n                wait_time = retry_delay * (attempt + 1)\n                print(f\"ìµœì¢… ìš”ì•½ API ê³¼ë¶€í•˜ ì˜¤ë¥˜. {wait_time}ì´ˆ í›„ ì¬ì‹œë„ ì¤‘... (ì‹œë„ {attempt + 1}/{max_retries})\")\n                time.sleep(wait_time)\n            else:\n                raise Exception(f\"ìµœì¢… ìš”ì•½ API í˜¸ì¶œ ì‹¤íŒ¨: {e}\")\n    \n    return {\n        \"final_summary\": final_summary,\n    }\n\n\ndef dispatch_artists(state: State):\n    \"\"\"\n    ìµœì¢… ìš”ì•½ì„ ê¸°ë°˜ìœ¼ë¡œ 3ê°œì˜ ì¸ë„¤ì¼ ìƒì„±ì„ ìœ„í•œ ë³‘ë ¬ ì²˜ë¦¬ ë””ìŠ¤íŒ¨ì¹˜\n    \"\"\"\n    return [\n        Send(\n            \"generate_thumbnails\",\n            {\n                \"id\": i,\n                \"summary\": state[\"final_summary\"],\n            },\n        )\n        for i in [1, 2, 3]\n    ]\n\n\ndef generate_thumbnails(args):\n    \"\"\"\n    ì¸ë„¤ì¼ í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ì´ë¯¸ì§€ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)\n    \"\"\"\n    concept_id = args[\"id\"]\n    summary = args[\"summary\"]\n    \n    # ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼ ì‚¬ìš©\n    image_style = THUMBNAIL_IMAGE_STYLE\n    \n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    # ì¸ë„¤ì¼ í”„ë¡¬í”„íŠ¸ ìƒì„± (ìŠ¤íƒ€ì¼ í¬í•¨)\n    prompt = f\"\"\"\n        Based on this video summary, create a detailed visual prompt for a YouTube thumbnail.\n\n        Create a detailed prompt for generating a thumbnail image that would attract viewers. Include:\n            - Main visual elements\n            - Color scheme\n            - Text overlay suggestions\n            - Overall composition\n            - Image style: {image_style} (specify this style in the prompt)\n        \n        Summary: {summary}\n    \"\"\"\n    \n    max_retries = 5\n    retry_delay = 2\n    \n    for attempt in range(max_retries):\n        try:\n            response = client.models.generate_content(\n                model=\"gemini-2.5-flash\",\n                contents=prompt\n            )\n            thumbnail_prompt = response.text\n            break\n        except ServerError as e:\n            if attempt < max_retries - 1:\n                wait_time = retry_delay * (attempt + 1)\n                print(f\"ì¸ë„¤ì¼ {concept_id} API ê³¼ë¶€í•˜ ì˜¤ë¥˜. {wait_time}ì´ˆ í›„ ì¬ì‹œë„ ì¤‘... (ì‹œë„ {attempt + 1}/{max_retries})\")\n                time.sleep(wait_time)\n            else:\n                raise Exception(f\"ì¸ë„¤ì¼ {concept_id} API í˜¸ì¶œ ì‹¤íŒ¨: {e}\")\n    \n    # ì´ë¯¸ì§€ ìƒì„±: OpenAI gpt-image-1 ì‚¬ìš©\n    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n    \n    if openai_api_key:\n        try:\n            openai_client = OpenAI(api_key=openai_api_key)\n            \n            # í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ì–´ë¡œ ë³€í™˜\n            translation_prompt = f\"\"\"\n                Translate the following Korean thumbnail prompt to English. \n                Keep it concise and suitable for image generation.\n                Make sure to include the image style ({image_style}) explicitly in the prompt.\n                Format: Start with the style description, then describe the scene.\n                \n                Korean prompt: {thumbnail_prompt}\n            \"\"\"\n            \n            max_translation_retries = 5\n            retry_delay_translation = 3\n            \n            for trans_attempt in range(max_translation_retries):\n                try:\n                    if trans_attempt > 0:\n                        time.sleep(retry_delay_translation * trans_attempt)\n                    \n                    translation_response = client.models.generate_content(\n                        model=\"gemini-2.5-flash\",\n                        contents=translation_prompt\n                    )\n                    english_prompt = translation_response.text.strip()\n                    break\n                except ServerError as e:\n                    error_str = str(e)\n                    if \"429\" in error_str or \"RESOURCE_EXHAUSTED\" in error_str:\n                        if trans_attempt < max_translation_retries - 1:\n                            wait_time = 15 + (trans_attempt * 5)\n                            print(f\"  â³ Gemini API í• ë‹¹ëŸ‰ ì´ˆê³¼. {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...\")\n                            time.sleep(wait_time)\n                        else:\n                            print(f\"  âš ï¸ ë²ˆì—­ ì‹¤íŒ¨. ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n                            english_prompt = thumbnail_prompt[:500]\n                            break\n                    else:\n                        raise\n            \n            print(f\"  ğŸ¨ ì¸ë„¤ì¼ {concept_id} ì´ë¯¸ì§€ ìƒì„± ì¤‘... (í”„ë¡¬í”„íŠ¸: {english_prompt[:50]}...)\")\n            \n            # gpt-image-1ë¡œ ì´ë¯¸ì§€ ìƒì„± (base64 í˜•ì‹ìœ¼ë¡œ ë°˜í™˜)\n            result = openai_client.images.generate(\n                model=\"gpt-image-1\",\n                prompt=english_prompt,\n                quality=\"low\",\n                moderation=\"low\",\n                size=\"auto\",\n            )\n            \n            # base64ë¡œ ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë””ì½”ë”© ë° ì €ì¥\n            image_bytes = base64.b64decode(result.data[0].b64_json)\n            \n            image_filename = f\"thumbnail_{concept_id}.jpg\"\n            with open(image_filename, \"wb\") as file:\n                file.write(image_bytes)\n            \n            print(f\"  âœ… ì¸ë„¤ì¼ {concept_id} ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {image_filename}\")\n            \n            # í”„ë¡¬í”„íŠ¸ë„ í•¨ê»˜ ì €ì¥\n            prompt_filename = f\"thumbnail_{concept_id}_prompt.txt\"\n            with open(prompt_filename, \"w\", encoding=\"utf-8\") as file:\n                file.write(f\"Korean Prompt:\\n{thumbnail_prompt}\\n\\n\")\n                file.write(f\"English Prompt:\\n{english_prompt}\\n\")\n            \n            return {\n                \"thumbnail_prompts\": [thumbnail_prompt],\n                \"thumbnail_sketches\": [image_filename],\n            }\n            \n        except Exception as e:\n            print(f\"  âš ï¸ ì¸ë„¤ì¼ {concept_id} ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}\")\n            print(f\"  ğŸ’¡ í”„ë¡¬í”„íŠ¸ë§Œ ì €ì¥í•©ë‹ˆë‹¤.\")\n            filename = f\"thumbnail_{concept_id}_prompt.txt\"\n            with open(filename, \"w\", encoding=\"utf-8\") as file:\n                file.write(thumbnail_prompt)\n            return {\n                \"thumbnail_prompts\": [thumbnail_prompt],\n                \"thumbnail_sketches\": [filename],\n            }\n    else:\n        print(f\"  âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ë§Œ ì €ì¥í•©ë‹ˆë‹¤.\")\n        filename = f\"thumbnail_{concept_id}_prompt.txt\"\n        with open(filename, \"w\", encoding=\"utf-8\") as file:\n            file.write(thumbnail_prompt)\n        return {\n            \"thumbnail_prompts\": [thumbnail_prompt],\n            \"thumbnail_sketches\": [filename],\n        }\n\n\ndef human_feedback(state: State):\n    \"\"\"\n    ì‚¬ìš©ìë¡œë¶€í„° í”¼ë“œë°±ì„ ë°›ëŠ” ì¸í„°ëŸ½íŠ¸ í•¨ìˆ˜\n    ìƒì„±ëœ ì¸ë„¤ì¼ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ê³  í”¼ë“œë°±ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"ğŸ“‹ ìƒì„±ëœ ì¸ë„¤ì¼ ëª©ë¡:\")\n    print(\"=\"*60)\n    for i, (prompt, sketch) in enumerate(zip(state[\"thumbnail_prompts\"], state[\"thumbnail_sketches\"]), 1):\n        print(f\"\\n[ì¸ë„¤ì¼ {i}]\")\n        if sketch.endswith(\".jpg\"):\n            print(f\"  ì´ë¯¸ì§€ íŒŒì¼: {sketch}\")\n        else:\n            print(f\"  í”„ë¡¬í”„íŠ¸ íŒŒì¼: {sketch}\")\n        print(f\"  í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {prompt[:100]}...\")\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"ğŸ’¬ ì‚¬ìš©ì í”¼ë“œë°± ì…ë ¥\")\n    print(\"=\"*60)\n    \n    # LangGraphì˜ interrupt()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°\n    answer = interrupt(\n        {\n            \"chosen_thumbnail\": \"ê°€ì¥ ë§ˆìŒì— ë“œëŠ” ì¸ë„¤ì¼ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš” (1-3): \",\n            \"feedback\": \"ì›í•˜ëŠ” ìˆ˜ì • ì‚¬í•­ì´ë‚˜ í”¼ë“œë°±ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 'ë¯¸ì†Œë¥¼ ë” ë°ê²Œ', 'ë¡œê³  ì œê±°' ë“±): \",\n        }\n    )\n    \n    # ì¸í„°ëŸ½íŠ¸ì—ì„œ ë°›ì€ ë‹µë³€ ì²˜ë¦¬\n    chosen_index = int(answer.get(\"chosen_thumbnail\", \"1\")) - 1\n    \n    if chosen_index < 0 or chosen_index >= len(state[\"thumbnail_prompts\"]):\n        print(\"âš ï¸ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ 1ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n        chosen_index = 0\n    \n    user_feedback_text = answer.get(\"feedback\", \"í˜„ì¬ ìŠ¤íƒ€ì¼ ìœ ì§€\")\n    if not user_feedback_text.strip():\n        user_feedback_text = \"í˜„ì¬ ìŠ¤íƒ€ì¼ ìœ ì§€\"\n    \n    chosen_prompt = state[\"thumbnail_prompts\"][chosen_index]\n    \n    print(f\"\\nâœ… ì„ íƒëœ ì¸ë„¤ì¼: {chosen_index + 1}\")\n    print(f\"âœ… í”¼ë“œë°±: {user_feedback_text}\")\n    \n    return {\n        \"user_feedback\": user_feedback_text,\n        \"chosen_prompt\": chosen_prompt,\n    }\n\n\ndef generate_hd_thumbnail(state: State):\n    \"\"\"\n    ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ìµœì¢… ê³ í™”ì§ˆ ì¸ë„¤ì¼ ìƒì„±\n    \"\"\"\n    chosen_prompt = state[\"chosen_prompt\"]\n    user_feedback = state[\"user_feedback\"]\n    \n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n    \n    # ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°˜ì˜í•œ í”„ë¡¬í”„íŠ¸ ê°œì„ \n    improvement_prompt = f\"\"\"\n    ë‹¹ì‹ ì€ ì „ë¬¸ YouTube ì¸ë„¤ì¼ ë””ìì´ë„ˆì…ë‹ˆë‹¤. \n    ë‹¤ìŒ ì›ë³¸ ì¸ë„¤ì¼ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©ìì˜ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ê°œì„ ëœ ë²„ì „ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.\n\n    ì›ë³¸ í”„ë¡¬í”„íŠ¸:\n    {chosen_prompt}\n\n    ì‚¬ìš©ì í”¼ë“œë°±:\n    {user_feedback}\n\n    ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•  ë•Œ ë‹¤ìŒì„ í¬í•¨í•´ì£¼ì„¸ìš”:\n        1. ì›ë³¸ í”„ë¡¬í”„íŠ¸ì˜ í•µì‹¬ ê°œë… ìœ ì§€\n        2. ì‚¬ìš©ì í”¼ë“œë°±ì„ êµ¬ì²´ì ìœ¼ë¡œ ë°˜ì˜\n        3. ì „ë¬¸ì ì¸ YouTube ì¸ë„¤ì¼ ì‚¬ì–‘ ì¶”ê°€:\n            - ë†’ì€ ëŒ€ë¹„ì™€ êµµì€ ì‹œê°ì  ìš”ì†Œ\n            - ëˆˆì„ ë„ëŠ” ëª…í™•í•œ ì´ˆì \n            - ì „ë¬¸ì ì¸ ì¡°ëª…ê³¼ êµ¬ì„±\n            - í…ìŠ¤íŠ¸ ë°°ì¹˜ì™€ ê°€ë…ì„± ìµœì í™” (ê°€ì¥ìë¦¬ì—ì„œ ì¶©ë¶„í•œ ì—¬ë°± í™•ë³´)\n            - ì£¼ëª©ì„ ë„ëŠ” ìƒ‰ìƒ\n            - ì‘ì€ ì¸ë„¤ì¼ í¬ê¸°ì—ì„œë„ ì˜ ë³´ì´ëŠ” ìš”ì†Œ\n            - ì¤‘ìš”: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ê²½ê³„ ì‚¬ì´ì— í•­ìƒ ì¶©ë¶„í•œ ì—¬ë°± í™•ë³´\n    \"\"\"\n    \n    max_retries = 5\n    retry_delay = 2\n    \n    for attempt in range(max_retries):\n        try:\n            response = client.models.generate_content(\n                model=\"gemini-2.5-flash\",\n                contents=improvement_prompt\n            )\n            final_thumbnail_prompt = response.text.strip()\n            break\n        except ServerError as e:\n            if attempt < max_retries - 1:\n                wait_time = retry_delay * (attempt + 1)\n                print(f\"í”„ë¡¬í”„íŠ¸ ê°œì„  API ê³¼ë¶€í•˜ ì˜¤ë¥˜. {wait_time}ì´ˆ í›„ ì¬ì‹œë„ ì¤‘...\")\n                time.sleep(wait_time)\n            else:\n                print(\"âš ï¸ í”„ë¡¬í”„íŠ¸ ê°œì„  ì‹¤íŒ¨. ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n                final_thumbnail_prompt = chosen_prompt\n                break\n    \n    # ì˜ì–´ë¡œ ë²ˆì—­\n    translation_prompt = f\"\"\"\n    Translate the following Korean thumbnail prompt to English. \n    Keep it concise and suitable for image generation.\n    Make sure to include the image style ({THUMBNAIL_IMAGE_STYLE}) explicitly.\n    \n    Korean prompt: {final_thumbnail_prompt}\n    \"\"\"\n    \n    try:\n        translation_response = client.models.generate_content(\n            model=\"gemini-2.5-flash\",\n            contents=translation_prompt\n        )\n        english_prompt = translation_response.text.strip()\n    except Exception as e:\n        print(f\"âš ï¸ ë²ˆì—­ ì‹¤íŒ¨. ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {e}\")\n        english_prompt = final_thumbnail_prompt[:500]\n    \n    print(f\"\\nğŸ¨ ìµœì¢… ê³ í™”ì§ˆ ì¸ë„¤ì¼ ìƒì„± ì¤‘...\")\n    print(f\"í”„ë¡¬í”„íŠ¸: {english_prompt[:100]}...\")\n    \n    # gpt-image-1ë¡œ ê³ í™”ì§ˆ ì´ë¯¸ì§€ ìƒì„± (base64 í˜•ì‹ìœ¼ë¡œ ë°˜í™˜)\n    result = openai_client.images.generate(\n        model=\"gpt-image-1\",\n        prompt=english_prompt,\n        quality=\"high\",\n        moderation=\"low\",\n        size=\"auto\",\n    )\n    \n    # base64ë¡œ ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë””ì½”ë”© ë° ì €ì¥\n    image_bytes = base64.b64decode(result.data[0].b64_json)\n    \n    final_filename = \"thumbnail_final.jpg\"\n    with open(final_filename, \"wb\") as file:\n        file.write(image_bytes)\n    \n    print(f\"âœ… ìµœì¢… ì¸ë„¤ì¼ ì €ì¥ ì™„ë£Œ: {final_filename}\")\n    \n    # í”„ë¡¬í”„íŠ¸ë„ í•¨ê»˜ ì €ì¥\n    prompt_filename = \"thumbnail_final_prompt.txt\"\n    with open(prompt_filename, \"w\", encoding=\"utf-8\") as file:\n        file.write(f\"Original Prompt:\\n{chosen_prompt}\\n\\n\")\n        file.write(f\"User Feedback:\\n{user_feedback}\\n\\n\")\n        file.write(f\"Improved Prompt (Korean):\\n{final_thumbnail_prompt}\\n\\n\")\n        file.write(f\"Final Prompt (English):\\n{english_prompt}\\n\")\n    \n    return {\n        \"thumbnail_sketches\": [final_filename],\n    }\n\n\n# ê·¸ë˜í”„ êµ¬ì„±\ngraph_builder = StateGraph(State)\n\ngraph_builder.add_node(\"extract_audio\", extract_audio)\ngraph_builder.add_node(\"transcribe_audio\", transcribe_audio)\ngraph_builder.add_node(\"summarize_chunk\", summarize_chunk)\ngraph_builder.add_node(\"mega_summary\", mega_summary)\ngraph_builder.add_node(\"generate_thumbnails\", generate_thumbnails)\ngraph_builder.add_node(\"human_feedback\", human_feedback)\ngraph_builder.add_node(\"generate_hd_thumbnail\", generate_hd_thumbnail)\n\ngraph_builder.add_edge(START, \"extract_audio\")\ngraph_builder.add_edge(\"extract_audio\", \"transcribe_audio\")\ngraph_builder.add_conditional_edges(\n    \"transcribe_audio\", dispatch_summarizers, [\"summarize_chunk\"]\n)\ngraph_builder.add_edge(\"summarize_chunk\", \"mega_summary\")\ngraph_builder.add_conditional_edges(\n    \"mega_summary\", dispatch_artists, [\"generate_thumbnails\"]\n)\ngraph_builder.add_edge(\"generate_thumbnails\", \"human_feedback\")\ngraph_builder.add_edge(\"human_feedback\", \"generate_hd_thumbnail\")\ngraph_builder.add_edge(\"generate_hd_thumbnail\", END)\n\n# LangGraph API ë°°í¬ë¥¼ ìœ„í•œ ê·¸ë˜í”„ ì»´íŒŒì¼ (name ì§€ì •)\ngraph = graph_builder.compile(name=\"mr_thumbs\")\n\n",
      "main.ipynb": "from langgraph.graph import END, START, StateGraph\nfrom langgraph.types import Send, interrupt, Command\nfrom typing import TypedDict\nfrom typing_extensions import Annotated\nimport subprocess\nimport google.genai as genai  # type: ignore\nfrom google.genai import types\nfrom google.genai.errors import ServerError\nfrom openai import OpenAI\nimport requests\nimport os\nimport textwrap\nimport operator\nimport base64\nimport time\nfrom dotenv import load_dotenv\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\nload_dotenv()\n\n# ì²´í¬í¬ì¸í„° ì„¤ì • (ìƒíƒœ ì €ì¥ ë° ë³µì›ìš©)\nmemory = InMemorySaver()\n\n# ì´ë¯¸ì§€ ìƒì„± ìŠ¤íƒ€ì¼ ì„¤ì • (gpt-image-1ìš©)\n# gpt-image-1ëŠ” style íŒŒë¼ë¯¸í„°ê°€ ì—†ì§€ë§Œ í”„ë¡¬í”„íŠ¸ì— ìŠ¤íƒ€ì¼ì„ ëª…ì‹œí•˜ë©´ ë©ë‹ˆë‹¤\n# ì‚¬ìš© ê°€ëŠ¥í•œ ìŠ¤íƒ€ì¼ ì˜µì…˜:\n# - \"photorealistic\": ì‚¬ì§„ì²˜ëŸ¼ ì‚¬ì‹¤ì ì¸ ìŠ¤íƒ€ì¼\n# - \"cinematic\": ì˜í™”ì  ìŠ¤íƒ€ì¼ (ê¸°ë³¸ê°’)\n# - \"anime\": ì• ë‹ˆë©”ì´ì…˜ ìŠ¤íƒ€ì¼\n# - \"cartoon\": ë§Œí™” ìŠ¤íƒ€ì¼\n# - \"watercolor\": ìˆ˜ì±„í™” ìŠ¤íƒ€ì¼\n# - \"digital art\": ë””ì§€í„¸ ì•„íŠ¸ ìŠ¤íƒ€ì¼\n# - \"3D render\": 3D ë Œë”ë§ ìŠ¤íƒ€ì¼\n# - \"comic book\": ì½”ë¯¹ë¶ ìŠ¤íƒ€ì¼\nTHUMBNAIL_IMAGE_STYLE = \"comic book\"  # ì›í•˜ëŠ” ìŠ¤íƒ€ì¼ë¡œ ë³€ê²½í•˜ì„¸ìš”\n\n\nclass State(TypedDict):\n\n    video_file: str\n    audio_file: str\n    transcription: str\n    summaries: Annotated[list[str], operator.add]  # ë³‘ë ¬ ìš”ì•½ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸\n    thumbnail_prompts: Annotated[list[str], operator.add]  # ì¸ë„¤ì¼ í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸\n    thumbnail_sketches: Annotated[list[str], operator.add]  # ìƒì„±ëœ ì¸ë„¤ì¼ íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸\n    final_summary: str  # ìµœì¢… í†µí•© ìš”ì•½\n    user_feedback: str  # ì‚¬ìš©ì í”¼ë“œë°±\n    chosen_prompt: str  # ì„ íƒëœ í”„ë¡¬í”„íŠ¸\n\n\n# ---\n\ndef extract_audio(state: State):\n    output_file = state[\"video_file\"].replace(\"mp4\", \"mp3\")\n    command = [\n        \"ffmpeg\",\n        \"-i\",\n        state[\"video_file\"],\n        \"-filter:a\",\n        \"atempo=2.0\",\n        \"-y\",\n        output_file,\n    ]\n    subprocess.run(command)\n    return {\n        \"audio_file\": output_file,\n    }\n\n\ndef transcribe_audio(state: State):\n    # Google API í‚¤ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´ (GEMINI_API_KEY)\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    # ì˜¤ë””ì˜¤ íŒŒì¼ì„ Gemini APIì— ì—…ë¡œë“œ\n    audio_file = client.files.upload(\n        file=state[\"audio_file\"],\n        config=types.UploadFileConfig(\n            display_name=\"Audio for transcription\",\n            mime_type=\"audio/mpeg\"\n        )\n    )\n    \n    # íŒŒì¼ì´ ì²˜ë¦¬ë  ë•Œê¹Œì§€ ëŒ€ê¸°\n    while audio_file.state == \"PROCESSING\":\n        time.sleep(1)\n        audio_file = client.files.get(name=audio_file.name)\n    \n    # Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ íŒŒì¼ ì „ì‚¬ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)\n    # gemini-2.5-flash: ìµœì‹  ì•ˆì • ëª¨ë¸ (gemini-1.5-flashëŠ” ë” ì´ìƒ ì§€ì›ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)\n    # ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„ì™€ ë©€í‹°ëª¨ë‹¬ ì§€ì›ìœ¼ë¡œ ì˜¤ë””ì˜¤ ì „ì‚¬ì— ì í•©\n    max_retries = 10  # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€\n    retry_delay = 5  # ì´ˆ (ì´ˆê¸° ëŒ€ê¸° ì‹œê°„ ì¦ê°€)\n    \n    for attempt in range(max_retries):\n        try:\n            response = client.models.generate_content(\n                model=\"gemini-2.5-flash\",  # ìµœì‹  ì•ˆì • ëª¨ë¸\n                contents=[\n                    \"Please transcribe this audio file accurately.\",\n                    audio_file\n                ]\n            )\n            transcription = response.text\n            print(f\"âœ… ì˜¤ë””ì˜¤ ì „ì‚¬ ì™„ë£Œ!\")\n            break\n        except ServerError as e:\n            if attempt < max_retries - 1:\n                wait_time = retry_delay * (attempt + 1)  # ì§€ìˆ˜ ë°±ì˜¤í”„\n                print(f\"âš ï¸ API ê³¼ë¶€í•˜ ì˜¤ë¥˜ ë°œìƒ. {wait_time}ì´ˆ í›„ ì¬ì‹œë„ ì¤‘... (ì‹œë„ {attempt + 1}/{max_retries})\")\n                time.sleep(wait_time)\n            else:\n                print(f\"âŒ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\")\n                raise Exception(f\"API í˜¸ì¶œ ì‹¤íŒ¨: {e}\")\n    \n    return {\n        \"transcription\": transcription,\n    }\n\n\ndef dispatch_summarizers(state: State):\n    \"\"\"\n    ì „ì‚¬ëœ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê³  ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ Send ê°ì²´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\n    ìµœëŒ€ 3ê°œì˜ ì²­í¬ë§Œ ìƒì„±\n    \"\"\"\n    transcription = state[\"transcription\"]\n    chunks = []\n    # í…ìŠ¤íŠ¸ë¥¼ ë” í° ì²­í¬ë¡œ ë‚˜ëˆ” (1000ì ë‹¨ìœ„) - ìµœëŒ€ 3ê°œë§Œ ìƒì„±\n    wrapped_chunks = textwrap.wrap(transcription, 1000)\n    # ìµœëŒ€ 3ê°œë§Œ ì‚¬ìš©\n    for i, chunk in enumerate(wrapped_chunks[:3]):\n        chunks.append({\"id\": i + 1, \"chunk\": chunk})\n    # ê° ì²­í¬ì— ëŒ€í•´ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ Send ê°ì²´ ìƒì„±\n    return [Send(\"summarize_chunk\", chunk) for chunk in chunks]\n\n\ndef summarize_chunk(chunk):\n    \"\"\"\n    ê° í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ë¡œ ìš”ì•½ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)\n    \"\"\"\n    chunk_id = chunk[\"id\"]\n    chunk_text = chunk[\"chunk\"]\n    \n    # Google Gemini API í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    # ìš”ì•½ ìš”ì²­ (í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ë„ë¡ ëª…ì‹œ, ì¬ì‹œë„ ë¡œì§ í¬í•¨)\n    max_retries = 5\n    retry_delay = 2\n    \n    for attempt in range(max_retries):\n        try:\n            response = client.models.generate_content(\n                model=\"gemini-2.5-flash\",\n                contents=f\"\"\"\n                ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.\n\n                í…ìŠ¤íŠ¸: {chunk_text}\n                \"\"\"\n            )\n            summary = f\"[Chunk {chunk_id}] {response.text}\"\n            break\n        except ServerError as e:\n            if attempt < max_retries - 1:\n                wait_time = retry_delay * (attempt + 1)\n                print(f\"ì²­í¬ {chunk_id} API ê³¼ë¶€í•˜ ì˜¤ë¥˜. {wait_time}ì´ˆ í›„ ì¬ì‹œë„ ì¤‘... (ì‹œë„ {attempt + 1}/{max_retries})\")\n                time.sleep(wait_time)\n            else:\n                raise Exception(f\"ì²­í¬ {chunk_id} API í˜¸ì¶œ ì‹¤íŒ¨: {e}\")\n    \n    return {\n        \"summaries\": [summary],\n    }\n\n\ndef mega_summary(state: State):\n    \"\"\"\n    ëª¨ë“  ì²­í¬ ìš”ì•½ì„ í†µí•©í•˜ì—¬ ìµœì¢… ìš”ì•½ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)\n    \"\"\"\n    all_summaries = \"\\n\".join(state[\"summaries\"])\n    \n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    prompt = f\"\"\"\n        You are given multiple summaries of different chunks from a video transcription.\n\n        Please create a comprehensive final summary that combines all the key points.\n\n        Individual summaries:\n\n        {all_summaries}\n    \"\"\"\n    \n    max_retries = 5\n    retry_delay = 2\n    \n    for attempt in range(max_retries):\n        try:\n            response = client.models.generate_content(\n                model=\"gemini-2.5-flash\",\n                contents=prompt\n            )\n            final_summary = response.text\n            break\n        except ServerError as e:\n            if attempt < max_retries - 1:\n                wait_time = retry_delay * (attempt + 1)\n                print(f\"ìµœì¢… ìš”ì•½ API ê³¼ë¶€í•˜ ì˜¤ë¥˜. {wait_time}ì´ˆ í›„ ì¬ì‹œë„ ì¤‘... (ì‹œë„ {attempt + 1}/{max_retries})\")\n                time.sleep(wait_time)\n            else:\n                raise Exception(f\"ìµœì¢… ìš”ì•½ API í˜¸ì¶œ ì‹¤íŒ¨: {e}\")\n    \n    return {\n        \"final_summary\": final_summary,\n    }\n\n\ndef dispatch_artists(state: State):\n    \"\"\"\n    ìµœì¢… ìš”ì•½ì„ ê¸°ë°˜ìœ¼ë¡œ 3ê°œì˜ ì¸ë„¤ì¼ ìƒì„±ì„ ìœ„í•œ ë³‘ë ¬ ì²˜ë¦¬ ë””ìŠ¤íŒ¨ì¹˜\n    \"\"\"\n    return [\n        Send(\n            \"generate_thumbnails\",\n            {\n                \"id\": i,\n                \"summary\": state[\"final_summary\"],\n            },\n        )\n        for i in [1, 2, 3]\n    ]\n\n\ndef generate_thumbnails(args):\n    \"\"\"\n    ì¸ë„¤ì¼ í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ì´ë¯¸ì§€ ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)\n    \"\"\"\n    concept_id = args[\"id\"]\n    summary = args[\"summary\"]\n    \n    # ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼ ì‚¬ìš© (ì…€ 0ì—ì„œ ì •ì˜ëœ ì „ì—­ ë³€ìˆ˜ ì‚¬ìš©)\n    image_style = THUMBNAIL_IMAGE_STYLE\n    \n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    # ì¸ë„¤ì¼ í”„ë¡¬í”„íŠ¸ ìƒì„± (ìŠ¤íƒ€ì¼ í¬í•¨)\n    prompt = f\"\"\"\n        Based on this video summary, create a detailed visual prompt for a YouTube thumbnail.\n\n        Create a detailed prompt for generating a thumbnail image that would attract viewers. Include:\n            - Main visual elements\n            - Color scheme\n            - Text overlay suggestions\n            - Overall composition\n            - Image style: {image_style} (specify this style in the prompt)\n        \n        Summary: {summary}\n    \"\"\"\n    \n    max_retries = 5\n    retry_delay = 2\n    \n    for attempt in range(max_retries):\n        try:\n            response = client.models.generate_content(\n                model=\"gemini-2.5-flash\",\n                contents=prompt\n            )\n            thumbnail_prompt = response.text\n            break\n        except ServerError as e:\n            if attempt < max_retries - 1:\n                wait_time = retry_delay * (attempt + 1)\n                print(f\"ì¸ë„¤ì¼ {concept_id} API ê³¼ë¶€í•˜ ì˜¤ë¥˜. {wait_time}ì´ˆ í›„ ì¬ì‹œë„ ì¤‘... (ì‹œë„ {attempt + 1}/{max_retries})\")\n                time.sleep(wait_time)\n            else:\n                raise Exception(f\"ì¸ë„¤ì¼ {concept_id} API í˜¸ì¶œ ì‹¤íŒ¨: {e}\")\n    \n    # ì´ë¯¸ì§€ ìƒì„±: OpenAI gpt-image-1 ì‚¬ìš©\n    # ì°¸ê³ : Gemini ë¬´ë£Œ APIëŠ” ì´ë¯¸ì§€ ìƒì„±ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ OpenAI gpt-image-1 ì‚¬ìš©\n    # OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤: export OPENAI_API_KEY=\"your-api-key\"\n    \n    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n    \n    if openai_api_key:\n        try:\n            openai_client = OpenAI(api_key=openai_api_key)\n            \n            # í”„ë¡¬í”„íŠ¸ë¥¼ ì˜ì–´ë¡œ ë³€í™˜ (DALL-EëŠ” ì˜ì–´ í”„ë¡¬í”„íŠ¸ê°€ ë” ì˜ ì‘ë™í•¨)\n            # Gemini API í• ë‹¹ëŸ‰ ì œí•œì„ í”¼í•˜ê¸° ìœ„í•´ ì¬ì‹œë„ ë¡œì§ ì¶”ê°€\n            # ìŠ¤íƒ€ì¼ì„ ëª…ì‹œì ìœ¼ë¡œ í¬í•¨í•˜ë„ë¡ ìš”ì²­\n            translation_prompt = f\"\"\"\n                Translate the following Korean thumbnail prompt to English. \n                Keep it concise and suitable for image generation.\n                Make sure to include the image style ({image_style}) explicitly in the prompt.\n                Format: Start with the style description, then describe the scene.\n                \n                Korean prompt: {thumbnail_prompt}\n            \"\"\"\n            \n            max_translation_retries = 5\n            retry_delay_translation = 3\n            \n            for trans_attempt in range(max_translation_retries):\n                try:\n                    # API í• ë‹¹ëŸ‰ ì œí•œì„ í”¼í•˜ê¸° ìœ„í•´ ì•½ê°„ì˜ ì§€ì—°\n                    if trans_attempt > 0:\n                        time.sleep(retry_delay_translation * trans_attempt)\n                    \n                    translation_response = client.models.generate_content(\n                        model=\"gemini-2.5-flash\",\n                        contents=translation_prompt\n                    )\n                    english_prompt = translation_response.text.strip()\n                    break\n                except ServerError as e:\n                    error_str = str(e)\n                    if \"429\" in error_str or \"RESOURCE_EXHAUSTED\" in error_str:\n                        if trans_attempt < max_translation_retries - 1:\n                            wait_time = 15 + (trans_attempt * 5)  # í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ë” ê¸´ ëŒ€ê¸°\n                            print(f\"  â³ Gemini API í• ë‹¹ëŸ‰ ì´ˆê³¼. {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...\")\n                            time.sleep(wait_time)\n                        else:\n                            # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (ì˜ì–´ë¡œ ê°„ë‹¨íˆ ë³€í™˜)\n                            print(f\"  âš ï¸ ë²ˆì—­ ì‹¤íŒ¨. ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n                            english_prompt = thumbnail_prompt[:500]  # ì›ë³¸ ì‚¬ìš© (ê¸¸ì´ ì œí•œ)\n                            break\n                    else:\n                        raise\n            \n            print(f\"  ğŸ¨ ì¸ë„¤ì¼ {concept_id} ì´ë¯¸ì§€ ìƒì„± ì¤‘... (í”„ë¡¬í”„íŠ¸: {english_prompt[:50]}...)\")\n            \n            # gpt-image-1ë¡œ ì´ë¯¸ì§€ ìƒì„± (base64 í˜•ì‹ìœ¼ë¡œ ë°˜í™˜)\n            result = openai_client.images.generate(\n                model=\"gpt-image-1\",\n                prompt=english_prompt,\n                quality=\"low\",\n                moderation=\"low\",\n                size=\"auto\",\n            )\n            \n            # base64ë¡œ ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë””ì½”ë”© ë° ì €ì¥\n            image_bytes = base64.b64decode(result.data[0].b64_json)\n            \n            image_filename = f\"thumbnail_{concept_id}.jpg\"\n            with open(image_filename, \"wb\") as file:\n                file.write(image_bytes)\n            \n            print(f\"  âœ… ì¸ë„¤ì¼ {concept_id} ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {image_filename}\")\n            \n            # í”„ë¡¬í”„íŠ¸ë„ í•¨ê»˜ ì €ì¥\n            prompt_filename = f\"thumbnail_{concept_id}_prompt.txt\"\n            with open(prompt_filename, \"w\", encoding=\"utf-8\") as file:\n                file.write(f\"Korean Prompt:\\n{thumbnail_prompt}\\n\\n\")\n                file.write(f\"English Prompt:\\n{english_prompt}\\n\")\n            \n            return {\n                \"thumbnail_prompts\": [thumbnail_prompt],\n                \"thumbnail_sketches\": [image_filename],\n            }\n            \n        except Exception as e:\n            print(f\"  âš ï¸ ì¸ë„¤ì¼ {concept_id} ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}\")\n            print(f\"  ğŸ’¡ í”„ë¡¬í”„íŠ¸ë§Œ ì €ì¥í•©ë‹ˆë‹¤.\")\n            # ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨ ì‹œ í”„ë¡¬í”„íŠ¸ë§Œ ì €ì¥\n            filename = f\"thumbnail_{concept_id}_prompt.txt\"\n            with open(filename, \"w\", encoding=\"utf-8\") as file:\n                file.write(thumbnail_prompt)\n            return {\n                \"thumbnail_prompts\": [thumbnail_prompt],\n                \"thumbnail_sketches\": [filename],\n            }\n    else:\n        # OpenAI API í‚¤ê°€ ì—†ëŠ” ê²½ìš° í”„ë¡¬í”„íŠ¸ë§Œ ì €ì¥\n        print(f\"  âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ë§Œ ì €ì¥í•©ë‹ˆë‹¤.\")\n        filename = f\"thumbnail_{concept_id}_prompt.txt\"\n        with open(filename, \"w\", encoding=\"utf-8\") as file:\n            file.write(thumbnail_prompt)\n        return {\n            \"thumbnail_prompts\": [thumbnail_prompt],\n            \"thumbnail_sketches\": [filename],\n        }\n\n\ndef human_feedback(state: State):\n    \"\"\"\n    ì‚¬ìš©ìë¡œë¶€í„° í”¼ë“œë°±ì„ ë°›ëŠ” ì¸í„°ëŸ½íŠ¸ í•¨ìˆ˜\n    ìƒì„±ëœ ì¸ë„¤ì¼ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ê³  í”¼ë“œë°±ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"ğŸ“‹ ìƒì„±ëœ ì¸ë„¤ì¼ ëª©ë¡:\")\n    print(\"=\"*60)\n    for i, (prompt, sketch) in enumerate(zip(state[\"thumbnail_prompts\"], state[\"thumbnail_sketches\"]), 1):\n        print(f\"\\n[ì¸ë„¤ì¼ {i}]\")\n        if sketch.endswith(\".jpg\"):\n            print(f\"  ì´ë¯¸ì§€ íŒŒì¼: {sketch}\")\n        else:\n            print(f\"  í”„ë¡¬í”„íŠ¸ íŒŒì¼: {sketch}\")\n        print(f\"  í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {prompt[:100]}...\")\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"ğŸ’¬ ì‚¬ìš©ì í”¼ë“œë°± ì…ë ¥\")\n    print(\"=\"*60)\n    \n    # LangGraphì˜ interrupt()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°\n    # ì¸í„°ëŸ½íŠ¸ í›„ Command(resume=...)ë¡œ ì¬ê°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n    answer = interrupt(\n        {\n            \"chosen_thumbnail\": \"ê°€ì¥ ë§ˆìŒì— ë“œëŠ” ì¸ë„¤ì¼ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš” (1-3): \",\n            \"feedback\": \"ì›í•˜ëŠ” ìˆ˜ì • ì‚¬í•­ì´ë‚˜ í”¼ë“œë°±ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 'ë¯¸ì†Œë¥¼ ë” ë°ê²Œ', 'ë¡œê³  ì œê±°' ë“±): \",\n        }\n    )\n    \n    # ì¸í„°ëŸ½íŠ¸ì—ì„œ ë°›ì€ ë‹µë³€ ì²˜ë¦¬\n    chosen_index = int(answer.get(\"chosen_thumbnail\", \"1\")) - 1\n    \n    if chosen_index < 0 or chosen_index >= len(state[\"thumbnail_prompts\"]):\n        print(\"âš ï¸ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ 1ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n        chosen_index = 0\n    \n    user_feedback_text = answer.get(\"feedback\", \"í˜„ì¬ ìŠ¤íƒ€ì¼ ìœ ì§€\")\n    if not user_feedback_text.strip():\n        user_feedback_text = \"í˜„ì¬ ìŠ¤íƒ€ì¼ ìœ ì§€\"\n    \n    chosen_prompt = state[\"thumbnail_prompts\"][chosen_index]\n    \n    print(f\"\\nâœ… ì„ íƒëœ ì¸ë„¤ì¼: {chosen_index + 1}\")\n    print(f\"âœ… í”¼ë“œë°±: {user_feedback_text}\")\n    \n    return {\n        \"user_feedback\": user_feedback_text,\n        \"chosen_prompt\": chosen_prompt,\n    }\n\n\ndef generate_hd_thumbnail(state: State):\n    \"\"\"\n    ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ìµœì¢… ê³ í™”ì§ˆ ì¸ë„¤ì¼ ìƒì„±\n    \"\"\"\n    chosen_prompt = state[\"chosen_prompt\"]\n    user_feedback = state[\"user_feedback\"]\n    \n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n    \n    # ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°˜ì˜í•œ í”„ë¡¬í”„íŠ¸ ê°œì„ \n    improvement_prompt = f\"\"\"\n    ë‹¹ì‹ ì€ ì „ë¬¸ YouTube ì¸ë„¤ì¼ ë””ìì´ë„ˆì…ë‹ˆë‹¤. \n    ë‹¤ìŒ ì›ë³¸ ì¸ë„¤ì¼ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©ìì˜ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ê°œì„ ëœ ë²„ì „ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.\n\n    ì›ë³¸ í”„ë¡¬í”„íŠ¸:\n    {chosen_prompt}\n\n    ì‚¬ìš©ì í”¼ë“œë°±:\n    {user_feedback}\n\n    ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•  ë•Œ ë‹¤ìŒì„ í¬í•¨í•´ì£¼ì„¸ìš”:\n        1. ì›ë³¸ í”„ë¡¬í”„íŠ¸ì˜ í•µì‹¬ ê°œë… ìœ ì§€\n        2. ì‚¬ìš©ì í”¼ë“œë°±ì„ êµ¬ì²´ì ìœ¼ë¡œ ë°˜ì˜\n        3. ì „ë¬¸ì ì¸ YouTube ì¸ë„¤ì¼ ì‚¬ì–‘ ì¶”ê°€:\n            - ë†’ì€ ëŒ€ë¹„ì™€ êµµì€ ì‹œê°ì  ìš”ì†Œ\n            - ëˆˆì„ ë„ëŠ” ëª…í™•í•œ ì´ˆì \n            - ì „ë¬¸ì ì¸ ì¡°ëª…ê³¼ êµ¬ì„±\n            - í…ìŠ¤íŠ¸ ë°°ì¹˜ì™€ ê°€ë…ì„± ìµœì í™” (ê°€ì¥ìë¦¬ì—ì„œ ì¶©ë¶„í•œ ì—¬ë°± í™•ë³´)\n            - ì£¼ëª©ì„ ë„ëŠ” ìƒ‰ìƒ\n            - ì‘ì€ ì¸ë„¤ì¼ í¬ê¸°ì—ì„œë„ ì˜ ë³´ì´ëŠ” ìš”ì†Œ\n            - ì¤‘ìš”: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ê²½ê³„ ì‚¬ì´ì— í•­ìƒ ì¶©ë¶„í•œ ì—¬ë°± í™•ë³´\n    \"\"\"\n    \n    max_retries = 5\n    retry_delay = 2\n    \n    for attempt in range(max_retries):\n        try:\n            response = client.models.generate_content(\n                model=\"gemini-2.5-flash\",\n                contents=improvement_prompt\n            )\n            final_thumbnail_prompt = response.text.strip()\n            break\n        except ServerError as e:\n            if attempt < max_retries - 1:\n                wait_time = retry_delay * (attempt + 1)\n                print(f\"í”„ë¡¬í”„íŠ¸ ê°œì„  API ê³¼ë¶€í•˜ ì˜¤ë¥˜. {wait_time}ì´ˆ í›„ ì¬ì‹œë„ ì¤‘...\")\n                time.sleep(wait_time)\n            else:\n                # ì‹¤íŒ¨ ì‹œ ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©\n                print(\"âš ï¸ í”„ë¡¬í”„íŠ¸ ê°œì„  ì‹¤íŒ¨. ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n                final_thumbnail_prompt = chosen_prompt\n                break\n    \n    # ì˜ì–´ë¡œ ë²ˆì—­\n    translation_prompt = f\"\"\"\n    Translate the following Korean thumbnail prompt to English. \n    Keep it concise and suitable for image generation.\n    Make sure to include the image style ({THUMBNAIL_IMAGE_STYLE}) explicitly.\n    \n    Korean prompt: {final_thumbnail_prompt}\n    \"\"\"\n    \n    try:\n        translation_response = client.models.generate_content(\n            model=\"gemini-2.5-flash\",\n            contents=translation_prompt\n        )\n        english_prompt = translation_response.text.strip()\n    except Exception as e:\n        print(f\"âš ï¸ ë²ˆì—­ ì‹¤íŒ¨. ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {e}\")\n        english_prompt = final_thumbnail_prompt[:500]\n    \n    print(f\"\\nğŸ¨ ìµœì¢… ê³ í™”ì§ˆ ì¸ë„¤ì¼ ìƒì„± ì¤‘...\")\n    print(f\"í”„ë¡¬í”„íŠ¸: {english_prompt[:100]}...\")\n    \n    # gpt-image-1ë¡œ ê³ í™”ì§ˆ ì´ë¯¸ì§€ ìƒì„± (base64 í˜•ì‹ìœ¼ë¡œ ë°˜í™˜)\n    result = openai_client.images.generate(\n        model=\"gpt-image-1\",\n        prompt=english_prompt,\n        quality=\"high\",\n        moderation=\"low\",\n        size=\"auto\",\n    )\n    \n    # base64ë¡œ ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë””ì½”ë”© ë° ì €ì¥\n    image_bytes = base64.b64decode(result.data[0].b64_json)\n    \n    final_filename = \"thumbnail_final.jpg\"\n    with open(final_filename, \"wb\") as file:\n        file.write(image_bytes)\n    \n    print(f\"âœ… ìµœì¢… ì¸ë„¤ì¼ ì €ì¥ ì™„ë£Œ: {final_filename}\")\n    \n    # í”„ë¡¬í”„íŠ¸ë„ í•¨ê»˜ ì €ì¥\n    prompt_filename = \"thumbnail_final_prompt.txt\"\n    with open(prompt_filename, \"w\", encoding=\"utf-8\") as file:\n        file.write(f\"Original Prompt:\\n{chosen_prompt}\\n\\n\")\n        file.write(f\"User Feedback:\\n{user_feedback}\\n\\n\")\n        file.write(f\"Improved Prompt (Korean):\\n{final_thumbnail_prompt}\\n\\n\")\n        file.write(f\"Final Prompt (English):\\n{english_prompt}\\n\")\n    \n    return {\n        \"thumbnail_sketches\": [final_filename],\n    }\n\n\n# ---\n\n# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ í™•ì¸ (ë¬¸ì œ ë°œìƒ ì‹œ ì‹¤í–‰)\nimport google.genai as genai\nimport os\n\nclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\nmodels = client.models.list()\nprint(\"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡:\")\nfor model in models:\n    print(f\"  - {model.name}\")\n    if hasattr(model, 'supported_generation_methods'):\n        print(f\"    ì§€ì› ë©”ì„œë“œ: {model.supported_generation_methods}\")\n\n\n# ---\n\ngraph_builder = StateGraph(State)\n\ngraph_builder.add_node(\"extract_audio\", extract_audio)\ngraph_builder.add_node(\"transcribe_audio\", transcribe_audio)\ngraph_builder.add_node(\"summarize_chunk\", summarize_chunk)\ngraph_builder.add_node(\"mega_summary\", mega_summary)\ngraph_builder.add_node(\"generate_thumbnails\", generate_thumbnails)\ngraph_builder.add_node(\"human_feedback\", human_feedback)\ngraph_builder.add_node(\"generate_hd_thumbnail\", generate_hd_thumbnail)\n\ngraph_builder.add_edge(START, \"extract_audio\")\ngraph_builder.add_edge(\"extract_audio\", \"transcribe_audio\")\n# ì „ì‚¬ í›„ ìš”ì•½ì„ ìœ„í•œ ë³‘ë ¬ ì²˜ë¦¬ ì—£ì§€ ì¶”ê°€\ngraph_builder.add_conditional_edges(\n    \"transcribe_audio\", dispatch_summarizers, [\"summarize_chunk\"]\n)\ngraph_builder.add_edge(\"summarize_chunk\", \"mega_summary\")\n# ìµœì¢… ìš”ì•½ í›„ ì¸ë„¤ì¼ ìƒì„±ì„ ìœ„í•œ ë³‘ë ¬ ì²˜ë¦¬ ì—£ì§€ ì¶”ê°€\ngraph_builder.add_conditional_edges(\n    \"mega_summary\", dispatch_artists, [\"generate_thumbnails\"]\n)\ngraph_builder.add_edge(\"generate_thumbnails\", \"human_feedback\")\ngraph_builder.add_edge(\"human_feedback\", \"generate_hd_thumbnail\")\ngraph_builder.add_edge(\"generate_hd_thumbnail\", END)\n\n# ì²´í¬í¬ì¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒíƒœ ì €ì¥ ë° ë³µì› ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •\ngraph = graph_builder.compile(checkpointer=memory)\n\n\n# ---\n\n# ê²°ê³¼ ì¶œë ¥ (ì¸í„°ëŸ½íŠ¸ í›„ì—ë„ ì‚¬ìš© ê°€ëŠ¥)\nresult = globals().get('result')\nif result is not None:\n    print(\"\\n=== ì „ì‚¬ ê²°ê³¼ ===\")\n    print(result.get(\"transcription\", \"\")[:500] + \"...\" if len(result.get(\"transcription\", \"\")) > 500 else result.get(\"transcription\", \"\"))\n\n    print(\"\\n=== ì²­í¬ë³„ ìš”ì•½ ê²°ê³¼ ===\")\n    for summary in result.get(\"summaries\", []):\n        print(summary)\n\n    print(\"\\n=== ìµœì¢… í†µí•© ìš”ì•½ ===\")\n    print(result.get(\"final_summary\", \"\"))\n\n    print(\"\\n=== ìƒì„±ëœ ì¸ë„¤ì¼ íŒŒì¼ ===\")\n    for sketch in result.get(\"thumbnail_sketches\", []):\n        print(f\"  - {sketch}\")\n\n    print(\"\\n=== ìµœì¢… ì¸ë„¤ì¼ ===\")\n    if \"thumbnail_final.jpg\" in result.get(\"thumbnail_sketches\", []):\n        print(\"âœ… ìµœì¢… ì¸ë„¤ì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: thumbnail_final.jpg\")\nelse:\n    print(\"âš ï¸ resultê°€ ì•„ì§ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Cell 5ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n\n\n# ---\n\n# ì„¤ì •: ì„¸ì…˜ ID (ì²´í¬í¬ì¸í„°ì—ì„œ ì‚¬ìš©)\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"1\",\n    },\n}\n\n# ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ë° ìš”ì•½ ì‹¤í–‰\n# human_feedback ë…¸ë“œì—ì„œ interrupt()ê°€ ë°œìƒí•˜ì—¬ ì‚¬ìš©ì ì…ë ¥ì„ ë°›ìŠµë‹ˆë‹¤\n# ì¸í„°ëŸ½íŠ¸ê°€ ë°œìƒí•˜ë©´ Command(resume=...)ë¡œ ì¬ê°œí•´ì•¼ í•©ë‹ˆë‹¤\nprint(\"ğŸš€ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘...\")\nresult = graph.invoke(\n    {\"video_file\": \"temp.mp4\"},\n    config=config,\n)\n\n\n# ---\n\n# ì˜µì…˜: ì‚¬ìš©ì í”¼ë“œë°±ì„ í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ ì œê³µí•˜ëŠ” ê²½ìš°\n# human_feedback ë…¸ë“œì—ì„œ interrupt()ê°€ ë°œìƒí–ˆì„ ë•Œ ì‚¬ìš©\n\n# ì˜ˆì‹œ: ì‚¬ìš©ì í”¼ë“œë°± ì œê³µ í›„ ì›Œí¬í”Œë¡œìš° ì¬ê°œ\nresponse = {\n    \"chosen_thumbnail\": \"1\",  # 1-3 ì¤‘ ì„ íƒ (ë¬¸ìì—´)\n    \"feedback\": \"ë¯¸ì†Œë¥¼ ë” ë°ê²Œ, ë¡œê³  ì œê±°, ì‚¬ì§„ì²˜ëŸ¼ ì‚¬ì‹¤ì ì¸ ìŠ¤íƒ€ì¼ë¡œ ë³€ê²½\",\n}\n\nresult = graph.invoke(\n    Command(resume=response),\n    config=config,\n)\n\nprint(\"\\n=== ìµœì¢… ì¸ë„¤ì¼ ===\")\nprint(\"âœ… ìµœì¢… ì¸ë„¤ì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: thumbnail_final.jpg\")\n",
      "main.py": "from langgraph.graph import END, START, StateGraph\nfrom typing import TypedDict\nimport subprocess\nfrom openai import OpenAI\n\n\nclass State(TypedDict):\n    video_file: str\n    audio_file: str\n    transcription: str\n\n\ndef extract_audio(state: State):\n    output_file = state[\"video_file\"].replace(\"mp4\", \"mp3\")\n    command = [\n        \"ffmpeg\",\n        \"-i\",\n        state[\"video_file\"],\n        \"-filter:a\",\n        \"atempo=2.0\",\n        \"-y\",\n        output_file,\n    ]\n    subprocess.run(command)\n    return {\n        \"audio_file\": output_file,\n    }\n\n\ndef transcribe_audio(state: State):\n    client = OpenAI()\n    with open(state[\"audio_file\"], \"rb\") as audio_file:\n        transcription = client.audio.transcriptions.create(\n            model=\"whisper-1\",\n            response_format=\"text\",\n            file=audio_file,\n            language=\"en\",\n            prompt=\"Netherlands, Rotterdam, Amsterdam, The Hage\",\n        )\n        return {\n            \"transcription\": transcription,\n        }\n\n\ndef main():\n    graph_builder = StateGraph(State)\n\n    graph_builder.add_node(\"extract_audio\", extract_audio)\n    graph_builder.add_node(\"transcribe_audio\", transcribe_audio)\n\n    graph_builder.add_edge(START, \"extract_audio\")\n    graph_builder.add_edge(\"extract_audio\", \"transcribe_audio\")\n    graph_builder.add_edge(\"transcribe_audio\", END)\n\n    graph = graph_builder.compile()\n\n    # ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ ì‹¤í–‰\n    result = graph.invoke({\"video_file\": \"netherlands.mp4\"})\n    print(\"Transcription:\", result.get(\"transcription\"))\n\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    "assets": [
      "assets/14_youtube-thumnail/thumbnail_1.jpg",
      "assets/14_youtube-thumnail/thumbnail_2.jpg",
      "assets/14_youtube-thumnail/thumbnail_3.jpg",
      "assets/14_youtube-thumnail/thumbnail_final.jpg"
    ],
    "techStack": [
      "LangGraph",
      "Gemini",
      "Python",
      "OpenAI"
    ]
  },
  {
    "id": "15",
    "title": "Workflow Architectures",
    "description": "ê³ ê¸‰ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì•„í‚¤í…ì²˜ íŒ¨í„´",
    "directory": "15_workflow-architectures",
    "readmePath": "15_workflow-architectures/README.md",
    "mainCodePath": "15_workflow-architectures/main.py",
    "readme": "# LangGraph Workflow Architectures\n\nA repository exploring advanced agentic workflow patterns using [LangGraph](https://langchain-ai.github.io/langgraph/) and [Google Gemini](https://deepmind.google/technologies/gemini/).\n\n## Overview\n\nThis project demonstrates how to build intelligent, adaptive workflows. The current implementation features a **Router Workflow** that dynamically assesses the difficulty of a user's question and routes it to the most appropriate processing node.\n\n## Key Features\n\n- **Router Pattern**: Automatically classifies input questions as \"Easy\", \"Medium\", or \"Hard\".\n- **Dynamic Routing**: Uses LangGraph's `Command` object to route execution flow based on the classification.\n- **Gemini Powered**: All components are powered by Google's `gemini-2.5-flash` model, ensuring high performance and efficiency.\n- **Structured Output**: Utilizes Pydantic models for reliable classification and structured responses.\n\n## Workflow Structure\n\n1. **`assess_difficulty`**: The entry point. Analyzes the question and determines its complexity.\n2. **Routing**:\n    - **Easy** -> `dumb_node` (Simulated GPT-3.5 level)\n    - **Medium** -> `average_node` (Simulated GPT-4o level)\n    - **Hard** -> `smart_node` (Simulated GPT-o3 level)\n3. **Execution**: The selected node generates the final answer.\n\n## Setup\n\n### Prerequisites\n\n- Python 3.10 or higher\n- `uv` (recommended for package management)\n\n### Installation\n\n1. Clone the repository.\n2. Install dependencies:\n    ```bash\n    uv sync\n    # or\n    pip install -r requirements.txt\n    ```\n\n### Configuration\n\nCreate a `.env` file in the root directory and add your Gemini API key:\n\n```env\nGEMINI_API_KEY=your_api_key_here\n```\n\n## Usage\n\nRun the Jupyter Notebook to see the workflow in action:\n\n```bash\njupyter notebook main.ipynb\n```\n",
    "codes": {
      "main.py": "def main():\n    print(\"Hello from 15-workflow-architectures!\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "main_evaluator.ipynb": "import os\nfrom dotenv import load_dotenv\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom typing import TypedDict, Literal\nfrom langgraph.graph import StateGraph, START, END\nfrom pydantic import BaseModel, Field\n\nload_dotenv()\n\nllm = ChatGoogleGenerativeAI(\n    model=\"gemini-2.5-flash\",\n    google_api_key=os.environ[\"GEMINI_API_KEY\"]\n)\n\n# ---\n\nclass State(TypedDict):\n    topic: str\n    joke: str\n    critique: str\n    status: Literal[\"accepted\", \"rejected\"]\n    iteration_count: int\n\nclass EvaluationOutput(BaseModel):\n    status: Literal[\"accepted\", \"rejected\"] = Field(description=\"Whether the joke is funny enough\")\n    critique: str = Field(description=\"Critique of the joke if rejected, or praise if accepted\")\n\n# ---\n\ndef generate_joke(state: State):\n    print(\"---GENERATING JOKE---\")\n    response = llm.invoke(f\"Tell me a joke about {state['topic']}\")\n    return {\n        \"joke\": response.content,\n        \"status\": \"rejected\",\n        \"critique\": \"\",\n        \"iteration_count\": 0\n    }\n\ndef evaluate_joke(state: State):\n    print(\"---EVALUATING JOKE---\")\n    structured_llm = llm.with_structured_output(EvaluationOutput)\n    response = structured_llm.invoke(\n        f\"\"\"Evaluate this joke for humor and cleverness.\n        Joke: {state['joke']}\n        \n        You are a harsh critic. Only accept jokes that are truly hilarious, original, and groundbreaking. \n        Reject anything even slightly clichÃ©, boring, or simple. \n        If rejected, provide a scathing critique.\"\"\"\n    )\n    return {\n        \"status\": response.status,\n        \"critique\": response.critique,\n        \"iteration_count\": state[\"iteration_count\"] + 1\n    }\n\ndef refine_joke(state: State):\n    print(\"---REFINING JOKE---\")\n    response = llm.invoke(\n        f\"\"\"The following joke was rejected.\n        Joke: {state['joke']}\n        Critique: {state['critique']}\n        \n        Please improve the joke based on the critique.\"\"\"\n    )\n    return {\"joke\": response.content}\n\ndef decide_next_node(state: State):\n    if state[\"status\"] == \"accepted\":\n        return END\n    if state[\"iteration_count\"] >= 5:\n        print(\"---MAX ITERATIONS REACHED---\")\n        return END\n    return \"refine_joke\"\n\n# ---\n\ngraph_builder = StateGraph(State)\n\ngraph_builder.add_node(\"generate_joke\", generate_joke)\ngraph_builder.add_node(\"evaluate_joke\", evaluate_joke)\ngraph_builder.add_node(\"refine_joke\", refine_joke)\n\ngraph_builder.add_edge(START, \"generate_joke\")\ngraph_builder.add_edge(\"generate_joke\", \"evaluate_joke\")\ngraph_builder.add_conditional_edges(\n    \"evaluate_joke\",\n    decide_next_node,\n)\ngraph_builder.add_edge(\"refine_joke\", \"evaluate_joke\")\n\ngraph = graph_builder.compile()\n\n# ---\n\nfor chunk in graph.stream({\"topic\": \"about cats\"}):\n    print(chunk)\n    print(\"----\")",
      "main_orchestrator.ipynb": "import os\nfrom dotenv import load_dotenv\nload_dotenv()\n\nfrom langgraph.types import Send\nfrom langgraph.graph import StateGraph, START, END\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom typing import List\nfrom typing_extensions import TypedDict, Literal, Annotated\nfrom pydantic import BaseModel\nfrom operator import add\n\n\nllm = ChatGoogleGenerativeAI(\n    model=\"gemini-2.5-flash\",\n    google_api_key=os.environ[\"GEMINI_API_KEY\"]\n)\n\n# ---\n\nclass State(TypedDict):\n\n    document: str\n    final_summary: str\n    summaries: Annotated[list[dict], add]\n\n# ---\n\ndef summarize_p(args):\n    paragraph = args[\"paragraph\"]\n    index = args[\"index\"]\n    response = llm.invoke(\n        f\"Write a 3-sentence summary for this paragraph: {paragraph}\",\n    )\n    return {\n        \"summaries\": [\n            {\n                \"summary\": response.content,\n                \"index\": index,\n            }\n        ],\n    }\n\n\ndef dispatch_summarizers(state: State):\n    chunks = state[\"document\"].split(\"\\n\\n\")\n    return [\n        Send(\"summarize_p\", {\"paragraph\": chunk, \"index\": index})\n        for index, chunk in enumerate(chunks)\n    ]\n\n\ndef final_summary(state: State):\n    response = llm.invoke(\n        f\"Using the following summaries, give me a final one {state[\"summaries\"]}\"\n    )\n    return {\n        \"final_summary\": response.content,\n    }\n\n# ---\n\ngraph_builder = StateGraph(State)\n\ngraph_builder.add_node(\"summarize_p\", summarize_p)\ngraph_builder.add_node(\"final_summary\", final_summary)\n\n\ngraph_builder.add_conditional_edges(\n    START,\n    dispatch_summarizers,\n    [\"summarize_p\"],\n)\n\ngraph_builder.add_edge(\"summarize_p\", \"final_summary\")\ngraph_builder.add_edge(\"final_summary\", END)\n\n\ngraph = graph_builder.compile()\n\n# ---\n\ngraph\n\n# ---\n\nwith open(\"fed_transcript.md\", \"r\", encoding=\"utf-8\") as file:\n    document = file.read()\n\n\nfor chunk in graph.stream(\n    {\"document\": document},\n    stream_mode=\"updates\",\n):\n    print(chunk, \"\\n\")",
      "main_parallel.ipynb": "import os\nfrom dotenv import load_dotenv\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom typing_extensions import TypedDict, Literal\nfrom typing import List\nfrom langgraph.types import Command\nfrom langgraph.graph import StateGraph, START, END\nfrom pydantic import BaseModel\n\nload_dotenv()\n\nllm = ChatGoogleGenerativeAI(\n    model=\"gemini-2.5-flash\",\n    google_api_key=os.environ[\"GEMINI_API_KEY\"]\n)\n\n# ---\n\nclass State(TypedDict):\n\n    document: str\n    summary: str\n    sentiment: str\n    key_points: str\n    recommendation: str\n    final_analysis: str\n\n# ---\n\ndef get_summary(state: State):\n    response = llm.invoke(\n        f\"Write a 3-sentence summary of this document {state[\"document\"]}\"\n    )\n    return {\n        \"summary\": response.content,\n    }\n\n\ndef get_sentiment(state: State):\n    response = llm.invoke(\n        f\"Analyse the sentiment and tone of this document {state[\"document\"]}\"\n    )\n    return {\n        \"sentiment\": response.content,\n    }\n\n\ndef get_key_points(state: State):\n    response = llm.invoke(\n        f\"List the 5 most important points of this document {state[\"document\"]}\"\n    )\n    return {\n        \"key_points\": response.content,\n    }\n\n\ndef get_recommendation(state: State):\n    response = llm.invoke(\n        f\"Based on the document, list 3 recommended next steps {state[\"document\"]}\"\n    )\n    return {\n        \"recommendation\": response.content,\n    }\n\n\ndef get_final_analysis(state: State):\n    response = llm.invoke(\n        f\"\"\"\n    Give me an analysis of the following report\n\n    DOCUMENT ANALYSIS REPORT\n    ========================\n\n    EXECUTIVE SUMMARY:\n    {state['summary']}\n\n    SENTIMENT ANALYSIS:\n    {state['sentiment']}\n\n    KEY POINTS:\n    {state.get(\"key_points\", \"\")}\n\n    RECOMMENDATIONS:\n    {state.get('recommendation', \"N/A\")}\n    \"\"\"\n    )\n    return {\n        \"final_analysis\": response.content,\n    }\n\n# ---\n\ngraph_builder = StateGraph(State)\n\ngraph_builder.add_node(\"get_summary\", get_summary)\ngraph_builder.add_node(\"get_sentiment\", get_sentiment)\ngraph_builder.add_node(\"get_key_points\", get_key_points)\ngraph_builder.add_node(\"get_recommendation\", get_recommendation)\ngraph_builder.add_node(\"get_final_analysis\", get_final_analysis)\n\n\ngraph_builder.add_edge(START, \"get_summary\")\ngraph_builder.add_edge(START, \"get_sentiment\")\ngraph_builder.add_edge(START, \"get_key_points\")\ngraph_builder.add_edge(START, \"get_recommendation\")\n\ngraph_builder.add_edge(\"get_summary\", \"get_final_analysis\")\ngraph_builder.add_edge(\"get_sentiment\", \"get_final_analysis\")\ngraph_builder.add_edge(\"get_key_points\", \"get_final_analysis\")\ngraph_builder.add_edge(\"get_recommendation\", \"get_final_analysis\")\n\ngraph_builder.add_edge(\"get_final_analysis\", END)\n\n\ngraph = graph_builder.compile()\n\n# ---\n\ngraph\n\n# ---\n\nwith open(\"fed_transcript.md\", \"r\", encoding=\"utf-8\") as file:\n    document = file.read()\n\nfor chunk in graph.stream(\n    {\"document\": document},\n    stream_mode=\"updates\",\n):\n    print(chunk, \"\\n\")",
      "main_prompt_chainning.ipynb": "import os\nfrom dotenv import load_dotenv\nfrom langchain_google_genai import ChatGoogleGenerativeAI\n\nload_dotenv()\n\nfrom typing_extensions import TypedDict\nfrom typing import List\nfrom langgraph.graph import StateGraph, START, END\nfrom pydantic import BaseModel\n\nllm = ChatGoogleGenerativeAI(\n    model=\"gemini-2.5-flash\",\n    google_api_key=os.environ[\"GEMINI_API_KEY\"]\n)\n\n# ---\n\nclass State(TypedDict):\n\n    dish: str\n    ingredients: list[dict]\n    recipe_steps: str\n    plating_instructions: str\n\n\nclass Ingredient(BaseModel):\n\n    name: str\n    quantity: str\n    unit: str\n\n\nclass IngredientsOutput(BaseModel):\n\n    ingredients: List[Ingredient]\n\n# ---\n\ndef list_ingredients(state: State):\n    structured_llm = llm.with_structured_output(IngredientsOutput)\n    response = structured_llm.invoke(\n        f\"List 5-8 ingredients needed to make {state['dish']}\"\n    )\n    return {\n        \"ingredients\": response.ingredients,\n    }\n\n\ndef create_recipe(state: State):\n    response = llm.invoke(\n        f\"Write a step by step cooking instruction for {state['dish']}, using these ingredients {state['ingredients']}\",\n    )\n    return {\n        \"recipe_steps\": response.content,\n    }\n\n\ndef describe_plating(state: State):\n    response = llm.invoke(\n        f\"Describe how to beautifully plate this dish {state['dish']} based on this recipe {state['recipe_steps']}\"\n    )\n    return {\n        \"plating_instructions\": response.content,\n    }\n\ndef gate(state: State):\n    ingredients = state[\"ingredients\"]\n    if len(ingredients) > 8 or len(ingredients) < 3:\n        return False\n    return True\n\n# ---\n\ngraph_builder = StateGraph(State)\n\ngraph_builder.add_node(\"list_ingredients\", list_ingredients)\ngraph_builder.add_node(\"create_recipe\", create_recipe)\ngraph_builder.add_node(\"describe_plating\", describe_plating)\n\ngraph_builder.add_edge(START, \"list_ingredients\")\n\ngraph_builder.add_conditional_edges(\n    \"list_ingredients\",\n    gate,\n    {\n        True: \"create_recipe\",\n        False: \"list_ingredients\",\n    },\n)\n\ngraph_builder.add_edge(\"create_recipe\", \"describe_plating\")\ngraph_builder.add_edge(\"describe_plating\", END)\n\ngraph = graph_builder.compile()\n\n# ---\n\ngraph\n\n# ---\n\ngraph.invoke({\"dish\": \"hummus\"})",
      "main_routing.ipynb": "import os\nfrom dotenv import load_dotenv\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom typing_extensions import TypedDict, Literal\nfrom typing import List\nfrom langgraph.types import Command\nfrom langgraph.graph import StateGraph, START, END\nfrom pydantic import BaseModel\n\nload_dotenv()\n\n# Using Gemini for all models as requested\nllm = ChatGoogleGenerativeAI(\n    model=\"gemini-2.5-flash\",\n    google_api_key=os.environ[\"GEMINI_API_KEY\"]\n)\n\ndumb_llm = llm\naverage_llm = llm\nsmart_llm = llm\n\n# ---\n\nclass State(TypedDict):\n\n    question: str\n    difficulty: str\n    answer: str\n    model_used: str\n\n\nclass DifficultyResponse(BaseModel):\n\n    difficulty_level: Literal[\"easy\", \"medium\", \"hard\"]\n\n# ---\n\ndef dumb_node(state: State):\n    response = dumb_llm.invoke(state[\"question\"])\n    return {\n        \"answer\": response.content,\n        \"model_used\": \"gpt-3.5\",\n    }\n\n\ndef average_node(state: State):\n    response = average_llm.invoke(state[\"question\"])\n    return {\n        \"answer\": response.content,\n        \"model_used\": \"gpt-4o\",\n    }\n\n\ndef smart_node(state: State):\n    response = smart_llm.invoke(state[\"question\"])\n    return {\n        \"answer\": response.content,\n        \"model_used\": \"gpt-o3\",\n    }\n\n\ndef assess_difficulty(state: State):\n    structured_llm = llm.with_structured_output(DifficultyResponse)\n    \n    response = structured_llm.invoke(\n        f\"\"\"\n        Assess the difficulty of this question\n        Question: {state[\"question\"]}\n\n        - EASY: Simple facts, basic definitions, yes/no answers\n        - MEDIUM: Requires explanation, comparison, analysis\n        - HARD: Complex reasoning, multiple steps, deep expertise.\n        \"\"\"\n    )\n    \n    difficulty_level = response.difficulty_level\n    \n    if difficulty_level == \"easy\":\n        goto = \"dumb_node\"\n    elif difficulty_level == \"medium\":\n        goto = \"average_node\"\n    elif difficulty_level == \"hard\":\n        goto = \"smart_node\"\n        \n    return Command(\n        goto=goto,\n        update={\n            \"difficulty\": difficulty_level,\n        },\n    )\n\n# ---\n\ngraph_builder = StateGraph(State)\n\ngraph_builder.add_node(\"dumb_node\", dumb_node)\ngraph_builder.add_node(\"average_node\", average_node)\ngraph_builder.add_node(\"smart_node\", smart_node)\ngraph_builder.add_node(\n    \"assess_difficulty\",\n    assess_difficulty,\n    destinations=[\"dumb_node\", \"average_node\", \"smart_node\"],\n)\n\ngraph_builder.add_edge(START, \"assess_difficulty\")\ngraph_builder.add_edge(\"dumb_node\", END)\ngraph_builder.add_edge(\"average_node\", END)\ngraph_builder.add_edge(\"smart_node\", END)\n\ngraph = graph_builder.compile()\n\n# ---\n\ngraph\n\n# ---\n\ngraph.invoke({\"question\": \"Investment potential of Uranium in 2026\"})"
    },
    "assets": [],
    "techStack": [
      "LangGraph",
      "Gemini",
      "Python"
    ]
  },
  {
    "id": "16",
    "title": "Testing Agent",
    "description": "ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ë° í’ˆì§ˆ ê²€ì¦ ì—ì´ì „íŠ¸",
    "directory": "16_testing-agent",
    "readmePath": "16_testing-agent/README.md",
    "mainCodePath": "16_testing-agent/main.py",
    "readme": "# AI Agent Workflow Testing\n\nLangGraphì™€ Geminië¥¼ í™œìš©í•œ ì´ë©”ì¼ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° ë° í…ŒìŠ¤íŠ¸ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.\n\n## í”„ë¡œì íŠ¸ ì†Œê°œ\n\nì´ í”„ë¡œì íŠ¸ëŠ” ìˆ˜ì‹ ëœ ì´ë©”ì¼ì„ ë¶„ì„í•˜ì—¬ ìë™ìœ¼ë¡œ ë¶„ë¥˜í•˜ê³ , ìš°ì„ ìˆœìœ„ë¥¼ ë§¤ê¸°ë©°, ì ì ˆí•œ ì‘ë‹µ ì´ˆì•ˆì„ ì‘ì„±í•˜ëŠ” AI ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ë˜í•œ, **LLM-as-a-Judge** íŒ¨í„´ì„ ë„ì…í•˜ì—¬ AIê°€ ìƒì„±í•œ ì‘ë‹µì„ ë˜ ë‹¤ë¥¸ AIê°€ í‰ê°€í•˜ëŠ” ìë™í™”ëœ í…ŒìŠ¤íŠ¸ í™˜ê²½ì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.\n\n### ì£¼ìš” ê¸°ëŠ¥\n\n1.  **ì´ë©”ì¼ ë¶„ë¥˜ (Categorize Email)**: ì´ë©”ì¼ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ `urgent`, `normal`, `spam` ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.\n2.  **ìš°ì„ ìˆœìœ„ í• ë‹¹ (Assign Priority)**: ë¶„ë¥˜ëœ ì¹´í…Œê³ ë¦¬ì™€ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ 1~10ì  ì‚¬ì´ì˜ ìš°ì„ ìˆœìœ„ ì ìˆ˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.\n3.  **ì‘ë‹µ ì´ˆì•ˆ ì‘ì„± (Draft Response)**: ì¹´í…Œê³ ë¦¬ì— ë§ì¶° ì ì ˆí•œ í†¤ì•¤ë§¤ë„ˆë¡œ ë‹µì¥ ì´ˆì•ˆì„ ìƒì„±í•©ë‹ˆë‹¤.\n4.  **ìë™í™” í…ŒìŠ¤íŠ¸ (Automated Testing)**: `pytest`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ì›Œí¬í”Œë¡œìš° ë° ê°œë³„ ë…¸ë“œë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n    *   **LLM-as-a-Judge**: ìƒì„±ëœ ì‘ë‹µì´ ëª¨ë²” ë‹µì•ˆê³¼ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ AIê°€ ì±„ì í•˜ì—¬ í…ŒìŠ¤íŠ¸ í†µê³¼ ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.\n\n## ì„¤ì¹˜ ë° ì„¤ì •\n\n### 1. ì˜ì¡´ì„± ì„¤ì¹˜\n\n`uv`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ì¡´ì„±ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.\n\n```bash\nuv sync\n```\n\n### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n\n`.env` íŒŒì¼ì„ ìƒì„±í•˜ê³  Google Gemini API í‚¤ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n\n```bash\nGOOGLE_API_KEY=your_api_key_here\n```\n\n## ì‚¬ìš© ë°©ë²•\n\n### í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n\nì´ í”„ë¡œì íŠ¸ì˜ í•µì‹¬ì€ í…ŒìŠ¤íŠ¸ ì½”ë“œì— ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\n\n```bash\nuv run pytest tests.py\n```\n\n### í…ŒìŠ¤íŠ¸ êµ¬ì„± (`tests.py`)\n\n*   `test_full_graph`: ì „ì²´ ì›Œí¬í”Œë¡œìš°ê°€ ì˜ˆìƒëŒ€ë¡œ ë™ì‘í•˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤. (ì ìˆ˜ ë²”ìœ„ ê²€ì¦ í¬í•¨)\n*   `test_individual_nodes`: ê° ë…¸ë“œ(í•¨ìˆ˜)ê°€ ë…ë¦½ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ì¶œë ¥ì„ ë‚´ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.\n    *   `draft_response` í…ŒìŠ¤íŠ¸ ì‹œ `judge_response` í•¨ìˆ˜ë¥¼ í†µí•´ AI ì‘ë‹µ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.\n*   `test_partial_execution`: ê·¸ë˜í”„ì˜ ì¤‘ê°„ ìƒíƒœë¥¼ ìˆ˜ì •í•˜ê³  ë¶€ë¶„ì ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” ê¸°ëŠ¥ì„ ê²€ì¦í•©ë‹ˆë‹¤.\n\n## ê¸°ìˆ  ìŠ¤íƒ\n\n*   **Language**: Python 3.13+\n*   **Framework**: LangGraph, LangChain\n*   **AI Model**: Google Gemini (gemini-2.5-flash)\n*   **Testing**: Pytest\n*   **Package Manager**: uv\n",
    "codes": {
      "main.py": "from typing import Literal\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom pydantic import BaseModel, Field\nfrom langchain.chat_models import init_chat_model\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\nllm = init_chat_model(\"google_genai:gemini-2.5-flash\")\n\ncheckpointer = MemorySaver()\n\nclass EmailState(TypedDict):\n    email: str\n    category: Literal[\"spam\", \"normal\", \"urgent\"]\n    priority_score: int\n    response: str\n\n\nclass EmailClassificationOuput(BaseModel):\n    category: Literal[\"spam\", \"normal\", \"urgent\"] = Field(\n        description=\"Category of the email\",\n    )\n\n\nclass PriorityScoreOutput(BaseModel):\n    priority_score: int = Field(\n        description=\"Priority score from 1 to 10\",\n        ge=1,\n        le=10,\n    )\n\n\ndef categorize_email(state: EmailState):\n    s_llm = llm.with_structured_output(EmailClassificationOuput)\n    \n    result = s_llm.invoke(\n        f\"\"\"Classify this email into one of three categories:\n        - urgent: time-sensitive, requires immediate attention\n        - normal: regular business communication\n        - spam: promotional, marketing, or unwanted content\n\n        Email: {state['email']}\"\"\"\n    )\n    \n    return {\n        \"category\": result.category,\n    }\n\n\ndef assign_priority(state: EmailState):\n    s_llm = llm.with_structured_output(PriorityScoreOutput)\n    \n    result = s_llm.invoke(\n        f\"\"\"Assign a priority score from 1-10 for this {state['category']} email.\n        Consider:\n        - Category: {state['category']}\n        - Email content: {state['email']}\n\n        Guidelines:\n        - Urgent emails: usually 8-10\n        - Normal emails: usually 4-7\n        - Spam emails: usually 1-3\"\"\"\n    )\n    \n    return {\n        \"priority_score\": result.priority_score,\n    }\n\n\ndef draft_response(state: EmailState) -> EmailState:\n    result = llm.invoke(\n        f\"\"\"Draft a response for this {state['category']} email.\n        Email content: {state['email']}\n        \n        Guidelines:\n        - Urgent: Be professional and promise quick action.\n        - Normal: Be polite and say you will get back soon.\n        - Spam: State that the message has been flagged as spam and filtered.\n        \"\"\"\n    )\n    return {\n        \"response\": result.content,\n    }\n\n\ngraph_builder = StateGraph(EmailState)\n\ngraph_builder.add_node(\"categorize_email\", categorize_email)\ngraph_builder.add_node(\"assign_priority\", assign_priority)\ngraph_builder.add_node(\"draft_response\", draft_response)\n\ngraph_builder.add_edge(START, \"categorize_email\")\ngraph_builder.add_edge(\"categorize_email\", \"assign_priority\")\ngraph_builder.add_edge(\"assign_priority\", \"draft_response\")\ngraph_builder.add_edge(\"draft_response\", END)\n\ngraph = graph_builder.compile(checkpointer=checkpointer)\n\n\ngraph_builder = StateGraph(EmailState)\n\ngraph_builder.add_node(\"categorize_email\", categorize_email)\ngraph_builder.add_node(\"assign_priority\", assign_priority)\ngraph_builder.add_node(\"draft_response\", draft_response)\n\ngraph_builder.add_edge(START, \"categorize_email\")\ngraph_builder.add_edge(\"categorize_email\", \"assign_priority\")\ngraph_builder.add_edge(\"assign_priority\", \"draft_response\")\ngraph_builder.add_edge(\"draft_response\", END)\n\ngraph = graph_builder.compile(checkpointer=checkpointer)\n\n\n",
      "tests.py": "import dotenv\n\ndotenv.load_dotenv()\n\nimport pytest\nfrom main import graph\nfrom pydantic import BaseModel, Field\nfrom langchain.chat_models import init_chat_model\n\nllm = init_chat_model(\"google_genai:gemini-2.5-flash\")\n\n\nclass SimilarityScoreOutput(BaseModel):\n    similarity_score: int = Field(\n        description=\"How similar is the response to the examples?\",\n        gt=0,\n        lt=100,\n    )\n\n\nRESPONSE_EXAMPLES = {\n    \"urgent\": [\n        \"Thank you for your urgent message. We are addressing this immediately and will respond as soon as possible.\",\n        \"We've received your urgent request and are prioritizing it. Our team is on it right away.\",\n        \"This urgent matter has our immediate attention. We'll respond promptly.\",\n    ],\n    \"normal\": [\n        \"Thank you for your email. We'll review it and get back to you within 24-48 hours.\",\n        \"We've received your message and will respond soon. Thank you for reaching out.\",\n        \"Thank you for contacting us. We'll process your request and respond shortly.\",\n        \"Thank you for the update. I will review the information and follow up as needed.\",\n        \"Thank you for the update on the project status. I will review and follow up by the end of the week.\",\n        \"Thanks for sharing this update. We'll review and respond accordingly.\",\n    ],\n    \"spam\": [\n        \"This message has been flagged as spam and filtered.\",\n        \"This email has been identified as promotional content.\",\n        \"This message has been marked as spam.\",\n    ],\n}\n\n\ndef judge_response(response: str, category: str):\n    s_llm = llm.with_structured_output(SimilarityScoreOutput)\n\n    examples = RESPONSE_EXAMPLES[category]\n    result = s_llm.invoke(\n        f\"\"\"\n        Score how similar this response is to the examples.\n\n        Category: {category}\n\n        Examples:\n        {\"\\n\".join(examples)}\n\n        Response to evaluate:\n        {response}\n\n        Scoring criteria:\n        - 90-100: Very similar in tone, content, and intent\n        - 70-89: Similar with minor differences\n        - 50-69: Moderately similar, captures main idea\n        - 30-49: Some similarity but missing key elements\n        - 0-29: Very different or inappropriate\n\n        \"\"\"\n    )\n\n    return result.similarity_score\n\n\n@pytest.mark.parametrize(\n    \"email, expected_category, min_score, max_score\",\n    [\n        (\"this is urgent!\", \"urgent\", 8, 10),\n        (\"i wanna talk to you\", \"normal\", 4, 7),\n        (\"i have an offer for you\", \"spam\", 1, 3),\n    ],\n)\ndef test_full_graph(email, expected_category, min_score, max_score):\n\n    result = graph.invoke(\n        {\"email\": email}, config={\"configurable\": {\"thread_id\": \"1\"}}\n    )\n\n    assert result[\"category\"] == expected_category\n    assert min_score <= result[\"priority_score\"] <= max_score\n\n\ndef test_individual_nodes():\n\n    # categorize_email\n    result = graph.nodes[\"categorize_email\"].invoke({\"email\": \"check out this offer\"})\n\n    assert result[\"category\"] == \"spam\"\n\n    # assing_priority\n    result = graph.nodes[\"assign_priority\"].invoke(\n        {\"category\": \"spam\", \"email\": \"buy this pot.\"}\n    )\n\n    assert 1 <= result[\"priority_score\"] <= 3\n\n    # draft_response\n    result = graph.nodes[\"draft_response\"].invoke(\n        {\n            \"category\": \"spam\",\n            \"email\": \"Get rich quick!!! I have a pyramid scheme for you!\",\n            \"priority_score\": 1,\n        }\n    )\n\n    similarity_score = judge_response(result[\"response\"], \"spam\")\n    assert similarity_score >= 70\n\n\ndef test_partial_execution():\n\n    graph.update_state(\n        config={\n            \"configurable\": {\n                \"thread_id\": \"1\",\n            },\n        },\n        values={\n            \"email\": \"please check out this offer\",\n            \"category\": \"spam\",\n        },\n        as_node=\"categorize_email\",\n    )\n\n    result = graph.invoke(\n        None,\n        config={\n            \"configurable\": {\n                \"thread_id\": \"1\",\n            },\n        },\n        interrupt_after=\"draft_response\",\n    )\n\n    assert 1 <= result[\"priority_score\"] <= 3\n"
    },
    "assets": [],
    "techStack": [
      "LangGraph",
      "Gemini",
      "Python"
    ]
  },
  {
    "id": "17",
    "title": "Multi Agent Architectures",
    "description": "ë©€í‹° ì—ì´ì „íŠ¸ í˜‘ì—… êµ¬ì¡° êµ¬í˜„ ì˜ˆì œ",
    "directory": "17_multi-agent-architectures",
    "readmePath": "17_multi-agent-architectures/README.md",
    "mainCodePath": "17_multi-agent-architectures/main.py",
    "readme": "# Multi-Agent Architectures with LangGraph\n\nLangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ë©€í‹° ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ íŒ¨í„´ì„ êµ¬í˜„í•œ ì˜ˆì œ ëª¨ìŒì…ë‹ˆë‹¤.\n\n## ğŸ“‹ ëª©ì°¨\n\n- [ê°œìš”](#ê°œìš”)\n- [ì•„í‚¤í…ì²˜ ì¢…ë¥˜](#ì•„í‚¤í…ì²˜-ì¢…ë¥˜)\n- [ì„¤ì¹˜ ë°©ë²•](#ì„¤ì¹˜-ë°©ë²•)\n- [ì‚¬ìš© ë°©ë²•](#ì‚¬ìš©-ë°©ë²•)\n- [í”„ë¡œì íŠ¸ êµ¬ì¡°](#í”„ë¡œì íŠ¸-êµ¬ì¡°)\n- [ìš”êµ¬ ì‚¬í•­](#ìš”êµ¬-ì‚¬í•­)\n\n## ê°œìš”\n\nì´ í”„ë¡œì íŠ¸ëŠ” LangGraphë¥¼ í™œìš©í•˜ì—¬ 4ê°€ì§€ ì£¼ìš” ë©€í‹° ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜ íŒ¨í„´ì„ êµ¬í˜„í•©ë‹ˆë‹¤:\n\n1. **Handoff Architecture** - ì—ì´ì „íŠ¸ ê°„ ì§ì ‘ ì „ë‹¬\n2. **Supervisor Architecture** - ì¤‘ì•™ ê°ë…ìê°€ ë¼ìš°íŒ…\n3. **Supervisor As Tools** - ë„êµ¬ ê¸°ë°˜ ë¼ìš°íŒ…\n4. **Prebuilt Agents** - ì‚¬ì „ êµ¬ì¶•ëœ í•¨ìˆ˜ í™œìš©\n\nê° ì•„í‚¤í…ì²˜ëŠ” ì„œë¡œ ë‹¤ë¥¸ ì‚¬ìš© ì‚¬ë¡€ì™€ ë³µì¡ë„ì— ì í•©í•˜ë©°, ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œì˜ ë‹¤ì–‘í•œ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n## ì•„í‚¤í…ì²˜ ì¢…ë¥˜\n\n### 1. Handoff Architecture (Network)\n\nì—ì´ì „íŠ¸ë“¤ì´ ì§ì ‘ `handoff_tool`ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë¡œ ì œì–´ê¶Œì„ ì „ë‹¬í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\n\n**íŠ¹ì§•:**\n- ë¶„ì‚°í˜• ë¼ìš°íŒ…: ê° ì—ì´ì „íŠ¸ê°€ ì§ì ‘ ë‹¤ìŒ ì—ì´ì „íŠ¸ë¥¼ ê²°ì •\n- ìœ ì—°í•œ ì œì–´ íë¦„: ì—ì´ì „íŠ¸ ê°„ ì§ì ‘ í†µì‹ \n- ê°„ë‹¨í•œ êµ¬ì¡°: ì¤‘ì•™ ê°ë…ì ì—†ì´ ì‘ë™\n\n**íŒŒì¼:**\n- `main_network.ipynb` / `graph_netwrok.py`\n\n**ì‚¬ìš© ì˜ˆì‹œ:**\n```python\n# ê° ì—ì´ì „íŠ¸ê°€ handoff_toolì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë¡œ ì „ë‹¬\nkorean_agent â†’ (handoff) â†’ spanish_agent â†’ (handoff) â†’ greek_agent\n```\n\n### 2. Supervisor Architecture\n\nì¤‘ì•™ ê°ë…ì(Supervisor)ê°€ êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ í†µí•´ ë‹¤ìŒ ì‘ì—…ì„ ì²˜ë¦¬í•  ì—ì´ì „íŠ¸ë¥¼ ê²°ì •í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\n\n**íŠ¹ì§•:**\n- ì¤‘ì•™í™”ëœ ë¼ìš°íŒ…: ëª¨ë“  ë¼ìš°íŒ… ê²°ì •ì„ Supervisorê°€ ê´€ë¦¬\n- êµ¬ì¡°í™”ëœ ì¶œë ¥: Pydantic BaseModelì„ ì‚¬ìš©í•œ ëª…í™•í•œ ë¼ìš°íŒ… ê²°ì •\n- ì¶”ì  ê°€ëŠ¥ì„±: reasoning í•„ë“œë¡œ ì˜ì‚¬ê²°ì • ê³¼ì • ì¶”ì \n\n**íŒŒì¼:**\n- `main_supervisor.ipynb` / `graph_supervisor.py`\n\n**ì‚¬ìš© ì˜ˆì‹œ:**\n```python\nSupervisor â†’ (structured output) â†’ next_agent ê²°ì •\n    â†“\nKorean Agent / Spanish Agent / Greek Agent\n    â†“\nSupervisor (ë‹¤ì‹œ ë¼ìš°íŒ… ë˜ëŠ” ì¢…ë£Œ)\n```\n\n### 3. Supervisor As Tools\n\nê° ì—ì´ì „íŠ¸ë¥¼ ë„êµ¬(Tool)ë¡œ ê°ì‹¸ì„œ, Supervisorê°€ ë„êµ¬ í˜¸ì¶œì„ í†µí•´ ë¼ìš°íŒ…í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\n\n**íŠ¹ì§•:**\n- ë„êµ¬ ê¸°ë°˜ ë¼ìš°íŒ…: í‘œì¤€ ë„êµ¬ í˜¸ì¶œ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©\n- ì¼ê´€ëœ íŒ¨í„´: ëª¨ë“  ì—ì´ì „íŠ¸ê°€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ ì œê³µ\n- ìì—°ìŠ¤ëŸ¬ìš´ í†µí•©: ê¸°ì¡´ ë„êµ¬ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì‘ë™\n\n**íŒŒì¼:**\n- `main_supervisor_as_tools.ipynb` / `graph_supervisor_as_tools.py`\n\n**ì‚¬ìš© ì˜ˆì‹œ:**\n```python\nSupervisor â†’ (tool call) â†’ korean_agent_tool()\nSupervisor â†’ (tool call) â†’ spanish_agent_tool()\n```\n\n### 4. Prebuilt Agents\n\nLangGraphì˜ `create_react_agent`ì™€ `langgraph-supervisor`ì˜ `create_supervisor`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹ ë¥´ê²Œ êµ¬ì¶•í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\n\n**íŠ¹ì§•:**\n- ìµœì†Œí•œì˜ ì½”ë“œ: ì‚¬ì „ êµ¬ì¶•ëœ í•¨ìˆ˜ í™œìš©\n- ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘: ë³µì¡í•œ ì„¤ì • ì—†ì´ ì¦‰ì‹œ ì‹œì‘ ê°€ëŠ¥\n- ìœ ì§€ë³´ìˆ˜ ìš©ì´: í‘œì¤€ íŒ¨í„´ ì‚¬ìš©\n\n**íŒŒì¼:**\n- `main_prebuilt.ipynb`\n\n**ì‚¬ìš© ì˜ˆì‹œ:**\n```python\n# ì—ì´ì „íŠ¸ ìƒì„±\nhistory_agent = create_react_agent(\n    model=MODEL,\n    tools=[],\n    name=\"history_agent\",\n    prompt=\"You are a history expert...\"\n)\n\n# Supervisor ìƒì„±\nsupervisor = create_supervisor(\n    agents=[history_agent, ...],\n    model=MODEL,\n    prompt=\"You are a supervisor...\"\n).compile()\n```\n\n## ì„¤ì¹˜ ë°©ë²•\n\n### 1. í”„ë¡œì íŠ¸ í´ë¡ \n\n```bash\ngit clone <repository-url>\ncd 17_multi-agent-architectures\n```\n\n### 2. Python ë²„ì „ í™•ì¸\n\nPython 3.13 ì´ìƒì´ í•„ìš”í•©ë‹ˆë‹¤.\n\n```bash\npython --version\n```\n\n### 3. ì˜ì¡´ì„± ì„¤ì¹˜\n\nì´ í”„ë¡œì íŠ¸ëŠ” `uv`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ì¡´ì„±ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.\n\n```bash\n# uv ì„¤ì¹˜ (í•„ìš”í•œ ê²½ìš°)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# ì˜ì¡´ì„± ì„¤ì¹˜\nuv sync\n```\n\në˜ëŠ” `pip`ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n```bash\npip install -r requirements.txt\n```\n\n### 4. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n\n`.env` íŒŒì¼ì„ ìƒì„±í•˜ê³  í•„ìš”í•œ API í‚¤ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤:\n\n```bash\n# OpenAI ì‚¬ìš© ì‹œ\nOPENAI_API_KEY=your_openai_api_key_here\n\n# Google Gemini ì‚¬ìš© ì‹œ (ì¼ë¶€ ì•„í‚¤í…ì²˜)\nGOOGLE_API_KEY=your_google_api_key_here\n```\n\n## ì‚¬ìš© ë°©ë²•\n\n### Jupyter Notebook ì‚¬ìš©\n\nê° ì•„í‚¤í…ì²˜ëŠ” Jupyter Notebookìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤:\n\n```bash\n# Jupyter ì‹œì‘\njupyter notebook\n\n# ë˜ëŠ” JupyterLab ì‚¬ìš©\njupyter lab\n```\n\nê° ë…¸íŠ¸ë¶ íŒŒì¼ì„ ì—´ê³  ì…€ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤.\n\n### Python íŒŒì¼ ì‚¬ìš© (LangGraph Studio)\n\nì¼ë¶€ ì•„í‚¤í…ì²˜ëŠ” LangGraph Studioì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” Python íŒŒì¼ë„ ì œê³µë©ë‹ˆë‹¤:\n\n```bash\n# LangGraph Studio ì‹œì‘\nlanggraph dev\n\n# ë˜ëŠ” íŠ¹ì • ê·¸ë˜í”„ ì‹¤í–‰\nlanggraph dev graph_supervisor.py:graph\n```\n\n### ì˜ˆì œ ì‹¤í–‰\n\n```python\n# ì˜ˆ: Prebuilt Agents ì•„í‚¤í…ì²˜\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# ë…¸íŠ¸ë¶ì˜ ì…€ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰\n# 1. Import ë° ì„¤ì •\n# 2. ì—ì´ì „íŠ¸ ìƒì„±\n# 3. Supervisor ìƒì„±\n# 4. ì§ˆë¬¸ ì‹¤í–‰\n```\n\n## í”„ë¡œì íŠ¸ êµ¬ì¡°\n\n```\n17_multi-agent-architectures/\nâ”œâ”€â”€ README.md                           # ì´ íŒŒì¼\nâ”œâ”€â”€ pyproject.toml                      # í”„ë¡œì íŠ¸ ì„¤ì • ë° ì˜ì¡´ì„±\nâ”œâ”€â”€ uv.lock                            # ì˜ì¡´ì„± ì ê¸ˆ íŒŒì¼\nâ”œâ”€â”€ langgraph.json                     # LangGraph Studio ì„¤ì •\nâ”‚\nâ”œâ”€â”€ main_prebuilt.ipynb                # Prebuilt Agents ì•„í‚¤í…ì²˜\nâ”‚\nâ”œâ”€â”€ main_supervisor.ipynb              # Supervisor ì•„í‚¤í…ì²˜\nâ”œâ”€â”€ graph_supervisor.py                # Supervisor ê·¸ë˜í”„ (Studioìš©)\nâ”‚\nâ”œâ”€â”€ main_supervisor_as_tools.ipynb     # Supervisor As Tools ì•„í‚¤í…ì²˜\nâ”œâ”€â”€ graph_supervisor_as_tools.py       # Supervisor As Tools ê·¸ë˜í”„ (Studioìš©)\nâ”‚\nâ”œâ”€â”€ main_network.ipynb                 # Handoff/Network ì•„í‚¤í…ì²˜\nâ”œâ”€â”€ graph_netwrok.py                   # Network ê·¸ë˜í”„ (Studioìš©)\nâ”‚\nâ””â”€â”€ main.py                            # ê¸°ë³¸ ì§„ì…ì \n```\n\n## ìš”êµ¬ ì‚¬í•­\n\n### í•„ìˆ˜ ì˜ì¡´ì„±\n\n- **langchain** (>=0.3.27): LLM í†µí•© ë° ì²´ì¸ êµ¬ì„±\n- **langgraph** (>=0.6.6): ê·¸ë˜í”„ ê¸°ë°˜ ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜\n- **langgraph-supervisor** (>=0.0.29): ì‚¬ì „ êµ¬ì¶•ëœ Supervisor ê¸°ëŠ¥\n- **python-dotenv** (>=1.1.1): í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬\n\n### ì„ íƒì  ì˜ì¡´ì„±\n\n- **langchain-google-genai** (>=2.0.0): Google Gemini ëª¨ë¸ ì§€ì›\n- **langgraph-checkpoint-sqlite** (>=2.0.11): ìƒíƒœ ì €ì¥ì†Œ\n- **langgraph-cli** (>=0.4.0): CLI ë„êµ¬\n\n### ëª¨ë¸ ì§€ì›\n\n- **OpenAI**: GPT-4, GPT-4o, GPT-5 (ê¶Œì¥)\n- **Google Gemini**: gemini-2.5-flash (ì¼ë¶€ ì•„í‚¤í…ì²˜)\n\n## ì£¼ì˜ì‚¬í•­\n\n### ëª¨ë¸ í˜¸í™˜ì„±\n\n- **Prebuilt Agents ì•„í‚¤í…ì²˜**: `langgraph-supervisor`ëŠ” í˜„ì¬ OpenAI ëª¨ë¸ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. Google Geminië¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ë‹¤ë¥¸ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n\n- **Supervisor Architecture**: Google Geminiì™€ ì™„ì „íˆ í˜¸í™˜ë©ë‹ˆë‹¤.\n\n### í™˜ê²½ ë³€ìˆ˜\n\nê° ì•„í‚¤í…ì²˜ì—ì„œ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì— ë”°ë¼ ì ì ˆí•œ API í‚¤ë¥¼ `.env` íŒŒì¼ì— ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n\n## ë¼ì´ì„ ìŠ¤\n\nì´ í”„ë¡œì íŠ¸ëŠ” êµìœ¡ ëª©ì ìœ¼ë¡œ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤.\n\n## ì°¸ê³  ìë£Œ\n\n- [LangGraph ê³µì‹ ë¬¸ì„œ](https://langchain-ai.github.io/langgraph/)\n- [LangGraph Supervisor ë¬¸ì„œ](https://github.com/langchain-ai/langgraph-supervisor)\n- [LangChain ë¬¸ì„œ](https://python.langchain.com/)\n\n",
    "codes": {
      "graph_netwrok.py": "from dotenv import load_dotenv\nload_dotenv()\n\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import Command\nfrom langgraph.graph.message import MessagesState\nfrom langgraph.prebuilt import ToolNode, tools_condition\nfrom langchain_core.tools import tool\nfrom langchain.chat_models import init_chat_model\n\n\nclass AgentsState(MessagesState):\n    current_agent: str\n    transfered_by: str\n\n\nllm = init_chat_model(\"google_genai:gemini-2.5-flash\")\n\n\ndef make_agent(prompt, tools):\n\n    def agent_node(state: AgentsState):\n        llm_with_tools = llm.bind_tools(tools)\n        response = llm_with_tools.invoke(\n            f\"\"\"\n        {prompt}\n\n        You have a tool called 'handoff_tool' use it to transfer to other agent, don't use it to transfer to yourself.\n\n        Conversation History:\n        {state[\"messages\"]}\n        \"\"\"\n        )\n        return {\"messages\": [response]}\n\n    agent_builder = StateGraph(AgentsState)\n\n    agent_builder.add_node(\"agent\", agent_node)\n    agent_builder.add_node(\n        \"tools\",\n        ToolNode(tools=tools),\n    )\n\n    agent_builder.add_edge(START, \"agent\")\n    agent_builder.add_conditional_edges(\"agent\", tools_condition)\n    agent_builder.add_edge(\"tools\", \"agent\")\n    agent_builder.add_edge(\"agent\", END)\n\n    return agent_builder.compile()\n\n\n@tool\ndef handoff_tool(transfer_to: str, transfered_by: str):\n    \"\"\"\n    Handoff to another agent.\n\n    Use this tool when the customer speaks a language that you don't understand.\n\n    Possible values for `transfer_to`:\n    - `korean_agent`\n    - `greek_agent`\n    - `spanish_agent`\n\n    Possible values for `transfered_by`:\n    - `korean_agent`\n    - `greek_agent`\n    - `spanish_agent`\n\n    Args:\n        transfer_to: The agent to transfer the conversation to\n        transfered_by: The agent that transferred the conversation\n    \"\"\"\n    if transfer_to == transfered_by:\n        return {\n            \"error\": \"Stop trying to transfer to yourself and answer the question or i will fire you.\"\n        }\n    \n    return Command(\n        update={\n            \"current_agent\": transfer_to,\n            \"transfered_by\": transfered_by,\n        },\n        goto=transfer_to,\n        graph=Command.PARENT,\n    )\n\n\ngraph_builder = StateGraph(AgentsState)\n\ngraph_builder.add_node(\n    \"korean_agent\",\n    make_agent(\n        prompt=\"You're a Korean customer support agent. You only speak and understand Korean.\",\n        tools=[handoff_tool],\n    ),\n    destinations=(\"greek_agent\", \"spanish_agent\"),\n)\ngraph_builder.add_node(\n    \"greek_agent\",\n    make_agent(\n        prompt=\"You're a Greek customer support agent. You only speak and understand Greek.\",\n        tools=[handoff_tool],\n    ),\n    destinations=(\"korean_agent\", \"spanish_agent\"),\n)\ngraph_builder.add_node(\n    \"spanish_agent\",\n    make_agent(\n        prompt=\"You're a Spanish customer support agent. You only speak and understand Spanish.\",\n        tools=[handoff_tool],\n    ),\n    destinations=(\"greek_agent\", \"korean_agent\"),\n)\n\ngraph_builder.add_edge(START, \"korean_agent\")\n\ngraph = graph_builder.compile()\n\n",
      "graph_prebuilt.py": "from dotenv import load_dotenv\nload_dotenv()\n\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph_supervisor import create_supervisor\n\nMODEL = \"openai:gpt-5\"\n\nhistory_agent = create_react_agent(\n    model=MODEL,\n    tools=[],\n    name=\"history_agent\",\n    prompt=\"You are a history expert. You only answer questions about history.\",\n)\ngeography_agent = create_react_agent(\n    model=MODEL,\n    tools=[],\n    name=\"geography_agent\",\n    prompt=\"You are a geography expert. You only answer questions about geography.\",\n)\nmaths_agent = create_react_agent(\n    model=MODEL,\n    tools=[],\n    name=\"maths_agent\",\n    prompt=\"You are a maths expert. You only answer questions about maths.\",\n)\nphilosophy_agent = create_react_agent(\n    model=MODEL,\n    tools=[],\n    name=\"philosophy_agent\",\n    prompt=\"You are a philosophy expert. You only answer questions about philosophy.\",\n)\n\nsupervisor = create_supervisor(\n    agents=[\n        history_agent,\n        maths_agent,\n        geography_agent,\n        philosophy_agent,\n    ],\n    model=init_chat_model(MODEL),\n    prompt=\"\"\"\n    You are a supervisor that routes student questions to the appropriate subject expert. \n    You manage a history agent, geography agent, maths agent, and philosophy agent. \n    Analyze the student's question and assign it to the correct expert based on the subject matter:\n        - history_agent: For historical events, dates, historical figures\n        - geography_agent: For locations, rivers, mountains, countries\n        - maths_agent: For mathematics, calculations, algebra, geometry\n        - philosophy_agent: For philosophical concepts, ethics, logic\n    \"\"\",\n).compile()\n\n# questions = [\n#     \"When was Madrid founded?\",\n#     \"What is the capital of France and what river runs through it?\",\n#     \"What is 15% of 240?\",\n#     \"Tell me about the Battle of Waterloo\",\n#     \"What are the highest mountains in Asia?\",\n#     \"If I have a rectangle with length 8 and width 5, what is its area and perimeter?\",\n#     \"Who was Alexander the Great?\",\n#     \"What countries border Switzerland?\",\n#     \"Solve for x: 2x + 10 = 30\",\n# ]\n\n# for question in questions:\n#     result = supervisor.invoke(\n#         {\n#             \"messages\": [\n#                 {\"role\": \"user\", \"content\": question},\n#             ]\n#         }\n#     )\n#     if result[\"messages\"]:\n#         for message in result[\"messages\"]:\n#             message.pretty_print()\n\n# Export graph for LangGraph Studio\ngraph = supervisor\n",
      "graph_supervisor.py": "from dotenv import load_dotenv\nload_dotenv()\n\nfrom typing import Literal\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\nfrom langgraph.types import Command\nfrom langgraph.graph.message import MessagesState\nfrom langgraph.prebuilt import ToolNode, tools_condition\nfrom langchain_core.tools import tool\nfrom langchain.chat_models import init_chat_model\nfrom pydantic import BaseModel\n\n\nclass SupervisorOutput(BaseModel):\n\n    next_agent: Literal[\"korean_agent\", \"spanish_agent\", \"greek_agent\", \"__end__\"]\n    reasoning: str\n\n\nclass AgentsState(MessagesState):\n    current_agent: str\n    transfered_by: str\n    reasoning: str\n\n\nllm = init_chat_model(\"google_genai:gemini-2.5-flash\")\n\n\ndef make_agent(prompt, tools):\n\n    def agent_node(state: AgentsState):\n        llm_with_tools = llm.bind_tools(tools)\n        response = llm_with_tools.invoke(\n            f\"\"\"\n        {prompt}\n\n        Conversation History:\n        {state[\"messages\"]}\n        \"\"\"\n        )\n        return {\"messages\": [response]}\n\n    agent_builder = StateGraph(AgentsState)\n\n    agent_builder.add_node(\"agent\", agent_node)\n    agent_builder.add_node(\n        \"tools\",\n        ToolNode(tools=tools),\n    )\n\n    agent_builder.add_edge(START, \"agent\")\n    agent_builder.add_conditional_edges(\"agent\", tools_condition)\n    agent_builder.add_edge(\"tools\", \"agent\")\n    agent_builder.add_edge(\"agent\", END)\n\n    return agent_builder.compile()\n\n\ndef supervisor(state: AgentState):\n    structured_llm = llm.with_structured_output(SupervisorOutput)\n    response = structured_llm.invoke(\n        f\"\"\"\n        You are a supervisor that routes conversations to the appropriate language agent.\n\n        Analyse the customers request and the conversation history and decide which agent should handle the conversation.\n\n        The options for the next agent are:\n        - greek_agent\n        - spanish_agent\n        - korean_agent\n\n        <CONVERSATION_HISTORY>\n        {state.get(\"messages\", [])}\n        </CONVERSATION_HISTORY>\n\n        IMPORTANT:\n\n        Never transfer to the same agent twice in a row.\n\n        If an agent has replied end the conversation by returning __end__\n        \"\"\"\n    )\n    return Command(\n        goto=response.next_agent,\n        update={\"reasoning\": response.reasoning},\n    )\n\n\ngraph_builder = StateGraph(AgentsState)\n\ngraph_builder.add_node(\n    \"supervisor\",\n    supervisor,\n    destinations=(\n        \"korean_agent\",\n        \"spanish_agent\",\n        \"greek_agent\",\n        END,\n    ),\n)\n\ngraph_builder.add_node(\n    \"korean_agent\",\n    make_agent(\n        prompt=\"You're a Korean customer support agent. You only speak and understand Korean.\",\n        tools=[],\n    ),\n)\ngraph_builder.add_node(\n    \"greek_agent\",\n    make_agent(\n        prompt=\"You're a Greek customer support agent. You only speak and understand Greek.\",\n        tools=[],\n    ),\n)\ngraph_builder.add_node(\n    \"spanish_agent\",\n    make_agent(\n        prompt=\"You're a Spanish customer support agent. You only speak and understand Spanish.\",\n        tools=[],\n    ),\n)\n\ngraph_builder.add_edge(START, \"supervisor\")\ngraph_builder.add_edge(\"korean_agent\", \"supervisor\")\ngraph_builder.add_edge(\"spanish_agent\", \"supervisor\")\ngraph_builder.add_edge(\"greek_agent\", \"supervisor\")\n\ngraph = graph_builder.compile()\n\n",
      "graph_supervisor_as_tools.py": "from dotenv import load_dotenv\nload_dotenv()\n\nfrom typing import Annotated, Literal\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\nfrom langgraph.types import Command\nfrom langgraph.graph.message import MessagesState\nfrom langgraph.prebuilt import InjectedState, ToolNode, tools_condition\nfrom langchain_core.tools import tool\nfrom langchain.chat_models import init_chat_model\nfrom pydantic import BaseModel\n\n\nclass SupervisorOutput(BaseModel):\n\n    next_agent: Literal[\"korean_agent\", \"spanish_agent\", \"greek_agent\", \"__end__\"]\n    reasoning: str\n\n\nclass AgentsState(MessagesState):\n    current_agent: str\n    transfered_by: str\n    reasoning: str\n\n\nllm = init_chat_model(\"google_genai:gemini-2.5-flash\")\n\n\ndef make_agent_tool(tool_name, tool_description, system_prompt, tools):\n\n    def agent_node(state: AgentsState):\n        llm_with_tools = llm.bind_tools(tools)\n        response = llm_with_tools.invoke(\n            f\"\"\"\n        {system_prompt}\n\n        Conversation History:\n        {state[\"messages\"]}\n        \"\"\"\n        )\n        return {\"messages\": [response]}\n\n    agent_builder = StateGraph(AgentsState)\n\n    agent_builder.add_node(\"agent\", agent_node)\n    agent_builder.add_node(\n        \"tools\",\n        ToolNode(tools=tools),\n    )\n\n    agent_builder.add_edge(START, \"agent\")\n    agent_builder.add_conditional_edges(\"agent\", tools_condition)\n    agent_builder.add_edge(\"tools\", \"agent\")\n    agent_builder.add_edge(\"agent\", END)\n\n    agent = agent_builder.compile()\n\n    @tool(\n        name_or_callable=tool_name,\n        description=tool_description,\n    )\n    def agent_tool(state: Annotated[dict, InjectedState]):\n        result = agent.invoke(state)\n        return result[\"messages\"][-1].content\n\n    return agent_tool\n\n\ntools = [\n    make_agent_tool(\n        tool_name=\"korean_agent\",\n        tool_description=\"Use this when the user is speaking korean\",\n        system_prompt=\"You're a korean customer support agent you speak in korean\",\n        tools=[],\n    ),\n    make_agent_tool(\n        tool_name=\"spanish_agent\",\n        tool_description=\"Use this when the user is speaking spanish\",\n        system_prompt=\"You're a spanish customer support agent you speak in spanish\",\n        tools=[],\n    ),\n    make_agent_tool(\n        tool_name=\"greek_agent\",\n        tool_description=\"Use this when the user is speaking greek\",\n        system_prompt=\"You're a greek customer support agent you speak in greek\",\n        tools=[],\n    ),\n]\n\n\ndef supervisor(state: AgentState):\n    llm_with_tools = llm.bind_tools(tools=tools)\n    result = llm_with_tools.invoke(state[\"messages\"])\n    return {\n        \"messages\": [result],\n    }\n\n\ngraph_builder = StateGraph(AgentsState)\n\ngraph_builder.add_node(\"supervisor\", supervisor)\ngraph_builder.add_node(\"tools\", ToolNode(tools=tools))\n\n\ngraph_builder.add_edge(START, \"supervisor\")\ngraph_builder.add_conditional_edges(\"supervisor\", tools_condition)\ngraph_builder.add_edge(\"tools\", \"supervisor\")\ngraph_builder.add_edge(\"supervisor\", END)\n\n\ngraph = graph_builder.compile()\n\n",
      "main.py": "def main():\n    print(\"Hello from 17-multi-agent-architectures!\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "main_network.ipynb": "from dotenv import load_dotenv\nload_dotenv()\n\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import Command\nfrom langgraph.graph.message import MessagesState\nfrom langgraph.prebuilt import ToolNode, tools_condition\nfrom langchain_core.tools import tool\nfrom langchain.chat_models import init_chat_model\n\n\n# ---\n\nclass AgentsState(MessagesState):\n    current_agent: str\n    transfered_by: str\n\n\nllm = init_chat_model(\"google_genai:gemini-2.5-flash\")\n\n\n# ---\n\ndef make_agent(prompt, tools):\n\n    def agent_node(state: AgentsState):\n        llm_with_tools = llm.bind_tools(tools)\n        response = llm_with_tools.invoke(\n            f\"\"\"\n        {prompt}\n\n        You have a tool called 'handoff_tool' use it to transfer to other agent, don't use it to transfer to yourself.\n\n        Conversation History:\n        {state[\"messages\"]}\n        \"\"\"\n        )\n        return {\"messages\": [response]}\n\n    agent_builder = StateGraph(AgentsState)\n\n    agent_builder.add_node(\"agent\", agent_node)\n    agent_builder.add_node(\n        \"tools\",\n        ToolNode(tools=tools),\n    )\n\n    agent_builder.add_edge(START, \"agent\")\n    agent_builder.add_conditional_edges(\"agent\", tools_condition)\n    agent_builder.add_edge(\"tools\", \"agent\")\n    agent_builder.add_edge(\"agent\", END)\n\n    return agent_builder.compile()\n\n\n# ---\n\n@tool\ndef handoff_tool(transfer_to: str, transfered_by: str):\n    \"\"\"\n    Handoff to another agent.\n\n    Use this tool when the customer speaks a language that you don't understand.\n\n    Possible values for `transfer_to`:\n    - `korean_agent`\n    - `greek_agent`\n    - `spanish_agent`\n\n    Possible values for `transfered_by`:\n    - `korean_agent`\n    - `greek_agent`\n    - `spanish_agent`\n\n    Args:\n        transfer_to: The agent to transfer the conversation to\n        transfered_by: The agent that transferred the conversation\n    \"\"\"\n    if transfer_to == transfered_by:\n        return {\n            \"error\": \"Stop trying to transfer to yourself and answer the question or i will fire you.\"\n        }\n    \n    return Command(\n        update={\n            \"current_agent\": transfer_to,\n            \"transfered_by\": transfered_by,\n        },\n        goto=transfer_to,\n        graph=Command.PARENT,\n    )\n\n\n# ---\n\ngraph_builder = StateGraph(AgentsState)\n\ngraph_builder.add_node(\n    \"korean_agent\",\n    make_agent(\n        prompt=\"You're a Korean customer support agent. You only speak and understand Korean.\",\n        tools=[handoff_tool],\n    ),\n)\ngraph_builder.add_node(\n    \"greek_agent\",\n    make_agent(\n        prompt=\"You're a Greek customer support agent. You only speak and understand Greek.\",\n        tools=[handoff_tool],\n    ),\n)\ngraph_builder.add_node(\n    \"spanish_agent\",\n    make_agent(\n        prompt=\"You're a Spanish customer support agent. You only speak and understand Spanish.\",\n        tools=[handoff_tool],\n    ),\n)\n\ngraph_builder.add_edge(START, \"korean_agent\")\n\ngraph = graph_builder.compile()\n\n\n# ---\n\nfor event in graph.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Hola! Necesito ayuda con mi cuenta.\",\n            }\n        ]\n    },\n    stream_mode=\"updates\",\n):\n    print(event)",
      "main_prebuilt.ipynb": "from dotenv import load_dotenv\nload_dotenv()\n\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph_supervisor import create_supervisor\n\n\n# ---\n\nMODEL = \"openai:gpt-5\"\n\n\n# ---\n\nhistory_agent = create_react_agent(\n    model=MODEL,\n    tools=[],\n    name=\"history_agent\",\n    prompt=\"You are a history expert. You only answer questions about history.\",\n)\ngeography_agent = create_react_agent(\n    model=MODEL,\n    tools=[],\n    name=\"geography_agent\",\n    prompt=\"You are a geography expert. You only answer questions about geography.\",\n)\nmaths_agent = create_react_agent(\n    model=MODEL,\n    tools=[],\n    name=\"maths_agent\",\n    prompt=\"You are a maths expert. You only answer questions about maths.\",\n)\nphilosophy_agent = create_react_agent(\n    model=MODEL,\n    tools=[],\n    name=\"philosophy_agent\",\n    prompt=\"You are a philosophy expert. You only answer questions about philosophy.\",\n)\n\n\n# ---\n\nsupervisor = create_supervisor(\n    agents=[\n        history_agent,\n        maths_agent,\n        geography_agent,\n        philosophy_agent,\n    ],\n    model=init_chat_model(MODEL),\n    prompt=\"\"\"\n    You are a supervisor that routes student questions to the appropriate subject expert. \n    You manage a history agent, geography agent, maths agent, and philosophy agent. \n    Analyze the student's question and assign it to the correct expert based on the subject matter:\n        - history_agent: For historical events, dates, historical figures\n        - geography_agent: For locations, rivers, mountains, countries\n        - maths_agent: For mathematics, calculations, algebra, geometry\n        - philosophy_agent: For philosophical concepts, ethics, logic\n    \"\"\",\n).compile()\n\n\n# ---\n\nquestions = [\n    \"When was Madrid founded?\",\n    \"What is the capital of France and what river runs through it?\",\n    \"What is 15% of 240?\",\n    \"Tell me about the Battle of Waterloo\",\n    \"What are the highest mountains in Asia?\",\n    \"If I have a rectangle with length 8 and width 5, what is its area and perimeter?\",\n    \"Who was Alexander the Great?\",\n    \"What countries border Switzerland?\",\n    \"Solve for x: 2x + 10 = 30\",\n]\n\n\n# ---\n\nfor question in questions:\n    result = supervisor.invoke(\n        {\n            \"messages\": [\n                {\"role\": \"user\", \"content\": question},\n            ]\n        }\n    )\n    if result[\"messages\"]:\n        for message in result[\"messages\"]:\n            message.pretty_print()\n",
      "main_supervisor.ipynb": "from dotenv import load_dotenv\nload_dotenv()\n\nfrom typing import Literal\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\nfrom langgraph.types import Command\nfrom langgraph.graph.message import MessagesState\nfrom langgraph.prebuilt import ToolNode, tools_condition\nfrom langchain_core.tools import tool\nfrom langchain.chat_models import init_chat_model\nfrom pydantic import BaseModel\n\n\n# ---\n\nclass SupervisorOutput(BaseModel):\n\n    next_agent: Literal[\"korean_agent\", \"spanish_agent\", \"greek_agent\", \"__end__\"]\n    reasoning: str\n\n\nclass AgentsState(MessagesState):\n    current_agent: str\n    transfered_by: str\n    reasoning: str\n\n\nllm = init_chat_model(\"google_genai:gemini-2.5-flash\")\n\n\n# ---\n\ndef make_agent(prompt, tools):\n\n    def agent_node(state: AgentsState):\n        llm_with_tools = llm.bind_tools(tools)\n        response = llm_with_tools.invoke(\n            f\"\"\"\n        {prompt}\n\n        Conversation History:\n        {state[\"messages\"]}\n        \"\"\"\n        )\n        return {\"messages\": [response]}\n\n    agent_builder = StateGraph(AgentsState)\n\n    agent_builder.add_node(\"agent\", agent_node)\n    agent_builder.add_node(\n        \"tools\",\n        ToolNode(tools=tools),\n    )\n\n    agent_builder.add_edge(START, \"agent\")\n    agent_builder.add_conditional_edges(\"agent\", tools_condition)\n    agent_builder.add_edge(\"tools\", \"agent\")\n    agent_builder.add_edge(\"agent\", END)\n\n    return agent_builder.compile()\n\n\n# ---\n\ndef supervisor(state: AgentState):\n    structured_llm = llm.with_structured_output(SupervisorOutput)\n    response = structured_llm.invoke(\n        f\"\"\"\n        You are a supervisor that routes conversations to the appropriate language agent.\n\n        Analyse the customers request and the conversation history and decide which agent should handle the conversation.\n\n        The options for the next agent are:\n        - greek_agent\n        - spanish_agent\n        - korean_agent\n\n        <CONVERSATION_HISTORY>\n        {state.get(\"messages\", [])}\n        </CONVERSATION_HISTORY>\n\n        IMPORTANT:\n\n        Never transfer to the same agent twice in a row.\n\n        If an agent has replied end the conversation by returning __end__\n        \"\"\"\n    )\n    return Command(\n        goto=response.next_agent,\n        update={\"reasoning\": response.reasoning},\n    )\n\n\n# ---\n\ngraph_builder = StateGraph(AgentsState)\n\ngraph_builder.add_node(\n    \"supervisor\",\n    supervisor,\n    destinations=(\n        \"korean_agent\",\n        \"spanish_agent\",\n        \"greek_agent\",\n        END,\n    ),\n)\n\ngraph_builder.add_node(\n    \"korean_agent\",\n    make_agent(\n        prompt=\"You're a Korean customer support agent. You only speak and understand Korean.\",\n        tools=[],\n    ),\n)\ngraph_builder.add_node(\n    \"greek_agent\",\n    make_agent(\n        prompt=\"You're a Greek customer support agent. You only speak and understand Greek.\",\n        tools=[],\n    ),\n)\ngraph_builder.add_node(\n    \"spanish_agent\",\n    make_agent(\n        prompt=\"You're a Spanish customer support agent. You only speak and understand Spanish.\",\n        tools=[],\n    ),\n)\n\ngraph_builder.add_edge(START, \"supervisor\")\ngraph_builder.add_edge(\"korean_agent\", \"supervisor\")\ngraph_builder.add_edge(\"spanish_agent\", \"supervisor\")\ngraph_builder.add_edge(\"greek_agent\", \"supervisor\")\n\n\ngraph = graph_builder.compile()\n\n\n# ---\n\nfor event in graph.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Hola! Necesito ayuda con mi cuenta.\",\n            }\n        ]\n    },\n    stream_mode=\"updates\",\n):\n    print(event)\n",
      "main_supervisor_as_tools.ipynb": "from dotenv import load_dotenv\nload_dotenv()\n\nfrom typing import Annotated, Literal\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\nfrom langgraph.types import Command\nfrom langgraph.graph.message import MessagesState\nfrom langgraph.prebuilt import InjectedState, ToolNode, tools_condition\nfrom langchain_core.tools import tool\nfrom langchain.chat_models import init_chat_model\nfrom pydantic import BaseModel\n\n\n# ---\n\nclass SupervisorOutput(BaseModel):\n\n    next_agent: Literal[\"korean_agent\", \"spanish_agent\", \"greek_agent\", \"__end__\"]\n    reasoning: str\n\n\nclass AgentsState(MessagesState):\n    current_agent: str\n    transfered_by: str\n    reasoning: str\n\n\nllm = init_chat_model(\"google_genai:gemini-2.5-flash\")\n\n\n# ---\n\ndef make_agent_tool(tool_name, tool_description, system_prompt, tools):\n\n    def agent_node(state: AgentsState):\n        llm_with_tools = llm.bind_tools(tools)\n        response = llm_with_tools.invoke(\n            f\"\"\"\n        {system_prompt}\n\n        Conversation History:\n        {state[\"messages\"]}\n        \"\"\"\n        )\n        return {\"messages\": [response]}\n\n    agent_builder = StateGraph(AgentsState)\n\n    agent_builder.add_node(\"agent\", agent_node)\n    agent_builder.add_node(\n        \"tools\",\n        ToolNode(tools=tools),\n    )\n\n    agent_builder.add_edge(START, \"agent\")\n    agent_builder.add_conditional_edges(\"agent\", tools_condition)\n    agent_builder.add_edge(\"tools\", \"agent\")\n    agent_builder.add_edge(\"agent\", END)\n\n    agent = agent_builder.compile()\n\n    @tool(\n        name_or_callable=tool_name,\n        description=tool_description,\n    )\n    def agent_tool(state: Annotated[dict, InjectedState]):\n        result = agent.invoke(state)\n        return result[\"messages\"][-1].content\n\n    return agent_tool\n\n\n# ---\n\ntools = [\n    make_agent_tool(\n        tool_name=\"korean_agent\",\n        tool_description=\"Use this when the user is speaking korean\",\n        system_prompt=\"You're a korean customer support agent you speak in korean\",\n        tools=[],\n    ),\n    make_agent_tool(\n        tool_name=\"spanish_agent\",\n        tool_description=\"Use this when the user is speaking spanish\",\n        system_prompt=\"You're a spanish customer support agent you speak in spanish\",\n        tools=[],\n    ),\n    make_agent_tool(\n        tool_name=\"greek_agent\",\n        tool_description=\"Use this when the user is speaking greek\",\n        system_prompt=\"You're a greek customer support agent you speak in greek\",\n        tools=[],\n    ),\n]\n\n\n# ---\n\ndef supervisor(state: AgentState):\n    llm_with_tools = llm.bind_tools(tools=tools)\n    result = llm_with_tools.invoke(state[\"messages\"])\n    return {\n        \"messages\": [result],\n    }\n\n\n# ---\n\ngraph_builder = StateGraph(AgentsState)\n\ngraph_builder.add_node(\"supervisor\", supervisor)\ngraph_builder.add_node(\"tools\", ToolNode(tools=tools))\n\n\ngraph_builder.add_edge(START, \"supervisor\")\ngraph_builder.add_conditional_edges(\"supervisor\", tools_condition)\ngraph_builder.add_edge(\"tools\", \"supervisor\")\ngraph_builder.add_edge(\"supervisor\", END)\n\n\ngraph = graph_builder.compile()\n\n\n# ---\n\nfrom langchain_core.messages import HumanMessage\n\nfor event in graph.stream(\n    {\n        \"messages\": [HumanMessage(content=\"ì•ˆë…•í•˜ì„¸ìš”! ê³„ì • ë¬¸ì œê°€ ìˆì–´ì„œ ë¬¸ì˜ë“œë¦½ë‹ˆë‹¤.\")],\n    }\n):\n    print(event)\n",
      "update_commit_dates.py": "#!/usr/bin/env python3\nimport subprocess\nimport sys\nfrom datetime import datetime, timedelta\n\n# December 7th 2025 00:10:33ë¶€í„° ì‹œì‘\nSTART_DATE = datetime(2025, 12, 7, 0, 10, 33)\nMINUTES_INTERVAL = 20\n\nprint(\"ì»¤ë°‹ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...\")\n\n# ëª¨ë“  ì»¤ë°‹ í•´ì‹œ ê°€ì ¸ì˜¤ê¸° (ê°€ì¥ ì˜¤ë˜ëœ ê²ƒë¶€í„°)\nresult = subprocess.run(\n    ['git', 'log', '--reverse', '--format=%H'],\n    capture_output=True,\n    text=True,\n    check=True\n)\n\ncommits = [c for c in result.stdout.strip().split('\\n') if c]\ntotal = len(commits)\n\nprint(f\"ì´ {total}ê°œì˜ ì»¤ë°‹ ë‚ ì§œë¥¼ ë³€ê²½í•©ë‹ˆë‹¤...\")\nprint(f\"ì‹œì‘ ë‚ ì§œ: {START_DATE.strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"ê°„ê²©: {MINUTES_INTERVAL}ë¶„\")\nprint()\n\n# ê° ì»¤ë°‹ì˜ ë‚ ì§œ ë¯¸ë¦¬ë³´ê¸°\nfor i, commit in enumerate(commits):\n    minutes_offset = i * MINUTES_INTERVAL\n    new_date = START_DATE + timedelta(minutes=minutes_offset)\n    print(f\"ì»¤ë°‹ {i+1}/{total}: {commit[:8]} -> {new_date.strftime('%Y-%m-%d %H:%M:%S')}\")\n\nprint()\nprint(\"ì»¤ë°‹ ë‚ ì§œë¥¼ ë³€ê²½í•˜ëŠ” ì¤‘...\")\n\n# Python ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‚ ì§œ ê³„ì‚°\nenv_filter_script = f'''#!/usr/bin/env python3\nimport sys\nfrom datetime import datetime, timedelta\n\nSTART_DATE = datetime(2025, 12, 7, 0, 10, 33)\nMINUTES_INTERVAL = 20\nCOMMITS = {commits}\n\ncommit_hash = sys.argv[1] if len(sys.argv) > 1 else \"\"\n\ntry:\n    idx = COMMITS.index(commit_hash)\n    minutes_offset = idx * MINUTES_INTERVAL\n    new_date = START_DATE + timedelta(minutes=minutes_offset)\n    print(new_date.strftime(\"%Y-%m-%d %H:%M:%S\"))\nexcept (ValueError, IndexError):\n    pass\n'''\n\n# ì„ì‹œ íŒŒì¼ì— Python ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±\nwith open('/tmp/calc_date.py', 'w') as f:\n    f.write(env_filter_script)\n\n# ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬\nsubprocess.run(['chmod', '+x', '/tmp/calc_date.py'], check=True)\n\n# git filter-branch ì‹¤í–‰\nenv_filter = '''commit_hash=\"$GIT_COMMIT\"\nnew_date=$(python3 /tmp/calc_date.py \"$commit_hash\")\nif [ -n \"$new_date\" ]; then\n    export GIT_AUTHOR_DATE=\"$new_date\"\n    export GIT_COMMITTER_DATE=\"$new_date\"\nfi\n'''\n\n# git filter-branch ì‹¤í–‰\nprint(\"git filter-branch ì‹¤í–‰ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\nresult = subprocess.run(\n    ['git', 'filter-branch', '-f', '--env-filter', env_filter, '--tag-name-filter', 'cat', '--', '--all'],\n    check=False\n)\n\nif result.returncode == 0:\n    print()\n    print(\"âœ“ ì™„ë£Œ! ì»¤ë°‹ ë‚ ì§œê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n    print(\"ë³€ê²½ì‚¬í•­ì„ í™•ì¸í•˜ë ¤ë©´: git log --date=format:'%Y-%m-%d %H:%M:%S'\")\nelse:\n    print()\n    print(\"âœ— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. git filter-branch ì‹¤í–‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n    sys.exit(1)\n"
    },
    "assets": [],
    "techStack": [
      "LangGraph",
      "Gemini",
      "Google ADK",
      "Python",
      "OpenAI"
    ]
  },
  {
    "id": "18",
    "title": "Tutor Agent",
    "description": "ê°œì¸í™” í•™ìŠµ ê²½í—˜ì„ ì œê³µí•˜ëŠ” AI íŠœí„°",
    "directory": "18_tutor-agent",
    "readmePath": "18_tutor-agent/README.md",
    "mainCodePath": "18_tutor-agent/main.py",
    "readme": "# Tutor Agent\n\nAI ê¸°ë°˜ ê°œì¸í™” í•™ìŠµ íŠœí„° ì‹œìŠ¤í…œì…ë‹ˆë‹¤. LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ì „ë¬¸ ì—ì´ì „íŠ¸ë¥¼ ì¡°ìœ¨í•˜ì—¬ í•™ìŠµìì˜ ìˆ˜ì¤€ê³¼ ì„ í˜¸ë„ì— ë§ëŠ” ë§ì¶¤í˜• í•™ìŠµ ê²½í—˜ì„ ì œê³µí•©ë‹ˆë‹¤.\n\n## ì£¼ìš” ê¸°ëŠ¥\n\n- **í•™ìŠµì í‰ê°€**: í•™ìŠµìì˜ ì§€ì‹ ìˆ˜ì¤€, í•™ìŠµ ìŠ¤íƒ€ì¼, êµìœ¡ì  ìš”êµ¬ì‚¬í•­ì„ íŒŒì•…\n- **ë§ì¶¤í˜• í•™ìŠµ ê²½ë¡œ**: í‰ê°€ ê²°ê³¼ì— ë”°ë¼ ìµœì ì˜ í•™ìŠµ ë°©ë²• ì œì•ˆ\n- **ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ**: 4ê°œì˜ ì „ë¬¸ ì—ì´ì „íŠ¸ê°€ í˜‘ë ¥í•˜ì—¬ í•™ìŠµ ì§€ì›\n- **ì›¹ ê²€ìƒ‰ í†µí•©**: ìµœì‹  ì •ë³´ë¥¼ í™œìš©í•œ ì •í™•í•œ í•™ìŠµ ì½˜í…ì¸  ì œê³µ\n- **í€´ì¦ˆ ìƒì„±**: ì—°êµ¬ ê¸°ë°˜ì˜ êµ¬ì¡°í™”ëœ ê°ê´€ì‹ í€´ì¦ˆ ìƒì„± ë° í”¼ë“œë°±\n\n## ì—ì´ì „íŠ¸\n\n### 1. Classification Agent (ë¶„ë¥˜ ì—ì´ì „íŠ¸)\n- **ì—­í• **: í•™ìŠµì í‰ê°€ ë° ì ì ˆí•œ í•™ìŠµ ë°©ë²• ì¶”ì²œ\n- **ê¸°ëŠ¥**: \n  - í•™ìŠµ ì£¼ì œ ë° í˜„ì¬ ì§€ì‹ ìˆ˜ì¤€ íŒŒì•…\n  - í•™ìŠµ ì„ í˜¸ë„ ì‹ë³„ (ì˜ˆì‹œ vs ì´ë¡ , ìƒì„¸ë„, í•™ìŠµ ì†ë„ ë“±)\n  - í•™ìŠµ ëª©í‘œ ë° ì„ í˜¸ë„ í™•ì¸\n  - ì ì ˆí•œ ì—ì´ì „íŠ¸ë¡œ ì „í™˜\n\n### 2. Teacher Agent (êµì‚¬ ì—ì´ì „íŠ¸)\n- **ì—­í• **: êµ¬ì¡°í™”ëœ ë‹¨ê³„ë³„ í•™ìŠµ ì œê³µ\n- **í”„ë¡œì„¸ìŠ¤**: \n  1. ì£¼ì œ ì¡°ì‚¬\n  2. ê°œë… ë¶„í•´\n  3. í•œ ë²ˆì— í•˜ë‚˜ì”© ì„¤ëª…\n  4. ì´í•´ë„ í™•ì¸ (í•„ìˆ˜)\n  5. ì¬ì„¤ëª… ë˜ëŠ” ì§„í–‰\n  6. ë‹¤ìŒ ê°œë… ë˜ëŠ” ì™„ë£Œ\n- **íŠ¹ì§•**: ê° ê°œë… ì„¤ëª… í›„ ë°˜ë“œì‹œ ì´í•´ë„ë¥¼ í™•ì¸í•˜ê³  ì§„í–‰\n\n### 3. Feynman Agent (íŒŒì¸ë§Œ ì—ì´ì „íŠ¸)\n- **ì—­í• **: íŒŒì¸ë§Œ ê¸°ë²•ì„ í†µí•œ ì´í•´ë„ ê²€ì¦\n- **í”„ë¡œì„¸ìŠ¤**:\n  1. í•„ìš”ì‹œ ì£¼ì œ ì¡°ì‚¬\n  2. ê°„ë‹¨í•œ ì„¤ëª… ìš”ì²­ (8ì„¸ ì•„ì´ì—ê²Œ ì„¤ëª…í•˜ë“¯ì´)\n  3. ì‚¬ìš©ì ì„¤ëª… ìˆ˜ì§‘\n  4. ë³µì¡ë„ í‰ê°€\n  5. ëª…í™•í™” ì§ˆë¬¸ (ë³µì¡í•œ ê²½ìš°)\n  6. ì™„ë£Œ ë˜ëŠ” ë°˜ë³µ\n- **ì² í•™**: \"ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•  ìˆ˜ ì—†ë‹¤ë©´, ì¶©ë¶„íˆ ì´í•´í•˜ì§€ ëª»í•œ ê²ƒì´ë‹¤\"\n\n### 4. Quiz Agent (í€´ì¦ˆ ì—ì´ì „íŠ¸)\n- **ì—­í• **: ì—°êµ¬ ê¸°ë°˜ í€´ì¦ˆ ìƒì„± ë° í•™ìŠµ í‰ê°€\n- **í”„ë¡œì„¸ìŠ¤**:\n  1. ì£¼ì œ ì¡°ì‚¬ (ì›¹ ê²€ìƒ‰)\n  2. í€´ì¦ˆ ê¸¸ì´ í™•ì¸\n  3. êµ¬ì¡°í™”ëœ í€´ì¦ˆ ìƒì„±\n  4. ë¬¸ì œë¥¼ í•˜ë‚˜ì”© ì œì‹œ\n  5. ìƒì„¸í•œ í”¼ë“œë°± ì œê³µ\n  6. í€´ì¦ˆ ì§„í–‰ ë° ìµœì¢… ì ìˆ˜ ì œê³µ\n\n## ê¸°ìˆ  ìŠ¤íƒ\n\n- **Python**: 3.13+\n- **LangGraph**: 0.6.6 - ìƒíƒœ ê¸°ë°˜ ë©€í‹° ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜\n- **LangChain**: 0.3.27 - LLM í†µí•©\n- **Google Gemini**: 2.5 Flash - AI ëª¨ë¸\n- **Firecrawl**: 2.16 - ì›¹ ìŠ¤í¬ë˜í•‘\n- **Pydantic**: êµ¬ì¡°í™”ëœ ë°ì´í„° ê²€ì¦\n\n## ì„¤ì¹˜\n\n### ì‚¬ì „ ìš”êµ¬ì‚¬í•­\n\n- Python 3.13 ì´ìƒ\n- [uv](https://github.com/astral-sh/uv) íŒ¨í‚¤ì§€ ê´€ë¦¬ì\n\n### ì„¤ì¹˜ ë‹¨ê³„\n\n1. ì €ì¥ì†Œ í´ë¡ :\n```bash\ngit clone <repository-url>\ncd tutor-agent\n```\n\n2. ì˜ì¡´ì„± ì„¤ì¹˜:\n```bash\nuv sync\n```\n\n## í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n\ní”„ë¡œì íŠ¸ ë£¨íŠ¸ì— `.env` íŒŒì¼ì„ ìƒì„±í•˜ê³  ë‹¤ìŒ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”:\n\n```env\n# Google Gemini API\nGOOGLE_API_KEY=your_google_api_key_here\n\n# Firecrawl API (ì›¹ ê²€ìƒ‰ìš©)\nFIRECRAWL_API_KEY=your_firecrawl_api_key_here\n\n# LangSmith (ì„ íƒì‚¬í•­, ë””ë²„ê¹… ë° ëª¨ë‹ˆí„°ë§ìš©)\nLANGSMITH_API_KEY=your_langsmith_api_key_here\n```\n\n## ì‹¤í–‰\n\n### LangGraph CLI ì‚¬ìš©\n\n```bash\nlanggraph dev\n```\n\nì´ ëª…ë ¹ì€ ë¡œì»¬ ê°œë°œ ì„œë²„ë¥¼ ì‹œì‘í•˜ê³ , LangGraph Studioì—ì„œ ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•˜ê³  í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n## í”„ë¡œì íŠ¸ êµ¬ì¡°\n\n```\ntutor-agent/\nâ”œâ”€â”€ agents/\nâ”‚   â”œâ”€â”€ classification_agent.py  # í•™ìŠµì í‰ê°€ ë° ë¼ìš°íŒ…\nâ”‚   â”œâ”€â”€ teacher_agent.py         # êµ¬ì¡°í™”ëœ í•™ìŠµ ì œê³µ\nâ”‚   â”œâ”€â”€ feynman_agent.py         # íŒŒì¸ë§Œ ê¸°ë²• ê²€ì¦\nâ”‚   â””â”€â”€ quiz_agent.py            # í€´ì¦ˆ ìƒì„± ë° í‰ê°€\nâ”œâ”€â”€ tools/\nâ”‚   â”œâ”€â”€ shared_tools.py          # ê³µìœ  ë„êµ¬ (ì—ì´ì „íŠ¸ ì „í™˜, ì›¹ ê²€ìƒ‰)\nâ”‚   â””â”€â”€ quiz_tools.py            # í€´ì¦ˆ ìƒì„± ë„êµ¬\nâ”œâ”€â”€ main.py                      # LangGraph ê·¸ë˜í”„ ì •ì˜\nâ”œâ”€â”€ langgraph.json               # LangGraph ì„¤ì •\nâ”œâ”€â”€ pyproject.toml               # í”„ë¡œì íŠ¸ ì˜ì¡´ì„±\nâ””â”€â”€ README.md                    # í”„ë¡œì íŠ¸ ë¬¸ì„œ\n```\n\n## ì—ì´ì „íŠ¸ ì „í™˜ íë¦„\n\n```\nSTART\n  â†“\nClassification Agent (í•™ìŠµì í‰ê°€)\n  â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚             â”‚             â”‚             â”‚\nTeacher    Feynman      Quiz         (END)\nAgent      Agent        Agent\n  â”‚             â”‚             â”‚\n  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n        (ì—ì´ì „íŠ¸ ê°„ ì „í™˜ ê°€ëŠ¥)\n```\n\n## ì‚¬ìš© ì˜ˆì‹œ\n\n### GODMODE (ê°œë°œì í…ŒìŠ¤íŠ¸ ëª¨ë“œ)\n\nì‚¬ìš©ìê°€ \"GODMODE\"ë¼ê³  ì…ë ¥í•˜ë©´, í‰ê°€ ê³¼ì •ì„ ê±´ë„ˆë›°ê³  ë°”ë¡œ Quiz Agentë¡œ ì´ë™í•©ë‹ˆë‹¤.\n\n### ì¼ë°˜ ì‚¬ìš© íë¦„\n\n1. **ì‹œì‘**: Classification Agentê°€ í•™ìŠµìì—ê²Œ ì¸ì‚¬í•˜ê³  ì£¼ì œë¥¼ ë¬»ìŠµë‹ˆë‹¤.\n2. **í‰ê°€**: í•™ìŠµìì˜ í˜„ì¬ ì§€ì‹ ìˆ˜ì¤€ê³¼ í•™ìŠµ ì„ í˜¸ë„ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.\n3. **ì¶”ì²œ**: ì ì ˆí•œ í•™ìŠµ ë°©ë²•ì„ ì¶”ì²œí•˜ê³  í•´ë‹¹ ì—ì´ì „íŠ¸ë¡œ ì „í™˜í•©ë‹ˆë‹¤.\n4. **í•™ìŠµ**: ì„ íƒëœ ì—ì´ì „íŠ¸ê°€ ë§ì¶¤í˜• í•™ìŠµì„ ì œê³µí•©ë‹ˆë‹¤.\n5. **ì „í™˜**: í•„ìš”ì‹œ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë¡œ ì „í™˜í•˜ì—¬ ë‹¤ì–‘í•œ í•™ìŠµ ê²½í—˜ì„ ì œê³µí•©ë‹ˆë‹¤.\n\n## ë¼ì´ì„ ìŠ¤\n\nì´ í”„ë¡œì íŠ¸ëŠ” êµìœ¡ ëª©ì ìœ¼ë¡œ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤.\n\n",
    "codes": {
      "main.py": "from dotenv import load_dotenv\nload_dotenv()\n\nfrom langgraph.graph import START, END, StateGraph, MessagesState\nfrom agents.classification_agent import classification_agent\nfrom agents.teacher_agent import teacher_agent\nfrom agents.feynman_agent import feynman_agent\nfrom agents.quiz_agent import quiz_agent\n\n\nclass TutorState(MessagesState):\n    current_agent: str\n\n\ndef router_check(state: TutorState):\n    current_agent = state.get(\"current_agent\", \"classification_agent\")\n    return current_agent\n\n\ngraph_builder = StateGraph(TutorState)\n\ngraph_builder.add_node(\n    \"classification_agent\",\n    classification_agent,\n    destinations=(\n        \"teacher_agent\",\n        \"feynman_agent\",\n    ),\n)\ngraph_builder.add_node(\n    \"teacher_agent\",\n    teacher_agent,\n    destinations=(\n        \"feynman_agent\",\n        \"quiz_agent\",\n    ),\n)\ngraph_builder.add_node(\n    \"feynman_agent\",\n    feynman_agent,\n    destinations=(\n        \"teacher_agent\",\n        \"quiz_agent\",\n    ),\n)\ngraph_builder.add_node(\n    \"quiz_agent\",\n    quiz_agent,\n    destinations=(\n        \"teacher_agent\",\n        \"feynman_agent\",\n    ),\n)\n\ngraph_builder.add_conditional_edges(\n    START,\n    router_check,\n    [\n        \"teacher_agent\",\n        \"feynman_agent\",\n        \"classification_agent\",\n        \"quiz_agent\",\n    ],\n)\ngraph_builder.add_edge(\"classification_agent\", END)\n\ngraph = graph_builder.compile()\n",
      "update_commit_dates.py": "#!/usr/bin/env python3\nimport subprocess\nimport sys\nfrom datetime import datetime, timedelta\n\n# December 8th 2025 00:10:21ë¶€í„° ì‹œì‘\nSTART_DATE = datetime(2025, 12, 8, 0, 10, 21)\nMINUTES_INTERVAL = 20\n\nprint(\"ì»¤ë°‹ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...\")\n\n# ëª¨ë“  ì»¤ë°‹ í•´ì‹œ ê°€ì ¸ì˜¤ê¸° (ê°€ì¥ ì˜¤ë˜ëœ ê²ƒë¶€í„°)\nresult = subprocess.run(\n    ['git', 'log', '--reverse', '--format=%H'],\n    capture_output=True,\n    text=True,\n    check=True\n)\n\ncommits = [c for c in result.stdout.strip().split('\\n') if c]\ntotal = len(commits)\n\nprint(f\"ì´ {total}ê°œì˜ ì»¤ë°‹ ë‚ ì§œë¥¼ ë³€ê²½í•©ë‹ˆë‹¤...\")\nprint(f\"ì‹œì‘ ë‚ ì§œ: {START_DATE.strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"ê°„ê²©: {MINUTES_INTERVAL}ë¶„\")\nprint()\n\n# ê° ì»¤ë°‹ì˜ ë‚ ì§œ ë¯¸ë¦¬ë³´ê¸°\nfor i, commit in enumerate(commits):\n    minutes_offset = i * MINUTES_INTERVAL\n    new_date = START_DATE + timedelta(minutes=minutes_offset)\n    print(f\"ì»¤ë°‹ {i+1}/{total}: {commit[:8]} -> {new_date.strftime('%Y-%m-%d %H:%M:%S')}\")\n\nprint()\nprint(\"ì»¤ë°‹ ë‚ ì§œë¥¼ ë³€ê²½í•˜ëŠ” ì¤‘...\")\n\n# Python ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‚ ì§œ ê³„ì‚°\nenv_filter_script = f'''#!/usr/bin/env python3\nimport sys\nfrom datetime import datetime, timedelta\n\nSTART_DATE = datetime(2025, 12, 8, 0, 10, 21)\nMINUTES_INTERVAL = 20\nCOMMITS = {commits}\n\ncommit_hash = sys.argv[1] if len(sys.argv) > 1 else \"\"\n\ntry:\n    idx = COMMITS.index(commit_hash)\n    minutes_offset = idx * MINUTES_INTERVAL\n    new_date = START_DATE + timedelta(minutes=minutes_offset)\n    print(new_date.strftime(\"%Y-%m-%d %H:%M:%S\"))\nexcept (ValueError, IndexError):\n    pass\n'''\n\n# ì„ì‹œ íŒŒì¼ì— Python ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±\nwith open('/tmp/calc_date.py', 'w') as f:\n    f.write(env_filter_script)\n\n# ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬\nsubprocess.run(['chmod', '+x', '/tmp/calc_date.py'], check=True)\n\n# git filter-branch ì‹¤í–‰\nenv_filter = '''commit_hash=\"$GIT_COMMIT\"\nnew_date=$(python3 /tmp/calc_date.py \"$commit_hash\")\nif [ -n \"$new_date\" ]; then\n    export GIT_AUTHOR_DATE=\"$new_date\"\n    export GIT_COMMITTER_DATE=\"$new_date\"\nfi\n'''\n\n# git filter-branch ì‹¤í–‰\nprint(\"git filter-branch ì‹¤í–‰ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\nresult = subprocess.run(\n    ['git', 'filter-branch', '-f', '--env-filter', env_filter, '--tag-name-filter', 'cat', '--', '--all'],\n    check=False\n)\n\nif result.returncode == 0:\n    print()\n    print(\"âœ“ ì™„ë£Œ! ì»¤ë°‹ ë‚ ì§œê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n    print(\"ë³€ê²½ì‚¬í•­ì„ í™•ì¸í•˜ë ¤ë©´: git log --date=format:'%Y-%m-%d %H:%M:%S'\")\nelse:\n    print()\n    print(\"âœ— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. git filter-branch ì‹¤í–‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n    sys.exit(1)\n"
    },
    "assets": [],
    "techStack": [
      "LangGraph",
      "Gemini",
      "Python"
    ]
  },
  {
    "id": "19",
    "title": "A2a",
    "description": "ì—ì´ì „íŠ¸ ê°„ ì§ì ‘ í†µì‹  ì•„í‚¤í…ì²˜ ë°ëª¨",
    "directory": "19_a2a",
    "readmePath": "19_a2a/README.md",
    "mainCodePath": "19_a2a/main.py",
    "readme": "# AI Agent-to-Agent (A2A) Architecture Demo\n\nì´ í”„ë¡œì íŠ¸ëŠ” **Google ADK**ì™€ **LangGraph**ë¥¼ ì‚¬ìš©í•˜ì—¬ **Agent-to-Agent (A2A)** í†µì‹  ì•„í‚¤í…ì²˜ë¥¼ êµ¬í˜„í•œ ì˜ˆì œì…ë‹ˆë‹¤. í•˜ë‚˜ì˜ ë©”ì¸ ì—ì´ì „íŠ¸ê°€ ë‘ ê°œì˜ ì „ë¬¸ ë¶„ì•¼ ì›ê²© ì—ì´ì „íŠ¸(Remote Agent)ì™€ í˜‘ë ¥í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ í•´ê²°í•©ë‹ˆë‹¤.\n\n## ğŸ—ï¸ ì•„í‚¤í…ì²˜ (Architecture)\n\nì‹œìŠ¤í…œì€ 3ê°œì˜ ë…ë¦½ì ì¸ ì—ì´ì „íŠ¸ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n\n1.  **StudentHelperAgent (User Facing Agent)**\n    *   **ì—­í• :** ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤(ì ‘ìˆ˜ì²˜), ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°\n    *   **ê¸°ëŠ¥:** ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì „ë¬¸ê°€(ì—­ì‚¬ ë˜ëŠ” ì² í•™)ì—ê²Œ ì‘ì—…ì„ ìœ„ì„í•©ë‹ˆë‹¤.\n    *   **ìœ„ì¹˜:** `user-facing-agent/`\n\n2.  **HistoryHelperAgent (Remote Agent 1)**\n    *   **ì—­í• :** ì—­ì‚¬ ë¶„ì•¼ ì „ë¬¸ê°€\n    *   **êµ¬í˜„:** **Google ADK** í‘œì¤€ ë°©ì‹ ì‚¬ìš©\n    *   **í¬íŠ¸:** `8001`\n    *   **ëª¨ë¸:** `gemini-2.5-flash`\n    *   **ìœ„ì¹˜:** `remote_adk_agent/`\n\n3.  **PhilosophyHelperAgent (Remote Agent 2)**\n    *   **ì—­í• :** ì² í•™ ë¶„ì•¼ ì „ë¬¸ê°€\n    *   **êµ¬í˜„:** **LangGraph** + **FastAPI** (Manual A2A í”„ë¡œí† ì½œ êµ¬í˜„)\n    *   **í¬íŠ¸:** `8002`\n    *   **ëª¨ë¸:** `gemini-2.5-flash`\n    *   **ìœ„ì¹˜:** `langraph_agent/`\n\n---\n\n## ğŸš€ ì‹œì‘í•˜ê¸° (Getting Started)\n\n### 1. í™˜ê²½ ì„¤ì • (Prerequisites)\n\nì´ í”„ë¡œì íŠ¸ëŠ” `uv` íŒ¨í‚¤ì§€ ë§¤ë‹ˆì €ì™€ `Python 3.13`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\n```bash\n# ì˜ì¡´ì„± ì„¤ì¹˜\nuv sync\n```\n\n### 2. API í‚¤ ì„¤ì • (.env)\n\ní”„ë¡œì íŠ¸ ë£¨íŠ¸ì— `.env` íŒŒì¼ì„ ìƒì„±í•˜ê³  Google Gemini API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n\n```ini\nGEMINI_API_KEY=your_api_key_here\n```\n\n### 3. ì‹¤í–‰ ë°©ë²• (Running the Agents)\n\nì´ ì‹œìŠ¤í…œì´ ì‘ë™í•˜ë ¤ë©´ **3ê°œì˜ í„°ë¯¸ë„**ì„ ê°ê° ì—´ì–´ì„œ ëª¨ë“  ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\n\n#### í„°ë¯¸ë„ 1: ì—­ì‚¬ ì „ë¬¸ê°€ (History Agent) ì‹¤í–‰\n```bash\nuv run uvicorn remote_adk_agent.agent:app --host 0.0.0.0 --port 8001 --reload\n```\n*   _8001ë²ˆ í¬íŠ¸ì—ì„œ ëŒ€ê¸°í•©ë‹ˆë‹¤._\n\n#### í„°ë¯¸ë„ 2: ì² í•™ ì „ë¬¸ê°€ (Philosophy Agent) ì‹¤í–‰\n```bash\nuv run uvicorn langraph_agent.server:app --host 0.0.0.0 --port 8002 --reload\n```\n*   _8002ë²ˆ í¬íŠ¸ì—ì„œ ëŒ€ê¸°í•©ë‹ˆë‹¤._\n\n#### í„°ë¯¸ë„ 3: ë©”ì¸ ì—ì´ì „íŠ¸ (User Facing Agent) ì‹¤í–‰\n```bash\nadk web user-facing-agent/user_facing_agent\n```\n*   _ì›¹ ë¸Œë¼ìš°ì €ê°€ ì—´ë¦¬ê³  ì±„íŒ… UIê°€ ì‹¤í–‰ë©ë‹ˆë‹¤._\n\n---\n\n## ğŸ› ï¸ ë¬¸ì œ í•´ê²° (Troubleshooting)\n\n### 503 Overloaded ì—ëŸ¬ê°€ ë°œìƒí•  ê²½ìš°\n`gemini-2.5-flash` ëª¨ë¸ ì‚¬ìš©ëŸ‰ì´ ë§ì„ ê²½ìš° 503 ì—ëŸ¬ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n1.  ì ì‹œ(1~2ë¶„) ê¸°ë‹¤ë ¸ë‹¤ê°€ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.\n2.  ê³„ì† ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ê° íŒŒì¼ì˜ ëª¨ë¸ ì„¤ì •ì„ `gemini-1.5-flash`ë¡œ ë³€ê²½í•˜ì—¬ ì•ˆì •ì ì¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n",
    "codes": {
      "main.py": "def main():\n    print(\"Hello from 19-a2a!\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    "assets": [],
    "techStack": [
      "LangGraph",
      "Gemini",
      "Google ADK",
      "FastAPI",
      "Python"
    ]
  },
  {
    "id": "20",
    "title": "Deploy Agents",
    "description": "FastAPI ê¸°ë°˜ ì—ì´ì „íŠ¸ ë°°í¬ ì„œë²„",
    "directory": "20_deploy-agents",
    "readmePath": "20_deploy-agents/README.md",
    "mainCodePath": "20_deploy-agents/main.py",
    "readme": "# 20-deploy-agents\n\nGoogle Gemini APIë¥¼ ì‚¬ìš©í•œ ëŒ€í™”í˜• ì±—ë´‡ ì„œë²„ì…ë‹ˆë‹¤. FastAPIë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„ë˜ì—ˆìœ¼ë©°, ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì§€ì›í•©ë‹ˆë‹¤.\n\n## ì£¼ìš” ê¸°ëŠ¥\n\n- ğŸ¤– **Gemini 2.5 Flash ëª¨ë¸** ì‚¬ìš©\n- ğŸ’¬ **ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬** - ê° ëŒ€í™”ì˜ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€\n- âš¡ **ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ** - ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µì„ ë°›ì„ ìˆ˜ ìˆëŠ” ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸\n- ğŸš€ **FastAPI ê¸°ë°˜** - ë¹ ë¥´ê³  í˜„ëŒ€ì ì¸ API í”„ë ˆì„ì›Œí¬\n\n## ìš”êµ¬ì‚¬í•­\n\n- Python 3.13 ì´ìƒ\n- Google Gemini API í‚¤\n\n## ì„¤ì¹˜\n\n1. í”„ë¡œì íŠ¸ í´ë¡  ë˜ëŠ” ë‹¤ìš´ë¡œë“œ\n\n2. ì˜ì¡´ì„± ì„¤ì¹˜ (uv ì‚¬ìš©)\n\n```bash\nuv sync\n```\n\në˜ëŠ” pip ì‚¬ìš©:\n\n```bash\npip install -r requirements.txt\n```\n\n## í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n\ní”„ë¡œì íŠ¸ ë£¨íŠ¸ì— `.env` íŒŒì¼ì„ ìƒì„±í•˜ê³  ë‹¤ìŒ ë‚´ìš©ì„ ì¶”ê°€í•˜ì„¸ìš”:\n\n```env\nGOOGLE_API_KEY=your_gemini_api_key_here\n```\n\nGemini API í‚¤ëŠ” [Google AI Studio](https://makersuite.google.com/app/apikey)ì—ì„œ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n## ì‹¤í–‰\n\nì„œë²„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤:\n\n```bash\nuvicorn main:app --reload\n```\n\nì„œë²„ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ `http://127.0.0.1:8000`ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.\n\nAPI ë¬¸ì„œëŠ” `http://127.0.0.1:8000/docs`ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n## API ì—”ë“œí¬ì¸íŠ¸\n\n### 1. ëŒ€í™” ìƒì„±\n\nìƒˆë¡œìš´ ëŒ€í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n\n**ìš”ì²­:**\n```http\nPOST /conversations\n```\n\n**ì‘ë‹µ:**\n```json\n{\n  \"conversation_id\": \"uuid-string\"\n}\n```\n\n### 2. ë©”ì‹œì§€ ì „ì†¡ (ì¼ë°˜)\n\nëŒ€í™”ì— ë©”ì‹œì§€ë¥¼ ì „ì†¡í•˜ê³  ì „ì²´ ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤.\n\n**ìš”ì²­:**\n```http\nPOST /conversations/{conversation_id}/message\nContent-Type: application/json\n\n{\n  \"question\": \"ì•ˆë…•í•˜ì„¸ìš”\"\n}\n```\n\n**ì‘ë‹µ:**\n```json\n{\n  \"answer\": \"ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\"\n}\n```\n\n### 3. ë©”ì‹œì§€ ì „ì†¡ (ìŠ¤íŠ¸ë¦¬ë°)\n\nëŒ€í™”ì— ë©”ì‹œì§€ë¥¼ ì „ì†¡í•˜ê³  ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤.\n\n**ìš”ì²­:**\n```http\nPOST /conversations/{conversation_id}/message-stream\nContent-Type: application/json\n\n{\n  \"question\": \"ì¤‘êµ­ì˜ ë§Œë¦¬ì¥ì„± í¬ê¸°ëŠ”?\"\n}\n```\n\n**ì‘ë‹µ:**\nìŠ¤íŠ¸ë¦¬ë° í…ìŠ¤íŠ¸ (text/plain)\n\n**cURL ì˜ˆì œ:**\n```bash\ncurl -N -X POST http://127.0.0.1:8000/conversations/{conversation_id}/message-stream \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"question\": \"ì¤‘êµ­ì˜ ë§Œë¦¬ì¥ì„± í¬ê¸°ëŠ”?\"}'\n```\n\n### 4. ë©”ì‹œì§€ ì „ì†¡ (ì „ì²´ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°)\n\nëª¨ë“  ì´ë²¤íŠ¸ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.\n\n**ìš”ì²­:**\n```http\nPOST /conversations/{conversation_id}/message-stream-all\nContent-Type: application/json\n\n{\n  \"question\": \"ì¤‘êµ­ì˜ ë§Œë¦¬ì¥ì„± í¬ê¸°ëŠ”?\"\n}\n```\n\n**ì‘ë‹µ:**\nJSON í˜•ì‹ì˜ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ (ê° ì¤„ì´ JSON ê°ì²´)\n\n## ì‚¬ìš© ì˜ˆì œ\n\n### Python ì˜ˆì œ\n\n```python\nimport requests\n\n# 1. ëŒ€í™” ìƒì„±\nresponse = requests.post(\"http://127.0.0.1:8000/conversations\")\nconversation_id = response.json()[\"conversation_id\"]\n\n# 2. ë©”ì‹œì§€ ì „ì†¡\nresponse = requests.post(\n    f\"http://127.0.0.1:8000/conversations/{conversation_id}/message\",\n    json={\"question\": \"ì•ˆë…•í•˜ì„¸ìš”\"}\n)\nprint(response.json()[\"answer\"])\n```\n\n### JavaScript/TypeScript ì˜ˆì œ\n\n```typescript\n// 1. ëŒ€í™” ìƒì„±\nconst convResponse = await fetch(\"http://127.0.0.1:8000/conversations\", {\n  method: \"POST\",\n});\nconst { conversation_id } = await convResponse.json();\n\n// 2. ìŠ¤íŠ¸ë¦¬ë° ë©”ì‹œì§€ ì „ì†¡\nconst streamResponse = await fetch(\n  `http://127.0.0.1:8000/conversations/${conversation_id}/message-stream`,\n  {\n    method: \"POST\",\n    headers: { \"Content-Type\": \"application/json\" },\n    body: JSON.stringify({ question: \"ì•ˆë…•í•˜ì„¸ìš”\" }),\n  }\n);\n\nconst reader = streamResponse.body?.getReader();\nconst decoder = new TextDecoder();\n\nwhile (true) {\n  const { done, value } = await reader!.read();\n  if (done) break;\n  const chunk = decoder.decode(value);\n  process.stdout.write(chunk);\n}\n```\n\n## í”„ë¡œì íŠ¸ êµ¬ì¡°\n\n```\n20_deploy-agents/\nâ”œâ”€â”€ main.py              # FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜\nâ”œâ”€â”€ pyproject.toml       # í”„ë¡œì íŠ¸ ì„¤ì • ë° ì˜ì¡´ì„±\nâ”œâ”€â”€ uv.lock              # ì˜ì¡´ì„± ì ê¸ˆ íŒŒì¼\nâ”œâ”€â”€ api.http             # API í…ŒìŠ¤íŠ¸ ì˜ˆì œ\nâ”œâ”€â”€ .env                 # í™˜ê²½ ë³€ìˆ˜ (gitignoreì— í¬í•¨)\nâ””â”€â”€ README.md            # í”„ë¡œì íŠ¸ ë¬¸ì„œ\n```\n\n## ê¸°ìˆ  ìŠ¤íƒ\n\n- **FastAPI** - ì›¹ í”„ë ˆì„ì›Œí¬\n- **Google Generative AI** - Gemini API í´ë¼ì´ì–¸íŠ¸\n- **Uvicorn** - ASGI ì„œë²„\n- **Pydantic** - ë°ì´í„° ê²€ì¦\n- **Python-dotenv** - í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬\n\n## ì°¸ê³ ì‚¬í•­\n\n- í˜„ì¬ ëŒ€í™” íˆìŠ¤í† ë¦¬ëŠ” ë©”ëª¨ë¦¬ì— ì €ì¥ë˜ë¯€ë¡œ ì„œë²„ ì¬ì‹œì‘ ì‹œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.\n- í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ë‚˜ ì˜êµ¬ ì €ì¥ì†Œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n- ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì€ ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ ìœ ì§€ë˜ëŠ” ë™ì•ˆë§Œ ì‘ë™í•©ë‹ˆë‹¤.\n\n## ë¼ì´ì„ ìŠ¤\n\nì´ í”„ë¡œì íŠ¸ëŠ” í•™ìŠµ ëª©ì ìœ¼ë¡œ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n",
    "codes": {
      "main.py": "import os\nimport uuid\nimport asyncio\nimport json\nfrom dotenv import load_dotenv\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.responses import StreamingResponse\nfrom pydantic import BaseModel\nimport google.generativeai as genai\n\nload_dotenv()\n\napp = FastAPI()\n\n# Gemini API ì„¤ì •\nGOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\nif not GOOGLE_API_KEY:\n    raise Exception(\"GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n\ngenai.configure(api_key=GOOGLE_API_KEY)\n\n# ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ (ë©”ëª¨ë¦¬ ê¸°ë°˜)\nconversations: dict[str, genai.ChatSession] = {}\n\n\nclass CreateConversationResponse(BaseModel):\n    conversation_id: str\n\n\nclass CreateMessageInput(BaseModel):\n    question: str\n\n\nclass CreateMessageOutput(BaseModel):\n    answer: str\n\n\n@app.post(\"/conversations\")\nasync def create_conversation() -> CreateConversationResponse:\n    conversation_id = str(uuid.uuid4())\n    \n    # Gemini ëª¨ë¸ ì´ˆê¸°í™” ë° ì±„íŒ… ì„¸ì…˜ ìƒì„±\n    model = genai.GenerativeModel(\n        \"gemini-2.5-flash\",\n        system_instruction=\"You help users with their questions.\",\n    )\n    chat = model.start_chat(history=[])\n    conversations[conversation_id] = chat\n    \n    return {\n        \"conversation_id\": conversation_id,\n    }\n\n\n@app.post(\"/conversations/{conversation_id}/message\")\nasync def create_message(\n    conversation_id: str, message_input: CreateMessageInput\n) -> CreateMessageOutput:\n    if conversation_id not in conversations:\n        raise HTTPException(status_code=404, detail=\"ëŒ€í™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n    \n    # ê¸°ì¡´ ì±„íŒ… ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸°\n    chat = conversations[conversation_id]\n    \n    # ë¹„ë™ê¸°ë¡œ ë©”ì‹œì§€ ì „ì†¡ (ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰)\n    response = await asyncio.to_thread(chat.send_message, message_input.question)\n    \n    return {\n        \"answer\": response.text,\n    }\n\n\n@app.post(\"/conversations/{conversation_id}/message-stream\")\nasync def create_message_stream(\n    conversation_id: str, message_input: CreateMessageInput\n):\n    if conversation_id not in conversations:\n        raise HTTPException(status_code=404, detail=\"ëŒ€í™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n    \n    # ê¸°ì¡´ ì±„íŒ… ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸°\n    chat = conversations[conversation_id]\n    \n    async def event_generator():\n        # ê¸°ì¡´ ì±„íŒ… ì„¸ì…˜ì˜ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°\n        history = chat.history if hasattr(chat, 'history') else []\n        \n        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ê¸°ì¡´ ì±„íŒ…ê³¼ ë™ì¼í•œ ì„¤ì •)\n        model = genai.GenerativeModel(\n            \"gemini-2.5-flash\",\n            system_instruction=\"You help users with their questions.\",\n        )\n        \n        # íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ ìƒˆ ì±„íŒ… ì‹œì‘í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš©\n        temp_chat = model.start_chat(history=history)\n        \n        # ChatSessionì˜ send_messageëŠ” ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ\n        # íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ ë©”ì‹œì§€ë¡œ ì§ì ‘ generate_content_stream ì‚¬ìš©\n        # íˆìŠ¤í† ë¦¬ + ìƒˆ ë©”ì‹œì§€ë¥¼ ê²°í•©\n        all_contents = []\n        for msg in history:\n            if isinstance(msg, dict) and \"parts\" in msg:\n                all_contents.append(msg)\n        all_contents.append({\"role\": \"user\", \"parts\": [message_input.question]})\n        \n        # ìŠ¤íŠ¸ë¦¬ë° ìƒì„± (íˆìŠ¤í† ë¦¬ í¬í•¨)\n        def generate_stream():\n            return model.generate_content(\n                all_contents,\n                stream=True,\n            )\n        \n        response_stream = await asyncio.to_thread(generate_stream)\n        \n        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ë¸íƒ€ë§Œ ì¶”ì¶œ\n        full_response = \"\"\n        for chunk in response_stream:\n            if chunk.text:\n                full_response += chunk.text\n                yield chunk.text\n        \n        # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ (ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„)\n        chat.history.append({\"role\": \"user\", \"parts\": [message_input.question]})\n        chat.history.append({\"role\": \"model\", \"parts\": [full_response]})\n    \n    return StreamingResponse(event_generator(), media_type=\"text/plain\")\n\n\n@app.post(\"/conversations/{conversation_id}/message-stream-all\")\nasync def create_message_stream_all(\n    conversation_id: str, message_input: CreateMessageInput\n):\n    if conversation_id not in conversations:\n        raise HTTPException(status_code=404, detail=\"ëŒ€í™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n    \n    # ê¸°ì¡´ ì±„íŒ… ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸°\n    chat = conversations[conversation_id]\n    \n    async def event_generator():\n        # ê¸°ì¡´ ì±„íŒ… ì„¸ì…˜ì˜ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°\n        history = chat.history if hasattr(chat, 'history') else []\n        \n        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ê¸°ì¡´ ì±„íŒ…ê³¼ ë™ì¼í•œ ì„¤ì •)\n        model = genai.GenerativeModel(\n            \"gemini-2.5-flash\",\n            system_instruction=\"You help users with their questions.\",\n        )\n        \n        # íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ ë©”ì‹œì§€ êµ¬ì„±\n        all_contents = []\n        for msg in history:\n            if isinstance(msg, dict) and \"parts\" in msg:\n                all_contents.append(msg)\n        all_contents.append({\"role\": \"user\", \"parts\": [message_input.question]})\n        \n        # ìŠ¤íŠ¸ë¦¬ë° ìƒì„± (íˆìŠ¤í† ë¦¬ í¬í•¨)\n        def generate_stream():\n            return model.generate_content(\n                all_contents,\n                stream=True,\n            )\n        \n        response_stream = await asyncio.to_thread(generate_stream)\n        \n        # ëª¨ë“  ì´ë²¤íŠ¸ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°\n        full_response = \"\"\n        for chunk in response_stream:\n            chunk_text = chunk.text if hasattr(chunk, 'text') and chunk.text else \"\"\n            full_response += chunk_text\n            \n            event_data = {\n                \"text\": chunk_text,\n                \"candidates\": [\n                    {\n                        \"content\": {\n                            \"parts\": [\n                                {\"text\": part.text}\n                                for part in chunk.candidates[0].content.parts\n                            ]\n                            if chunk.candidates and len(chunk.candidates) > 0\n                            else []\n                        }\n                    }\n                ]\n                if chunk.candidates\n                else [],\n            }\n            yield f\"{json.dumps(event_data)}\\n\"\n        \n        # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ (ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„)\n        chat.history.append({\"role\": \"user\", \"parts\": [message_input.question]})\n        chat.history.append({\"role\": \"model\", \"parts\": [full_response]})\n    \n    return StreamingResponse(event_generator(), media_type=\"text/plain\")\n"
    },
    "assets": [],
    "techStack": [
      "Gemini",
      "FastAPI",
      "Python"
    ]
  }
]